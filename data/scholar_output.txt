{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Deep convolutional neural networks for image classification: A comprehensive review', 'author': ['W Rawat', 'Z Wang'], 'pub_year': '2017', 'venue': 'Neural computation', 'abstract': 'Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/8016501/', 'author_id': ['ga7B-FEAAAAJ', 'gKnO50cAAAAJ'], 'num_citations': 1428, 'url_scholarbib': '/scholar?q=info:pG_5AsQO3_EJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BDeep%2Bconvolutional%2Bneural%2Bnetworks%2Bfor%2Bimage%2Bclassification:%2BA%2Bcomprehensive%2Breview%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pG_5AsQO3_EJ&ei=Y8IdYazgCIyMyASPrJboBg', 'citedby_url': '/scholar?cites=17428665317973389220&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:pG_5AsQO3_EJ:scholar.google.com/&scioq=+Deep+convolutional+neural+networks+for+image+classification:+A+comprehensive+review&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.mitpressjournals.org/doi/full/10.1162/neco_a_00990'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Deep convolutional neural networks for image classification: A comprehensive review', 'author': ['W Rawat', 'Z Wang'], 'pub_year': '2017', 'venue': 'Neural computation', 'abstract': 'Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/8016501/', 'author_id': ['ga7B-FEAAAAJ', 'gKnO50cAAAAJ'], 'num_citations': 1428, 'url_scholarbib': '/scholar?q=info:pG_5AsQO3_EJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BDeep%2Bconvolutional%2Bneural%2Bnetworks%2Bfor%2Bimage%2Bclassification:%2BA%2Bcomprehensive%2Breview%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pG_5AsQO3_EJ&ei=Y8IdYazgCIyMyASPrJboBg', 'citedby_url': '/scholar?cites=17428665317973389220&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:pG_5AsQO3_EJ:scholar.google.com/&scioq=+Deep+convolutional+neural+networks+for+image+classification:+A+comprehensive+review&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.mitpressjournals.org/doi/full/10.1162/neco_a_00990'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Imagenet: A large-scale hierarchical image database', 'author': ['J Deng', 'W Dong', 'R Socher', 'LJ Li', 'K Li'], 'pub_year': '2009', 'venue': '2009 IEEE conference …', 'abstract': 'The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/5206848/', 'author_id': ['U3Eub-EAAAAJ', '8T5VDv8AAAAJ', 'FaOcyfMAAAAJ', 'feX1fWAAAAAJ', '9MSpWOUAAAAJ'], 'num_citations': 30883, 'url_scholarbib': '/scholar?q=info:Uiwet4dVeggJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BImagenet:%2BA%2Blarge-scale%2Bhierarchical%2Bimage%2Bdatabase%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Uiwet4dVeggJ&ei=ZMIdYYndMbaO6rQPk9af6A0', 'citedby_url': '/scholar?cites=610894740843277394&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:Uiwet4dVeggJ:scholar.google.com/&scioq=+Imagenet:+A+large-scale+hierarchical+image+database&hl=en&as_sdt=0,33', 'eprint_url': 'https://core.ac.uk/download/pdf/194724521.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Microsoft coco: Common objects in context', 'author': ['TY Lin', 'M Maire', 'S Belongie', 'J Hays', 'P Perona'], 'pub_year': '2014', 'venue': 'European conference on …', 'abstract': 'We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48', 'author_id': ['_BPdgV0AAAAJ', 'HXowq5YAAAAJ', 'ORr4XJYAAAAJ', 'vjZrDKQAAAAJ', 'j29kMCwAAAAJ'], 'num_citations': 17612, 'url_scholarbib': '/scholar?q=info:bAkOpKbOBicJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BMicrosoft%2Bcoco:%2BCommon%2Bobjects%2Bin%2Bcontext%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bAkOpKbOBicJ&ei=ZsIdYcSZLM6TywSO-L7IDQ', 'citedby_url': '/scholar?cites=2812162232451729772&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:bAkOpKbOBicJ:scholar.google.com/&scioq=+Microsoft+coco:+Common+objects+in+context&hl=en&as_sdt=0,33', 'eprint_url': 'https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Deepfashion: Powering robust clothes recognition and retrieval with rich annotations', 'author': ['Z Liu', 'P Luo', 'S Qiu', 'X Wang'], 'pub_year': '2016', 'venue': 'Proceedings of the IEEE …', 'abstract': 'Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_cvpr_2016/html/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.html', 'author_id': ['lc45xlcAAAAJ', 'aXdjxb4AAAAJ', 'JtPSC9sAAAAJ', '-B5JgjsAAAAJ'], 'num_citations': 991, 'url_scholarbib': '/scholar?q=info:4MjRiH30b4UJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BDeepfashion:%2BPowering%2Brobust%2Bclothes%2Brecognition%2Band%2Bretrieval%2Bwith%2Brich%2Bannotations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4MjRiH30b4UJ&ei=aMIdYfHgIMWUywSUvK7ABQ', 'citedby_url': '/scholar?cites=9615172549462837472&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:4MjRiH30b4UJ:scholar.google.com/&scioq=+Deepfashion:+Powering+robust+clothes+recognition+and+retrieval+with+rich+annotations&hl=en&as_sdt=0,33', 'eprint_url': 'https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images', 'author': ['Y Ge', 'R Zhang', 'X Wang', 'X Tang'], 'pub_year': '2019', 'venue': 'Proceedings of the …', 'abstract': 'Understanding fashion images has been advanced by benchmarks with rich annotations such as DeepFashion, whose labels include clothing categories, landmarks, and consumer-commercial image pairs. However, DeepFashion has nonnegligible issues such as single clothing-item per image, sparse landmarks (4 8 only), and no per-pixel masks, making it had significant gap from real-world scenarios. We fill in the gap by presenting DeepFashion2 to address these issues. It is a versatile benchmark of four tasks including clothes detection'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.html', 'author_id': ['', 'ZJwZdtgAAAAJ', '-B5JgjsAAAAJ', 'qpBtpGsAAAAJ'], 'num_citations': 141, 'url_scholarbib': '/scholar?q=info:U_3gphDRcMMJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BDeepfashion2:%2BA%2Bversatile%2Bbenchmark%2Bfor%2Bdetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=U_3gphDRcMMJ&ei=asIdYdWcJ7yO6rQPp8mcwAY', 'citedby_url': '/scholar?cites=14082985904236985683&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:U_3gphDRcMMJ:scholar.google.com/&scioq=+Deepfashion2:+A+versatile+benchmark+for+detection&hl=en&as_sdt=0,33', 'eprint_url': 'https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': '3d shapenets: A deep representation for volumetric shapes', 'author': ['Z Wu', 'S Song', 'A Khosla', 'F Yu', 'L Zhang'], 'pub_year': '2015', 'venue': 'Proceedings of the …', 'abstract': "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5 D depth sensors (eg Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5 D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D"}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wu_3D_ShapeNets_A_2015_CVPR_paper.html', 'author_id': ['lH4zgcIAAAAJ', '5031vK4AAAAJ', '75x4pdcAAAAJ', '-XCiamcAAAAJ', 'C7DtSzYAAAAJ'], 'num_citations': 2857, 'url_scholarbib': '/scholar?q=info:BP-4kRyc1pQJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2B3d%2Bshapenets:%2BA%2Bdeep%2Brepresentation%2Bfor%2Bvolumetric%2Bshapes%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BP-4kRyc1pQJ&ei=bMIdYYrQE4r2yATNnK7YAg', 'citedby_url': '/scholar?cites=10724931209147252484&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:BP-4kRyc1pQJ:scholar.google.com/&scioq=+3d+shapenets:+A+deep+representation+for+volumetric+shapes&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_3D_ShapeNets_A_2015_CVPR_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Shapenet: An information-rich 3d model repository', 'author': ['AX Chang', 'T Funkhouser', 'L Guibas'], 'pub_year': '2015', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1512.03012', 'author_id': ['8gfs8XIAAAAJ', 'BghVDhgAAAAJ', '5JlEyTAAAAAJ'], 'num_citations': 2079, 'url_scholarbib': '/scholar?q=info:hDjW2atTnhIJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BShapenet:%2BAn%2Binformation-rich%2B3d%2Bmodel%2Brepository%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hDjW2atTnhIJ&ei=bsIdYZaVEayN6rQPmbSt8A8', 'citedby_url': '/scholar?cites=1341601736562194564&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:hDjW2atTnhIJ:scholar.google.com/&scioq=+Shapenet:+An+information-rich+3d+model+repository&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1512.03012'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models', 'author': ['M Aubry', 'D Maturana', 'AA Efros'], 'pub_year': '2014', 'venue': 'Proceedings of the …', 'abstract': 'This paper poses object category detection in images as a type of 2D-to-3D alignment problem, utilizing the large quantities of 3D CAD models that have been made publicly available online. Using the" chair" class as a running example, we propose an exemplar-based 3D category representation, which can explicitly model chairs of different styles as well as the large variation in viewpoint. We develop an approach to establish part-based correspondences between 3D CAD models and real photographs. This is achieved by (i)'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_cvpr_2014/html/Aubry_Seeing_3D_Chairs_2014_CVPR_paper.html', 'author_id': ['0MiPsosAAAAJ', 'JcZUd5IAAAAJ', 'd97bGd8AAAAJ'], 'num_citations': 502, 'url_scholarbib': '/scholar?q=info:P9d38424OfoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BSeeing%2B3d%2Bchairs:%2Bexemplar%2Bpart-based%2B2d-3d%2Balignment%2Busing%2Ba%2Blarge%2Bdataset%2Bof%2Bcad%2Bmodels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=P9d38424OfoJ&ei=b8IdYc3MOIzyyAS6-KLoCw', 'citedby_url': '/scholar?cites=18030645502969108287&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:P9d38424OfoJ:scholar.google.com/&scioq=+Seeing+3d+chairs:+exemplar+part-based+2d-3d+alignment+using+a+large+dataset+of+cad+models&hl=en&as_sdt=0,33', 'eprint_url': 'http://openaccess.thecvf.com/content_cvpr_2014/papers/Aubry_Seeing_3D_Chairs_2014_CVPR_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Benchmark Datasets for Fault Detection and Classification in Sensor Data.', 'author': ['B de Bruijn', 'TA Nguyen', 'D Bucur', 'K Tei'], 'pub_year': '2016', 'venue': 'SENSORNETS', 'abstract': 'Data measured and collected from embedded sensors often contains faults, ie, data points which are not an accurate representation of the physical phenomenon monitored by the sensor. These data faults may be caused by deployment conditions outside the operational bounds for the node, and short-or long-term hardware, software, or communication problems. On the other hand, the applications will expect accurate sensor data, and recent literature proposes algorithmic solutions for the fault detection and classification in sensor'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://research.rug.nl/files/128994363/SENSORNETS_2016_14.pdf', 'author_id': ['', 'opKs7jMAAAAJ', 'TO9gB8kAAAAJ', 'TfDuEawAAAAJ'], 'num_citations': 11, 'url_scholarbib': '/scholar?q=info:8H5xObx9gYoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BBenchmark%2BDatasets%2Bfor%2BFault%2BDetection%2Band%2BClassification%2Bin%2BSensor%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8H5xObx9gYoJ&ei=ccIdYfb8M47gyQTtwYKwBA', 'citedby_url': '/scholar?cites=9980396496600792816&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:8H5xObx9gYoJ:scholar.google.com/&scioq=+Benchmark+Datasets+for+Fault+Detection+and+Classification+in+Sensor+Data&hl=en&as_sdt=0,33', 'eprint_url': 'https://research.rug.nl/files/128994363/SENSORNETS_2016_14.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'A style-based generator architecture for generative adversarial networks', 'author': ['T Karras', 'S Laine', 'T Aila'], 'pub_year': '2019', 'venue': '… of the IEEE/CVF Conference on …', 'abstract': 'We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (eg, pose and identity when trained on human faces) and stochastic variation in the generated images (eg, freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html', 'author_id': ['-50qJW8AAAAJ', 'UCXJOTUAAAAJ', 'e7abmgkAAAAJ'], 'num_citations': 2112, 'url_scholarbib': '/scholar?q=info:NZKs74dWLn4J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BA%2Bstyle-based%2Bgenerator%2Barchitecture%2Bfor%2Bgenerative%2Badversarial%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=NZKs74dWLn4J&ei=dMIdYaSHCIyMyASPrJboBg', 'citedby_url': '/scholar?cites=9092299839549248053&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:NZKs74dWLn4J:scholar.google.com/&scioq=+A+style-based+generator+architecture+for+generative+adversarial+networks&hl=en&as_sdt=0,33', 'eprint_url': 'https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'A compact embedding for facial expression similarity', 'author': ['R Vemulapalli', 'A Agarwala'], 'pub_year': '2019', 'venue': 'Proceedings of the IEEE/CVF …', 'abstract': 'Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Vemulapalli_A_Compact_Embedding_for_Facial_Expression_Similarity_CVPR_2019_paper.html', 'author_id': ['0OFqm7YAAAAJ', 'JY-WzksAAAAJ'], 'num_citations': 35, 'url_scholarbib': '/scholar?q=info:W2CNxQUy8t0J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BA%2Bcompact%2Bembedding%2Bfor%2Bfacial%2Bexpression%2Bsimilarity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=W2CNxQUy8t0J&ei=dsIdYbKZC7aO6rQPk9af6A0', 'citedby_url': '/scholar?cites=15992900227116654683&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:W2CNxQUy8t0J:scholar.google.com/&scioq=+A+compact+embedding+for+facial+expression+similarity&hl=en&as_sdt=0,33', 'eprint_url': 'https://openaccess.thecvf.com/content_CVPR_2019/papers/Vemulapalli_A_Compact_Embedding_for_Facial_Expression_Similarity_CVPR_2019_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': 'Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding', 'author': ['K Mo', 'S Zhu', 'AX Chang', 'L Yi'], 'pub_year': '2019', 'venue': 'Proceedings of the …', 'abstract': 'We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Mo_PartNet_A_Large-Scale_Benchmark_for_Fine-Grained_and_Hierarchical_Part-Level_3D_CVPR_2019_paper.html', 'author_id': ['pL7JsOsAAAAJ', '', '8gfs8XIAAAAJ', 'UyZL660AAAAJ'], 'num_citations': 182, 'url_scholarbib': '/scholar?q=info:z5T4G12LCCgJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2BPartnet:%2BA%2Blarge-scale%2Bbenchmark%2Bfor%2Bfinegrained%2Band%2Bhierarchical%2Bpart-level%2B3d%2Bobject%2Bunderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=z5T4G12LCCgJ&ei=eMIdYabiIc6TywSO-L7IDQ', 'citedby_url': '/scholar?cites=2884708793348297935&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:z5T4G12LCCgJ:scholar.google.com/&scioq=+Partnet:+A+large-scale+benchmark+for+finegrained+and+hierarchical+part-level+3d+object+understanding&hl=en&as_sdt=0,33', 'eprint_url': 'https://openaccess.thecvf.com/content_CVPR_2019/papers/Mo_PartNet_A_Large-Scale_Benchmark_for_Fine-Grained_and_Hierarchical_Part-Level_3D_CVPR_2019_paper.pdf'}
{'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 1>, 'bib': {'title': '3d object representations for fine-grained categorization', 'author': ['J Krause', 'M Stark', 'J Deng', 'L Fei-Fei'], 'pub_year': '2013', 'venue': 'Proceedings of the IEEE …', 'abstract': 'While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html', 'author_id': ['7DwDYzkAAAAJ', 'cCda-zQAAAAJ', 'U3Eub-EAAAAJ', 'rDfyQnIAAAAJ'], 'num_citations': 1415, 'url_scholarbib': '/scholar?q=info:P_zOyPBjdFMJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3D%2B3d%2Bobject%2Brepresentations%2Bfor%2Bfine-grained%2Bcategorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=P_zOyPBjdFMJ&ei=esIdYZviI8WUywSUvK7ABQ', 'citedby_url': '/scholar?cites=6013541288258763839&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:P_zOyPBjdFMJ:scholar.google.com/&scioq=+3d+object+representations+for+fine-grained+categorization&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/papers/Krause_3D_Object_Representations_2013_ICCV_paper.pdf'}


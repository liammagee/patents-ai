{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scientific Paper Analysis\n",
    "\n",
    "This notebook performs several actions to analyse (a) a 'seed' scientific paper, and (b) extract and download any references that can be found in that paper in arXiv.org. It can perform that step recursively, to build up a small graph of papers referred to by other papers. \n",
    "\n",
    "It uses a lot of heuristics and regular expressions to parse PDF files. Due to the huge variety of citation formats, it is very brittle. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install and import dependencies, other preparation tasks\n",
    "\n",
    "(Note: dependies should be installed via requirements.txt)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# conda install pytorch torchvision torchaudio -c pytorch\n",
    "# pip install transformers\n",
    "# pip install nltk\n",
    "# pip install numpy \n",
    "# pip install pandas\n",
    "#pip install wordcloud\n",
    "# conda install matplotlib\n",
    "# pip install scholarly\n",
    "#pip install icecream"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "source": [
    "\n",
    "# Common Python libraries, used for regular expressions, file downloads and other things\n",
    "import os, os.path\n",
    "import re\n",
    "import urllib, urllib.request, urllib.parse\n",
    "# Parse arXiv API responses (in ATOM XML format)\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Alternative to print()\n",
    "from icecream import ic\n",
    "\n",
    "# Used to extract PDF text\n",
    "import textract\n",
    "\n",
    "# Numpy and Pandas for data manipulation.\n",
    "# All references stored in a Pandas DataFrame for easy analysis.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Not used yet - but helpful for simple machine learning tasks.\n",
    "# Maybe helpful for text summarisation in particular.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Natural Language Toolkit - for tokenisation and basic linguistic analysis\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Simple word cloud generator\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Google Scholar wrapper - not that useful yet\n",
    "from scholarly import scholarly\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below downloads a set of English language stopwords ('a', 'the' etc), and adds commonly occuring terms in PDFs, to remove further noise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Preparatory steps\n",
    "nltk.download('stopwords')\n",
    "full_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "full_stop_words.add('b')\n",
    "full_stop_words.add('c')\n",
    "full_stop_words.add('d')\n",
    "full_stop_words.add('e')\n",
    "full_stop_words.add('f')\n",
    "full_stop_words.add('g')\n",
    "full_stop_words.add('h')\n",
    "full_stop_words.add('j')\n",
    "full_stop_words.add('k')\n",
    "full_stop_words.add('l')\n",
    "full_stop_words.add('m')\n",
    "full_stop_words.add('n')\n",
    "full_stop_words.add('p')\n",
    "full_stop_words.add('q')\n",
    "full_stop_words.add('r')\n",
    "full_stop_words.add('u')\n",
    "full_stop_words.add('v')\n",
    "full_stop_words.add('x')\n",
    "full_stop_words.add('w')\n",
    "full_stop_words.add('y')\n",
    "full_stop_words.add('z')\n",
    "full_stop_words.add('pp')\n",
    "full_stop_words.add('et')\n",
    "full_stop_words.add('al')\n",
    "full_stop_words.add('ha')\n",
    "full_stop_words.add('li')\n",
    "full_stop_words.add('sij')\n",
    "full_stop_words.add('arxiv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/liam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reference Extraction and Parsing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "A series of regular expression utilities for parsing and extraction whoe references from PDFs.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "source": [
    "# Common functions for parsing references from plain text\n",
    "\n",
    "def re_matches(text, regex):\n",
    "    iterator = re.findall(regex, text)\n",
    "\n",
    "    refs = []\n",
    "    for match in iterator:\n",
    "        refs.append(match) \n",
    "\n",
    "    return refs\n",
    "\n",
    "def re_digit():\n",
    "    return r'\\[\\d*\\]\\ '\n",
    "\n",
    "def re_first_first():\n",
    "    return r'(?:(?:\\-?[A-Z]\\.\\ )+(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\,?\\ ?)+)'\n",
    "\n",
    "def re_first_first_multiple():\n",
    "    return r'(?:' + re_first_first() +'+and\\ )?'\n",
    "\n",
    "def re_first_last():\n",
    "    return r'(?:(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)'\n",
    "\n",
    "def re_first_last_multiple():\n",
    "    return '(?:(?:' + re_first_last() +'\\,\\ )+' + re_first_last() + r'\\,?\\ and\\ )?'\n",
    "\n",
    "def re_full_first():\n",
    "    return r'(?:(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\ )+(?:\\-?[A-Z]\\.?\\ )*(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\,?\\ ?)+)'\n",
    "\n",
    "def re_year():\n",
    "    return r'\\(?(?:\\d{4})\\)?'\n",
    "\n",
    "def re_trailing_digit():\n",
    "    return r'(?:[\\d\\ \\,]+\\ )?'\n",
    "\n",
    "\n",
    "def re_in_journal():\n",
    "    return r'(?:(?:in)?\\ [^\\,]+\\,\\ )'\n",
    "\n",
    "def re_pages():\n",
    "    return r'(?:\\ (?:pp\\.\\ )?\\d+\\-?\\d+)?'\n",
    "\n",
    "\n",
    "# Type 0\n",
    "# Example: 2106.12139.pdf\n",
    "def gen_refs_with_pages(text):\n",
    "    r = ( \n",
    "        re_digit() \n",
    "        + r'('\n",
    "        + re_first_first()  \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        + re_year() \n",
    "        + re_pages()\n",
    "        + r')'\n",
    "        + r'\\.?'\n",
    "        # + re_trailing_digit() \n",
    "        )\n",
    "    regex = re.compile(r)\n",
    "    return re_matches(text, regex)\n",
    "\n",
    "# Type 1\n",
    "# Example: Fashionista: A fashion-aware graphical system for exploring\n",
    "def gen_refs_end_year_firstname_first(text):\n",
    "    r = ( \n",
    "        re_digit() \n",
    "        + r'('\n",
    "        + re_first_first_multiple() \n",
    "        + re_first_first()  \n",
    "        + r'[^\\[]*?' \n",
    "        + re_in_journal()\n",
    "        + r'[^\\[]*?' \n",
    "        + re_year() \n",
    "        + r')'\n",
    "        + r'\\.?'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "# Type 2\n",
    "# Example: Fashion-gen: The generative fashion dataset and challenge.pdf\n",
    "def gen_refs_end_year_firstname_last(text):\n",
    "    regex = r'((?:(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)[^\\.]*\\.\\ [^\\.]*\\,\\ \\d{4}\\.)'\n",
    "    r = ( \n",
    "        r'('\n",
    "        + re_first_last_multiple() \n",
    "        + re_first_last()  \n",
    "        # + r'[^\\[]*?\\,?\\ ' \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        # + re_pages()\n",
    "        + re_year() \n",
    "        + r'\\.'\n",
    "        + r')'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "\n",
    "# Type 3\n",
    "# Example: A compact embedding for facial expression similarity.pdf\n",
    "def gen_refs_with_pages_conjoined(text):\n",
    "    r = ( \n",
    "        # re_digit() \n",
    "        r'('\n",
    "        + re_first_first_multiple() \n",
    "        + re_first_first()  \n",
    "        + r'\\.[^\\[]*?\\,\\ ' \n",
    "        + re_year() \n",
    "        + r'\\.' \n",
    "        + re_trailing_digit() \n",
    "        + r')'\n",
    "        )\n",
    "    regex = re.compile(r)\n",
    "    return re_matches(text, regex)\n",
    "\n",
    "# Type 4\n",
    "# Example: Shapenet/ An information-rich 3d model repository.pdf\n",
    "def gen_refs_expanded_name(text):\n",
    "    r = ( \n",
    "        r'\\[(?:\\d+)\\]\\ '\n",
    "        + r'('\n",
    "        + re_full_first()  \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        + re_year() \n",
    "        + r'\\.?'\n",
    "        + r')'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "\n",
    "# Defunct functions\n",
    "\n",
    "# def gen_refs_end_year(text):\n",
    "#     regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "#     return re_matches(text, regex) \n",
    "\n",
    "# def gen_refs_end_year_with_text_brackets(text):\n",
    "#     regex = r'\\[(?:[A-Za-z\\ ]*\\.?\\,\\ \\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "#     return re_matches(text, regex) \n",
    "\n",
    "# def gen_refs_end_year_with_number_brackets(text):\n",
    "#     regex = r'\\[(?:\\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "#     return re_matches(text, regex) \n",
    "    \n",
    "# def gen_refs_end_pages(text):\n",
    "#     text = text.replace('\\n', ' ')\n",
    "#     regex = r'\\[\\d*\\]\\ [^[]*pp\\.\\ \\d+\\.\\ '\n",
    "#     return re_matches(text, regex) \n",
    "\n",
    "\n",
    "# Main reference 'generator'. Looks for a variety of reference patterns,\n",
    "# and returns, along with the extracted references, a number that acts to flag\n",
    "# the best internal reference parsing option to apply.\n",
    "def gen_refs_multiple_pass(text):\n",
    "    refs0 = gen_refs_with_pages(text)\n",
    "    refs1 = gen_refs_end_year_firstname_first(text)\n",
    "    refs2 = gen_refs_end_year_firstname_last(text)\n",
    "    refs3 = gen_refs_with_pages_conjoined(text)\n",
    "\n",
    "    # Don't use this just yet - too greedy\n",
    "    refs4 = gen_refs_expanded_name(text)\n",
    "\n",
    "    lst = [refs0, refs1, refs2, refs3, refs4]\n",
    "    lst_sorted = sorted(lst, key=len, reverse=True)\n",
    "    refs = lst_sorted[0]\n",
    "    ref_type = lst.index(refs)\n",
    "\n",
    "    # Specific checks\n",
    "    if ref_type == 0 and len(refs0) == len(refs1):\n",
    "        refs = refs1\n",
    "        ref_type = 1\n",
    "\n",
    "    return refs, ref_type\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, messy code to parse a reference string into parts (authors, title, journal / conference, publisher, volume, pages, year).\n",
    "\n",
    "The type returned by gen_refs_multiple_pass() corresponds to the parse_refX function invoked here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "source": [
    "# Common functions for extracting references parts\n",
    "\n",
    "# Makes a data frame object from a set of reference parameters\n",
    "def make_dataframe(authors, title, journal, publisher, volume, year, pages, ref, source_file, source_title):\n",
    "    return pd.DataFrame([[(', ').join(authors), title, journal, publisher, volume, year, pages, ref, source_file, source_title]], \n",
    "                        columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "# Uses a general citation pattern to extract the authors, title, journal, volume, year, and pages\n",
    "# The author pattern extracts individual authors. \n",
    "# The mapping object enables different orders in the regular expression.\n",
    "def _parse_ref_internals(ref, citation_pattern, authors_pattern, mapping = {\n",
    "        'first_authors': 0,\n",
    "        'last_author': 1,\n",
    "        'title': 2,\n",
    "        'journal': 3,\n",
    "        'publisher': 4,\n",
    "        'volume': 5,\n",
    "        'year': 6,\n",
    "        'pages': 7\n",
    "    }):\n",
    "    all_authors = []\n",
    "    title = ''\n",
    "    journal = ''\n",
    "    publisher = ''\n",
    "    volume = ''\n",
    "    year = ''\n",
    "    pages = ''\n",
    "\n",
    "    iterator = re.findall(citation_pattern, ref)\n",
    "    for i in iterator:\n",
    "        first_authors = i[mapping['first_authors']]\n",
    "        # ic(first_authors)\n",
    "        last_author = i[mapping['last_author']].strip()\n",
    "        title = i[mapping['title']]\n",
    "        journal = i[mapping['journal']]\n",
    "        publisher = i[mapping['publisher']]\n",
    "        volume = i[mapping['volume']]\n",
    "        year = i[mapping['year']]\n",
    "        pages = i[mapping['pages']]\n",
    "        if first_authors != '':\n",
    "            authors = re.findall(authors_pattern, first_authors)\n",
    "            for a in authors:\n",
    "                all_authors.append(a)\n",
    "        all_authors.append(last_author)\n",
    "    return all_authors, title, journal, publisher, volume, year, pages\n",
    "\n",
    "# Parses a reference that follows this pattern:\n",
    "# Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, M.S. Lew, Deep learning for visual understanding: A review, Neurocomputing, 187 (2016) 27-48\n",
    "# From 2106.12139.pdf\n",
    "# - Year before pages\n",
    "# - All fields are comma-separated\n",
    "# - Authors are not joined by a conjuntion\n",
    "def parse_ref0(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+)\\,\\ )*)' # authors\n",
    "        + r'((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+))\\,\\ ' # last author\n",
    "        + r'([^\\,\\.]*)\\,\\ ' # title\n",
    "        + r'((?:[^\\,]*)\\,\\ )?' # journal\n",
    "        + r'((?:[^\\,]*)\\,\\ )?' # publisher\n",
    "        + r'(\\d+\\ )?' # volume\n",
    "        + r'\\(?(\\d{4})\\)?\\,?\\ ?' # year\n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-\\d+)?') # pages\n",
    "    authors_pattern = re.compile(\n",
    "        r'((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\ \\-]+))')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "\n",
    "\n",
    "# Parses a reference that follows this pattern:\n",
    "def parse_ref1(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.\\ )+(?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+)\\,?\\ )*and\\ )?' \n",
    "        + r'((?:\\-?[A-Zvd]\\.\\ )+(?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+))[\\,\\.]\\ '  # last author\n",
    "        + r'[\\u0080-\\uFFFF]?([^\\,\\.]*)[\\,\\.\\u0080-\\uFFFF]+' # title\n",
    "        + r'(?:\\ in\\ )?([^\\,\\.]*)' # journal\n",
    "        + r'(?:([^\\,]*)\\,\\ )?' # publisher \n",
    "        + r'(\\d+\\ )?' # volume\n",
    "        + r'(\\d{4})?\\,?\\ ?' # year\n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-?\\d+)?' # pages\n",
    "        ) \n",
    "    authors_pattern = re.compile(r'(?:\\-?[A-Z]\\.\\ )+[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "# Parses a reference that follows this pattern:\n",
    "def parse_ref2(ref):\n",
    "    \n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:[A-Zvd][A-Za-z\\-\\ \\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?'\n",
    "        + r'([A-Zvd][A-Za-za\\-\\ \\u0080-\\uFFFF]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)' \n",
    "        + r'([^\\.]*)' # title\n",
    "        + r'\\.\\ '\n",
    "        + r'((?:[^\\,]*))?'  # journal\n",
    "        + r'(?:\\,\\ (.*(?:pp\\.\\ |d+\\(d+\\)\\:)?\\d+(?:\\-\\d*)?))?' # pages \n",
    "        + r'(\\.\\ (?:[^\\,]*))?' # publisher\n",
    "        + r'\\,\\ '\n",
    "        + r'(\\d{4})'\n",
    "        + r'(\\,\\ \\d+\\ )?' # volume\n",
    "        # + r'\\.'\n",
    "        )\n",
    "    authors_pattern = re.compile(\n",
    "        r'(?:[A-Zvd][A-Za-za\\-\\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern, mapping = {\n",
    "        'first_authors': 0,\n",
    "        'last_author': 1,\n",
    "        'title': 2,\n",
    "        'journal': 3,\n",
    "        'pages': 4,\n",
    "        'publisher': 5,\n",
    "        'volume': 7,\n",
    "        'year': 6\n",
    "    })\n",
    "\n",
    "def parse_ref3(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\ \\-]+\\,\\ ))*)and\\ ((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\ \\-]+\\.))?' \n",
    "        + r'([^\\,]*)\\,\\ ([^\\,]*)\\,\\ (?:([^\\,]*)\\,\\ )?' \n",
    "        + r'(?:(\\d+)\\ )?' # volume\n",
    "        + r'\\(?(\\d{4})\\)?\\,?\\ ?' \n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-?\\d+)?')\n",
    "    authors_pattern = re.compile(r'((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\ \\-]+))')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "\n",
    "\n",
    "def parse_ref4(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\ )+(?:\\-?[A-Z]\\.?\\ )*(?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+)\\,?\\ )*)and\\ ' \n",
    "        + r'((?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\ )+(?:\\-?[A-Zvd]\\.?\\ )*(?:[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+))[\\,\\.]\\ (?:et\\ al\\.\\ )?'  # last author\n",
    "        + r'([^\\.]*)\\.\\ ' # title\n",
    "        + r'(?:In\\ )?([^\\,]*)(?:[\\,\\.]\\ )' # journal\n",
    "        + r'(\\d+(?:\\(\\d+\\))?\\:?)?' # volume\n",
    "        + r'(?:pages\\ )?(\\d+(?:[\\–|\\u2014]\\d+)?)?' # pages\n",
    "        + r'(?:([^\\,]*)\\,\\ )?' # publisher \n",
    "        + r'.*(\\d{4})\\.' # year\n",
    "        # + r'(\\!)?'\n",
    "        # + r'(\\!)?'\n",
    "        # + r'(\\!)?'\n",
    "        # + r'(\\!)?'\n",
    "        ) \n",
    "    authors_pattern = re.compile(r'(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\ )+(?:\\-?[A-Z]\\.\\ )*[A-Zvd][A-Za-z\\u0080-\\uFFFF\\-\\ ]+')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern, mapping = {\n",
    "        'first_authors': 0,\n",
    "        'last_author': 1,\n",
    "        'title': 2,\n",
    "        'journal': 3,\n",
    "        'volume': 4,\n",
    "        'pages': 5,\n",
    "        'publisher': 6,\n",
    "        'year': 7\n",
    "    })\n",
    "\n",
    "def parse_ref(ref, ref_type):\n",
    "    if ref_type == 0:\n",
    "        return parse_ref0(ref)\n",
    "    elif ref_type == 1:\n",
    "        return parse_ref1(ref)\n",
    "    elif ref_type == 2:\n",
    "        return parse_ref2(ref)\n",
    "    elif ref_type == 3:\n",
    "        return parse_ref3(ref)\n",
    "    elif ref_type == 4:\n",
    "        return parse_ref4(ref)\n",
    "    else:\n",
    "        return parse_ref0(ref)\n",
    "\n",
    "\n",
    "def parse_ref_and_makedataframe(ref, ref_type, source_file, source_title):\n",
    "    authors, title, journal, publisher, volume, year, pages = parse_ref(ref, ref_type)\n",
    "    return make_dataframe(authors, title, journal, publisher, volume, year, pages, ref, source_file, source_title)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A set of utility functions to help with processing PDF and analysing text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "source": [
    "# Other utility functions\n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def remove_set_from_dict(s, d):\n",
    "    for t in s:\n",
    "        if t in d:\n",
    "            del d[t]\n",
    "    return d\n",
    "    \n",
    "def word_frequencies(text):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    frequencies = {}\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "def word_frequencies_dist(text, frequencies):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "\n",
    "def wc(freqs, file_name):\n",
    "    wordcloud = WordCloud(background_color=\"white\", width=600, height=600, max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "    #wordcloud.generate(long_string)\n",
    "    wordcloud.generate_from_frequencies(freqs)\n",
    "    wordcloud.to_file(f'data/word-cloud-{file_name}.png')\n",
    "    wordcloud.to_image()\n",
    "\n",
    "\n",
    "\n",
    "def arxiv_results(title):\n",
    "    t =  urllib.parse.quote_plus(title)\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:'+t+'&start=0&max_results=1'\n",
    "    data = urllib.request.urlopen(url)\n",
    "    results = data.read().decode('utf-8')\n",
    "    return results\n",
    "\n",
    "def make_up_file_name(file_name, directory):\n",
    "    return os.path.join(directory, file_name + '.pdf')\n",
    "\n",
    "def extract_and_save_pdf_from_atom(atom_xml, title, title_no_colon, title_stem, directory, pass_through=False):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    for e in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        t = e.find('{http://www.w3.org/2005/Atom}title')\n",
    "        link = e.find('{http://www.w3.org/2005/Atom}link')\n",
    "        if t.text.lower().find(title_stem.lower()) == 0:\n",
    "            for link in e.findall('{http://www.w3.org/2005/Atom}link'):\n",
    "                # Title must match and link must be a pdf\n",
    "                if 'type' in link.attrib and link.attrib['type'] == 'application/pdf':\n",
    "                    u  = link.attrib['href']\n",
    "                    ic(u, title)\n",
    "                    if not pass_through:\n",
    "                        urllib.request.urlretrieve(u, \n",
    "                            make_up_file_name(title, directory))\n",
    "\n",
    "def test_element(atom_xml):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
    "    if entry is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def top_dist(freqs, n):\n",
    "    counter = 0\n",
    "    for w in sorted(freqs, key = freqs.get, reverse = True):\n",
    "        counter = counter + 1\n",
    "        print(w, freqs[w])\n",
    "        if counter == n:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below analyses frequencies and extracts references for a 'seed' paper.\n",
    "\n",
    "Both file name and title must be supplied, and the PDF file downloaded to the 'refs' directory first."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "source": [
    "\n",
    "# Input\n",
    "file_name = '2106.12139.pdf'\n",
    "title = 'PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database'\n",
    "\n",
    "\n",
    "# Extract the text\n",
    "text = textract.process(\"refs/\" + file_name).decode('utf-8')\n",
    "\n",
    "# Obtain word frequencies\n",
    "freqs = word_frequencies(text)\n",
    "\n",
    "# Set up a references dataframe\n",
    "references = pd.DataFrame(columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "# Extract the references\n",
    "refs, ref_type = gen_refs_multiple_pass(text)\n",
    "\n",
    "# Add the references to the dataframe\n",
    "for r in refs:\n",
    "    r = r.replace('\\n', ' ') # Is this necessary? \n",
    "    references = references.append(parse_ref_and_makedataframe(r, ref_type, file_name, title))\n",
    "\n",
    "# Save references to a csv\n",
    "references.to_csv('data/references.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# Test code\n",
    "\n",
    "# t = references['title'].iloc[7]\n",
    "# print(t)\n",
    "# r = arxiv_results(t.strip())\n",
    "# print(r)\n",
    "# # ET.tostring(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "source": [
    "## Test code\n",
    "\n",
    "# s = \"\"\"Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\n",
    "# face attributes in the wild. In Proceedings of the IEEE\n",
    "# International Conference on Computer Vision, pp. 3730–\n",
    "# 3738, 2015.\"\"\"\n",
    "\n",
    "# # \\1{5}.*\\d\\d\\d\\d\\.\n",
    "# # regex = r'[A-Z][A-Za-z\\,\\.\\ ]+[A-Z]\\.\\ [A-Z](?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# pattern = re.compile(regex)\n",
    "# iterator = re.findall(pattern, s)\n",
    "# print(iterator)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "source": [
    "\n",
    "\n",
    "# More test code.\n",
    "# This is used to process and extract references from downloaded pdfs.\n",
    "# It helps to check how the parsing code is identifying types of citations, \n",
    "# before actual parsing occurs.\n",
    "def test_ref_type(file):\n",
    "    f = os.path.join(file)\n",
    "    text = textract.process(f).decode('utf-8')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # regex = r'(?:[A-Zvd][A-Za-zá\\-]+)\\,\\ (?:[A-Z]\\.\\ ?)+'\n",
    "    # regex = r'((?:(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)[^\\.]*\\.\\ [^\\.]*\\,\\ \\d{4}\\.)'\n",
    "    # pattern = re.compile(regex)\n",
    "    # iterator = re.findall(pattern, text)\n",
    "    # ic(file)\n",
    "    # for i in iterator:\n",
    "    #     ic(i)\n",
    "    refs_local, ref_type = gen_refs_multiple_pass(text)\n",
    "    # refs_local = gen_refs_with_pages(text)\n",
    "    # refs_local = gen_refs_end_year_firstname_first(text)\n",
    "    # refs_local = gen_refs_end_year_firstname_last(text)\n",
    "    # refs_local = gen_refs_with_pages_conjoined(text)\n",
    "    # refs_local = gen_refs_expanded_name(text)\n",
    "    \n",
    "    # refs_local = gen_refs_text_brackets(text)\n",
    "    # ref_type = 1\n",
    "    ic(file)\n",
    "    ic(ref_type)\n",
    "    ic(len(refs_local))\n",
    "    # ic(refs_local)\n",
    "    # ic(text)\n",
    "\n",
    "\n",
    "\n",
    "# test_ref_type('./refs/2106.12139.pdf')\n",
    "# test_ref_type('./refs/download/A compact embedding for facial expression similarity.pdf')\n",
    "# test_ref_type('./refs/download/Central similarity quantization for efficient image and video retrieval.pdf')\n",
    "# test_ref_type('./refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf')\n",
    "# test_ref_type('./refs/download/Efficientnet: Rethinking model scaling for convolutional neural networks.pdf')\n",
    "# test_ref_type('./refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf')\n",
    "# test_ref_type('./refs/download/Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.pdf')\n",
    "# test_ref_type('./refs/download/Fashionista: A fashion-aware graphical system for exploring visually similar items.pdf')\n",
    "# test_ref_type('./refs/download/Hashnet: Deep learning to hash by continuation.pdf')\n",
    "# test_ref_type('./refs/download/Imagenet: A large-scale hierarchical image database.pdf')\n",
    "# test_ref_type('./refs/download/Partnet: A large-scale benchmark for finegrained and hierarchical part-level 3d object understanding.pdf')\n",
    "# test_ref_type('./refs/download/Shapenet: An information-rich 3d model repository.pdf')\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "source": [
    "\n",
    "\n",
    "# Attempt to locate references on Arxiv, and save any found files as pdfs\n",
    "def download_from_arxiv(titles, directory):\n",
    "    counter = 0\n",
    "    for title in titles:\n",
    "\n",
    "        # Check if the string is empty\n",
    "        if len(title) == 0:\n",
    "            continue\n",
    "\n",
    "        f = make_up_file_name(title, directory)\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "        # If the file already exists, \n",
    "        if not os.path.isfile(f):\n",
    "            \n",
    "            # Search for the title on Arxiv\n",
    "            r = arxiv_results(title)\n",
    "\n",
    "            # Create abridged versions\n",
    "            title_stem = title\n",
    "            title_no_colon = title\n",
    "            if title.find(':') > -1:\n",
    "                title_stem = title[:title.index(':')]\n",
    "                title_no_colon = title[:title.index(':')] + title[title.index(':')+1:]\n",
    "            \n",
    "            # Did we find a result?\n",
    "            has_entry = test_element(r)\n",
    "\n",
    "            # Remove the semi-colon - seems to confuse Arxiv API\n",
    "            if not has_entry:\n",
    "                r = arxiv_results(title_no_colon)\n",
    "                has_entry = test_element(r)\n",
    "\n",
    "            # Remove everything after the semi-colon\n",
    "            if not has_entry:\n",
    "                r = arxiv_results(title_stem)\n",
    "                has_entry = test_element(r)\n",
    "\n",
    "            print(counter, title)\n",
    "            if has_entry:\n",
    "                extract_and_save_pdf_from_atom(r, title, title_no_colon, title_stem, False)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "source": [
    "\n",
    "titles = references['title']\n",
    "for t in titles:\n",
    "    print(str(t))\n",
    "# download_from_arxiv(titles, './refs/download')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Deep learning for visual understanding: A review\n",
      "Deep convolutional neural networks for image classification: A comprehensive review\n",
      "Imagenet: A large-scale hierarchical image database\n",
      "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations\n",
      "\n",
      "3d shapenets: A deep representation for volumetric shapes\n",
      "Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models\n",
      "Benchmark Datasets for Fault Detection and Classification in Sensor Data\n",
      "A style-based generator architecture for generative adversarial networks\n",
      "A compact embedding for facial expression similarity\n",
      "Partnet: A large-scale benchmark for finegrained and hierarchical part-level 3d object understanding\n",
      "3d object representations for fine-grained categorization\n",
      "Fashionista: A fashion-aware graphical system for exploring visually similar items\n",
      "Fashionai: A hierarchical dataset for fashion understanding\n",
      "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\n",
      "\n",
      "Fashion-gen: The generative fashion dataset and challenge\n",
      "Very deep convolutional networks for large-scale image recognition\n",
      "Going deeper with convolutions\n",
      "Deep residual learning for image recognition\n",
      "Densely connected convolutional networks\n",
      "Efficientnet: Rethinking model scaling for convolutional neural networks\n",
      "Central similarity quantization for efficient image and video retrieval\n",
      "Hashnet: Deep learning to hash by continuation\n",
      "\n",
      "Incomplete multi-modal visual data grouping\n",
      "Doubly Aligned Incomplete Multi-view Clustering\n",
      "Incomplete Multiview Spectral Clustering With Adaptive Graph Learning\n",
      "Anchors bring ease: An embarrassingly simple approach to partial multi-view clustering\n",
      "A comparison of extrinsic clustering evaluation metrics based on formal constraints\n",
      "Adaptive structure discovery for multimedia analysis using multiple features\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "source": [
    "d = './refs/download'\n",
    "# d = './refs'\n",
    "\n",
    "all_freqs = {**freqs}\n",
    "all_references = references.copy()\n",
    "for file in os.listdir(d):\n",
    "    # if file.endswith(\"Fashionista: A fashion-aware graphical system for exploring visually similar items.pdf\"):\n",
    "    # if file.endswith(\"A compact embedding for facial expression similarity.pdf\"):\n",
    "    # if file.endswith(\"Fashion-gen: The generative fashion dataset and challenge.pdf\"):\n",
    "    # if file.endswith(\"Partnet: A large-scale benchmark for finegrained and hierarchical part-level 3d object understanding.pdf\"):\n",
    "    # if file.endswith(\"2106.12139.pdf\"):\n",
    "    # if file.endswith(\"Shapenet: An information-rich 3d model repository.pdf\"):    \n",
    "    # if file.endswith(\"Imagenet: A large-scale hierarchical image database.pdf\"):    \n",
    "    if file.endswith(\".pdf\") and file.find('Deepfashion') == -1:\n",
    "        f = os.path.join(d, file)\n",
    "        ic(f)\n",
    "        try:\n",
    "            text_local = textract.process(f).decode('utf-8')\n",
    "            text_local = text_local.replace('\\n', ' ')\n",
    "            all_freqs = word_frequencies_dist(text_local, all_freqs)\n",
    "            refs_local, ref_type  = gen_refs_multiple_pass(text_local)\n",
    "\n",
    "            ic(len(refs_local))\n",
    "            ic(ref_type)\n",
    "            counter = 0\n",
    "            for r in refs_local:\n",
    "                counter = counter + 1\n",
    "\n",
    "                # if counter > 3:\n",
    "                #     break\n",
    "                # ic(r)\n",
    "\n",
    "                # authors, title, journal, publisher, volume, year, pages = parse_ref1(r)\n",
    "                # ic(authors)\n",
    "                # ic(title)\n",
    "                # ic(journal)\n",
    "                # ic(publisher)\n",
    "                # ic(volume)\n",
    "                # ic(year)\n",
    "                # ic(pages)\n",
    "\n",
    "                all_references = all_references.append(parse_ref_and_makedataframe(r, ref_type, f, ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            ic(e)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| f: ('./refs/download/Fashionista: A fashion-aware graphical system for exploring '\n",
      "        'visually similar items.pdf')\n",
      "ic| len(refs_local): 7\n",
      "ic| ref_type: 1\n",
      "ic| f: './refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf'\n",
      "ic| len(refs_local): 31\n",
      "ic| ref_type: 2\n",
      "ic| f: ('./refs/download/Central similarity quantization for efficient image and '\n",
      "        'video retrieval.pdf')\n",
      "ic| len(refs_local): 44\n",
      "ic| ref_type: 3\n",
      "ic| f: './refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf'\n",
      "ic| len(refs_local): 0\n",
      "ic| ref_type: 1\n",
      "ic| f: ('./refs/download/Partnet: A large-scale benchmark for finegrained and '\n",
      "        'hierarchical part-level 3d object understanding.pdf')\n",
      "ic| len(refs_local): 43\n",
      "ic| ref_type: 1\n",
      "ic| f: ('./refs/download/Efficientnet: Rethinking model scaling for convolutional '\n",
      "        'neural networks.pdf')\n",
      "ic| len(refs_local): 52\n",
      "ic| ref_type: 2\n",
      "ic| f: './refs/download/A compact embedding for facial expression similarity.pdf'\n",
      "ic| len(refs_local): 52\n",
      "ic| ref_type: 3\n",
      "ic| f: ('./refs/download/Fashion-mnist: a novel image dataset for benchmarking '\n",
      "        'machine learning algorithms.pdf')\n",
      "ic| len(refs_local): 5\n",
      "ic| ref_type: 2\n",
      "ic| f: './refs/download/Imagenet: A large-scale hierarchical image database.pdf'\n",
      "ic| len(refs_local): 9\n",
      "ic| ref_type: 1\n",
      "ic| f: './refs/download/Hashnet: Deep learning to hash by continuation.pdf'\n",
      "ic| len(refs_local): 41\n",
      "ic| ref_type: 1\n",
      "ic| f: './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
      "ic| len(refs_local): 36\n",
      "ic| ref_type: 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "source": [
    "all_references.to_csv('data/all_references.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Language processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "source": [
    "# Generate the word cloud from the seed file frequencies.\n",
    "wc(freqs, file_name)\n",
    "\n",
    "# Generate the word cloud from frequencies of all downloaded files.\n",
    "wc(all_freqs, 'all_freqs')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "top_dist(freqs, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 95\n",
      "industrial 92\n",
      "good 81\n",
      "view 67\n",
      "patentnet 61\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "source": [
    "top_dist(all_freqs, 10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 495\n",
      "model 309\n",
      "part 283\n",
      "hash 280\n",
      "method 267\n",
      "learning 262\n",
      "category 261\n",
      "dataset 256\n",
      "shape 249\n",
      "data 247\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('patents-ai': conda)"
  },
  "interpreter": {
   "hash": "9492b3f3ffaca3e3deebd53b740a5e3b3a42fa9584de750d11d509ba01fba996"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
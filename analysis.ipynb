{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# conda install pytorch torchvision torchaudio -c pytorch\n",
    "# pip install transformers\n",
    "# pip install nltk\n",
    "# pip install numpy \n",
    "# pip install pandas\n",
    "#pip install wordcloud\n",
    "# conda install matplotlib\n",
    "# pip install scholarly\n",
    "#pip install icecream"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import textract\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from scholarly import scholarly\n",
    "from icecream import ic"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/liam/anaconda3/envs/patents-ai/lib/python3.9/site-packages/fake_useragent/utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"/Users/liam/anaconda3/envs/patents-ai/lib/python3.9/site-packages/fake_useragent/utils.py\", line 99, in get_browsers\n",
      "    html = html.split('<table class=\"w3-table-all notranslate\">')[1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Preparatory steps\n",
    "nltk.download('stopwords')\n",
    "full_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "full_stop_words.add('b')\n",
    "full_stop_words.add('c')\n",
    "full_stop_words.add('d')\n",
    "full_stop_words.add('e')\n",
    "full_stop_words.add('f')\n",
    "full_stop_words.add('g')\n",
    "full_stop_words.add('h')\n",
    "full_stop_words.add('j')\n",
    "full_stop_words.add('k')\n",
    "full_stop_words.add('l')\n",
    "full_stop_words.add('m')\n",
    "full_stop_words.add('n')\n",
    "full_stop_words.add('p')\n",
    "full_stop_words.add('q')\n",
    "full_stop_words.add('r')\n",
    "full_stop_words.add('u')\n",
    "full_stop_words.add('v')\n",
    "full_stop_words.add('x')\n",
    "full_stop_words.add('w')\n",
    "full_stop_words.add('y')\n",
    "full_stop_words.add('z')\n",
    "full_stop_words.add('pp')\n",
    "full_stop_words.add('et')\n",
    "full_stop_words.add('al')\n",
    "full_stop_words.add('ha')\n",
    "full_stop_words.add('li')\n",
    "full_stop_words.add('sij')\n",
    "full_stop_words.add('arxiv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/liam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "source": [
    "# Common functions for parsing references from plain text\n",
    "\n",
    "def re_matches(text, regex):\n",
    "    iterator = re.findall(regex, text)\n",
    "\n",
    "    refs = []\n",
    "    for match in iterator:\n",
    "        refs.append(match) \n",
    "\n",
    "    # print(len(refs))\n",
    "    return refs\n",
    "\n",
    "def re_digit():\n",
    "    return r'\\[\\d*\\]\\ '\n",
    "\n",
    "def re_first_first():\n",
    "    return r'(?:(?:\\-?[A-Z]\\.\\ )+(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+\\,?\\ ?)+)'\n",
    "\n",
    "def re_first_first_multiple():\n",
    "    return '[^\\,]\\ (?:' + re_first_first() +'+and\\ )?'\n",
    "\n",
    "def re_first_last():\n",
    "    return r'(?:(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+\\,?\\ ?)'\n",
    "\n",
    "def re_first_last_multiple():\n",
    "    return '(?:' + re_first_last() +'+\\,?and\\ )'\n",
    "\n",
    "\n",
    "def re_year():\n",
    "    return r'\\(?(?:\\d{4})\\)?'\n",
    "\n",
    "def re_trailing_digit():\n",
    "    return r'(?:[\\d\\ \\,]+\\ )?'\n",
    "\n",
    "\n",
    "def re_in_journal():\n",
    "    return r'(?:(?:in)?\\ [^\\,]+\\,\\ )'\n",
    "\n",
    "def re_pages():\n",
    "    return r'(?:\\ (?:pp\\.\\ )?\\d+\\-?\\d+)?'\n",
    "\n",
    "\n",
    "# 3\n",
    "# Example: A compact embedding for facial expression similarity.pdf\n",
    "def gen_refs_with_pages_conjoined(text):\n",
    "    r = ( \n",
    "        # re_digit() \n",
    "        r'('\n",
    "        + re_first_first_multiple() \n",
    "        + re_first_first()  \n",
    "        + r'\\.[^\\[]*?\\,\\ ' \n",
    "        + re_year() \n",
    "        + r'\\.' \n",
    "        + re_trailing_digit() \n",
    "        + r')'\n",
    "        )\n",
    "    regex = re.compile(r)\n",
    "    return re_matches(text, regex)\n",
    "\n",
    "# 0\n",
    "# Example: 2106.12139.pdf\n",
    "def gen_refs_with_pages(text):\n",
    "    regex = re.compile(r'\\[\\d*\\]\\ ((?:(?:\\-?[A-Z]\\.\\ )+(?:[A-Zvd][A-Za-z]+\\,?\\ )+)+[^\\[]*\\(?(?:\\d{4})\\)?(?:\\ pp\\.\\ \\d+\\-?\\d+)?\\.)')\n",
    "    r = ( \n",
    "        re_digit() \n",
    "        + r'('\n",
    "        + re_first_first()  \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        + re_year() \n",
    "        + re_pages()\n",
    "        + r')'\n",
    "        + r'\\.?'\n",
    "        # + re_trailing_digit() \n",
    "        )\n",
    "    regex = re.compile(r)\n",
    "    return re_matches(text, regex)\n",
    "\n",
    "# 1\n",
    "# Example: Fashionista: A fashion-aware graphical system for exploring\n",
    "def gen_refs_end_year_firstname_first(text):\n",
    "    # regex = r'([A-Z]\\.\\ ?)+[A-Z][A-Za-z\\-]+\\,\\ (?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "    r = ( \n",
    "        re_digit() \n",
    "        + r'('\n",
    "        + re_first_first_multiple() \n",
    "        + re_first_first()  \n",
    "        + r'[^\\[]*?' \n",
    "        + re_in_journal()\n",
    "        + re_year() \n",
    "        + r')'\n",
    "        + r'\\.?'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "# 2\n",
    "# Example: Fashion-gen: The generative fashion dataset and challenge.pdf\n",
    "def gen_refs_end_year_firstname_last(text):\n",
    "    regex = r'((?:(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)[^\\.]*\\.\\ [^\\.]*\\,\\ \\d{4}\\.)'\n",
    "    r = ( \n",
    "        r'('\n",
    "        + re_first_last_multiple() \n",
    "        # + re_first_last()  \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        # + re_pages()\n",
    "        + re_year() \n",
    "        + r'\\.'\n",
    "        + r')'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "# 2\n",
    "# Example: Doubly Aligned Incomplete Multi-view Clustering\n",
    "def gen_refs_text_brackets(text):\n",
    "    r = ( \n",
    "        r'\\[(?:\\d+)\\]\\ '\n",
    "        + r'('\n",
    "        # + re_first_first_multiple() \n",
    "        # + re_first_first()  \n",
    "        + r'[^\\[]*?\\,?\\ ' \n",
    "        + re_year() \n",
    "        + r')'\n",
    "        + r'\\.?'\n",
    "        )\n",
    "    regex = re.compile(r)    \n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_end_year(text):\n",
    "    regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_end_year_with_text_brackets(text):\n",
    "    regex = r'\\[(?:[A-Za-z\\ ]*\\.?\\,\\ \\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_end_year_with_number_brackets(text):\n",
    "    regex = r'\\[(?:\\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "    \n",
    "def gen_refs_end_pages(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    regex = r'\\[\\d*\\]\\ [^[]*pp\\.\\ \\d+\\.\\ '\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_multiple_pass(text):\n",
    "    refs = gen_refs_text_brackets(text)\n",
    "    ref_type = 4\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_with_pages_conjoined(text)\n",
    "        ref_type = 3\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_year_firstname_first(text)\n",
    "        ref_type = 1\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_with_pages(text)\n",
    "        ref_type = 0\n",
    "    if len(refs) == 0:\n",
    "        # refs = gen_refs_end_year(text)\n",
    "        refs = gen_refs_end_year_firstname_last(text)\n",
    "        ref_type = 2\n",
    "    # if len(refs) == 0:\n",
    "    #     refs = gen_refs_end_year_with_text_brackets(text)\n",
    "    #     ref_type = 3\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_year_with_number_brackets(text)\n",
    "        ref_type = 4\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_text_brackets(text)\n",
    "        ref_type = 5    \n",
    "    return refs, ref_type\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "# Common functions for extracting references parts\n",
    "\n",
    "def make_dataframe(authors, title, journal, publisher, volume, year, pages, ref, source_file, source_title):\n",
    "    return pd.DataFrame([[(', ').join(authors), title, journal, publisher, volume, year, pages, ref, source_file, source_title]], \n",
    "                        columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "# Deprecated\n",
    "def parse_ref00(ref, source_file, source_title):\n",
    "    segments = ref.split(',')\n",
    "    authors = []\n",
    "    title = ''\n",
    "    journal = ''\n",
    "    publisher = ''\n",
    "    volume = ''\n",
    "    year = ''\n",
    "    pages = ''\n",
    "    finished_authors = False\n",
    "    author_count = 0\n",
    "    for count, value in enumerate(segments):\n",
    "        if value.find('.') > -1 and finished_authors == False:\n",
    "            authors.append(value.strip())\n",
    "            author_count = author_count + 1\n",
    "        elif value.find('.') == -1 and count < author_count + 2:\n",
    "            if finished_authors == False:\n",
    "                finished_authors = True\n",
    "                title = value.strip()\n",
    "            else:\n",
    "                journal = value.strip()\n",
    "        elif count == len(segments) - 1 and author_count + 2 == len(segments) - 1:\n",
    "            match = re.search(r'(.*)\\((\\d\\d\\d\\d)\\)(.*)', value)\n",
    "            if match is not None:\n",
    "                volume = match.groups()[0]\n",
    "                year = match.groups()[1]\n",
    "                pages = match.groups()[2]\n",
    "        # For when year and pages are comma-separated\n",
    "        elif count == len(segments) - 2 and author_count + 3 == len(segments) - 1:\n",
    "            year = value\n",
    "        elif count == len(segments) - 1 and author_count + 3 == len(segments) - 1:\n",
    "            pages = value\n",
    "        # For when volume / publisher, year and pages are comma-separated\n",
    "        elif count == len(segments) - 3 and author_count + 4 == len(segments) - 1:\n",
    "            if value.isdigit():\n",
    "                volume = value\n",
    "            else:\n",
    "                publisher = value\n",
    "        elif count == len(segments) - 2 and author_count + 4 == len(segments) - 1:\n",
    "            year = value\n",
    "        elif count == len(segments) - 1 and author_count + 4 == len(segments) - 1:\n",
    "            pages = value\n",
    "    ic(authors)\n",
    "    return make_dataframe(authors, title, journal, publisher, volume, year, pages, ref, source_file, source_title)\n",
    "\n",
    "def _parse_ref_internals(ref, citation_pattern, authors_pattern, mapping = {\n",
    "        'first_authors': 0,\n",
    "        'last_author': 1,\n",
    "        'title': 2,\n",
    "        'journal': 3,\n",
    "        'publisher': 4,\n",
    "        'year': 5,\n",
    "        'pages': 6\n",
    "    }):\n",
    "    all_authors = []\n",
    "    title = ''\n",
    "    journal = ''\n",
    "    publisher = ''\n",
    "    volume = ''\n",
    "    year = ''\n",
    "    pages = ''\n",
    "\n",
    "    iterator = re.findall(citation_pattern, ref)\n",
    "    for i in iterator:\n",
    "        first_authors = i[mapping['first_authors']]\n",
    "        last_author = i[mapping['last_author']].strip()\n",
    "        title = i[mapping['title']]\n",
    "        journal = i[mapping['journal']]\n",
    "        publisher = i[mapping['publisher']]\n",
    "        year = i[mapping['year']]\n",
    "        pages = i[mapping['pages']]\n",
    "        if first_authors != '':\n",
    "            authors = re.findall(authors_pattern, first_authors)\n",
    "            for a in authors:\n",
    "                all_authors.append(a)\n",
    "        all_authors.append(last_author)\n",
    "    # ic(all_authors)\n",
    "    # ic(title)\n",
    "    # ic(journal)\n",
    "    # ic(publisher)\n",
    "    # ic(year)\n",
    "    # ic(pages)\n",
    "    return all_authors, title, journal, publisher, volume, year, pages\n",
    "\n",
    "def parse_ref0(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\ \\-]+\\,\\ ))*)' \n",
    "        + r'([^\\,]*)\\,\\ ([^\\,]*)\\,\\ (?:([^\\,]*)\\,\\ )?(?:(\\d+)\\ )?' \n",
    "        + r'\\(?(\\d{4})\\)?\\,?\\ ?' \n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-?\\d+)?')\n",
    "    authors_pattern = re.compile(\n",
    "        r'((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\ \\-]+))')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "\n",
    "def parse_ref1(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.\\ )+(?:[A-Zvd][A-Za-zá\\-\\ ]+\\ ?)\\,?\\ )*)and\\ ((?:\\-?[A-Zvd]\\.\\ )+(?:[A-Zvd][A-Za-zá\\-\\ ]+\\ ?)\\,\\ )' \n",
    "        + r'([^\\,]*)\\,.\\ ([^\\,\\.]*)\\,\\ (?:([^\\,]*)\\,\\ )?' \n",
    "        + r'(\\d{4})\\,?\\ ?' \n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-?\\d+)?')\n",
    "    authors_pattern = re.compile(r'(?:\\-?[A-Z]\\.\\ )+[A-Zvd][A-Za-zá\\-\\ ]+')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "\n",
    "\n",
    "def parse_ref2(ref):\n",
    "    # citation_pattern = re.compile(r'((?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?([A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)([^\\.]*)\\.\\ (.*)\\,\\ (?:([^\\,]*)\\,\\ )?(\\d{4})\\,?\\ ?(?:pp\\.\\ )?(\\d+\\-?\\d+)?')\n",
    "    \n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:[A-Zvd][A-Za-z\\-\\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?'\n",
    "        + r'([A-Zvd][A-Za-za\\-\\u0080-\\uFFFF]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)' \n",
    "        + r'([^\\.]*)' # title\n",
    "        + r'\\.\\ '\n",
    "        + r'((?:[^\\,]*))?'  # journal\n",
    "        + r'(?:\\,\\ (.*(?:pp\\.\\ |d+\\(d+\\)\\:)?\\d+(?:.\\d*)?))?' # pages \n",
    "        + r'(\\.\\ (?:[^\\,]*))?' # publisher\n",
    "        + r'\\,\\ '\n",
    "        + r'(\\d{4})'\n",
    "        # + r'\\.'\n",
    "        )\n",
    "    authors_pattern = re.compile(\n",
    "        r'(?:[A-Zvd][A-Za-za\\-\\u0080-\\uFFFF]+)+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern, mapping = {\n",
    "        'first_authors': 0,\n",
    "        'last_author': 1,\n",
    "        'title': 2,\n",
    "        'journal': 3,\n",
    "        'pages': 4,\n",
    "        'publisher': 5,\n",
    "        'year': 6\n",
    "    })\n",
    "\n",
    "def parse_ref3(ref):\n",
    "    citation_pattern = re.compile(\n",
    "        r'((?:(?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\ \\-]+\\,\\ ))*)and\\ ((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\ \\-]+\\.))?' \n",
    "        + r'([^\\,]*)\\,\\ ([^\\,]*)\\,\\ (?:([^\\,]*)\\,\\ )?' \n",
    "        + r'(?:(\\d+)\\ )?' \n",
    "        + r'\\(?(\\d{4})\\)?\\,?\\ ?' \n",
    "        + r'(?:pp\\.\\ )?(\\d+\\-?\\d+)?')\n",
    "    authors_pattern = re.compile(r'((?:\\-?[A-Z]\\.)+\\ (?:[A-Zvd][A-Za-z\\ \\-]+))')\n",
    "\n",
    "    return _parse_ref_internals(ref, citation_pattern, authors_pattern)\n",
    "\n",
    "\n",
    "\n",
    "def parse_ref(ref, ref_type):\n",
    "    if ref_type == 0:\n",
    "        return parse_ref0(ref)\n",
    "    elif ref_type == 1:\n",
    "        return parse_ref1(ref)\n",
    "    elif ref_type == 2:\n",
    "        return parse_ref2(ref)\n",
    "    elif ref_type == 3:\n",
    "        return parse_ref3(ref)\n",
    "    else:\n",
    "        return parse_ref0(ref)\n",
    "\n",
    "\n",
    "def parse_ref_and_makedataframe(ref, ref_type, source_file, source_title):\n",
    "    authors, title, journal, publisher, volume, year, pages = parse_ref(ref, ref_type)\n",
    "    return make_dataframe(authors, title, journal, publisher, volume, year, pages, ref, source_file, source_title)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Other common functions\n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def remove_set_from_dict(s, d):\n",
    "    for t in s:\n",
    "        if t in d:\n",
    "            del d[t]\n",
    "    return d\n",
    "    \n",
    "def word_frequencies(text):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    frequencies = {}\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "def word_frequencies_dist(text, frequencies):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "\n",
    "def wc(freqs, file_name):\n",
    "    wordcloud = WordCloud(background_color=\"white\", width=600, height=600, max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "    #wordcloud.generate(long_string)\n",
    "    wordcloud.generate_from_frequencies(freqs)\n",
    "    wordcloud.to_file(f'data/word-cloud-{file_name}.png')\n",
    "    wordcloud.to_image()\n",
    "\n",
    "\n",
    "\n",
    "def arxiv_results(title):\n",
    "    t =  urllib.parse.quote_plus(title)\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:'+t+'&start=0&max_results=1'\n",
    "    data = urllib.request.urlopen(url)\n",
    "    results = data.read().decode('utf-8')\n",
    "    return results\n",
    "\n",
    "def make_up_file_name(file_name):\n",
    "    return 'refs/download/' +  file_name + '.pdf'\n",
    "\n",
    "def extract_and_save_pdf_from_atom(atom_xml, title, title_no_colon, title_stem, pass_through=False):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    for e in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        t = e.find('{http://www.w3.org/2005/Atom}title')\n",
    "        link = e.find('{http://www.w3.org/2005/Atom}link')\n",
    "        if t.text.lower().find(title_stem.lower()) == 0:\n",
    "            for link in e.findall('{http://www.w3.org/2005/Atom}link'):\n",
    "                # Title must match and link must be a pdf\n",
    "                if 'type' in link.attrib and link.attrib['type'] == 'application/pdf':\n",
    "                    u  = link.attrib['href']\n",
    "                    print(u, title)\n",
    "                    if not pass_through:\n",
    "                        urllib.request.urlretrieve(u, make_up_file_name(title))\n",
    "\n",
    "def test_element(atom_xml):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
    "    if entry is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def top_dist(freqs, n):\n",
    "    counter = 0\n",
    "    for w in sorted(freqs, key = freqs.get, reverse = True):\n",
    "        counter = counter + 1\n",
    "        print(w, freqs[w])\n",
    "        if counter == n:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "file_name = '2106.12139.pdf'\n",
    "title = 'PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database'\n",
    "\n",
    "\n",
    "text = textract.process(\"refs/\" + file_name).decode('utf-8')\n",
    "freqs = word_frequencies(text)\n",
    "\n",
    "references = pd.DataFrame(columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "refs = gen_refs_multiple_pass(text)\n",
    "for r in refs:\n",
    "    r = r.replace('\\n', ' ')\n",
    "    references = references.append(parse_ref_and_makedataframe(r, 0, file_name, title))\n",
    "\n",
    "references.to_csv('data/references.csv')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db09a9cf00f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_refs_multiple_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_ref_and_makedataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# Generate the word cloud from the seed file frequencies.\n",
    "wc(freqs, file_name)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# Test code\n",
    "\n",
    "# t = references['title'].iloc[7]\n",
    "# print(t)\n",
    "# r = arxiv_results(t.strip())\n",
    "# print(r)\n",
    "# # ET.tostring(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Attempt to locate references on Arxiv, and save\n",
    "counter = 0\n",
    "for title in references['title']:\n",
    "    f = make_up_file_name(title)\n",
    "    counter = counter + 1\n",
    "    if not os.path.isfile(f):\n",
    "        r = arxiv_results(title)\n",
    "        title_stem = title\n",
    "        title_no_colon = title\n",
    "        if title.find(':') > -1:\n",
    "            title_stem = title[:title.index(':')]\n",
    "            title_no_colon = title[:title.index(':')] + title[title.index(':')+1:]\n",
    "        \n",
    "        has_entry = test_element(r)\n",
    "\n",
    "        # Remove the semi-colon - seems to confuse Arxiv API\n",
    "        if not has_entry:\n",
    "            r = arxiv_results(title_no_colon)\n",
    "            has_entry = test_element(r)\n",
    "\n",
    "        # Remove everything after the semi-colon\n",
    "        if not has_entry:\n",
    "            r = arxiv_results(title_stem)\n",
    "            has_entry = test_element(r)\n",
    "\n",
    "        print(counter, title)\n",
    "        if has_entry:\n",
    "            extract_and_save_pdf_from_atom(r, title, title_no_colon, title_stem, False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Deep learning for visual understanding: A review\n",
      "2 Deep convolutional neural networks for image classification: A comprehensive review\n",
      "4 Microsoft coco: Common objects in context\n",
      "5 Deepfashion: Powering robust clothes recognition and retrieval with rich annotations\n",
      "http://arxiv.org/pdf/1901.07973v1 Deepfashion: Powering robust clothes recognition and retrieval with rich annotations\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cb7ada3e6ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_entry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mextract_and_save_pdf_from_atom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_no_colon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_stem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-4e56010d4d80>\u001b[0m in \u001b[0;36mextract_and_save_pdf_from_atom\u001b[0;34m(atom_xml, title, title_no_colon, title_stem, pass_through)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpass_through\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_up_file_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "source": [
    "\n",
    "def test_ref_type(file):\n",
    "    f = os.path.join(file)\n",
    "    text = textract.process(f).decode('utf-8')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # regex = r'(?:[A-Zvd][A-Za-zá\\-]+)\\,\\ (?:[A-Z]\\.\\ ?)+'\n",
    "    # regex = r'((?:(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?(?:[A-Zvd][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)[^\\.]*\\.\\ [^\\.]*\\,\\ \\d{4}\\.)'\n",
    "    # pattern = re.compile(regex)\n",
    "    # iterator = re.findall(pattern, text)\n",
    "    # ic(file)\n",
    "    # for i in iterator:\n",
    "    #     ic(i)\n",
    "    refs_local, ref_type = gen_refs_multiple_pass(text)\n",
    "    # refs_local = gen_refs_text_brackets(text)\n",
    "    # ref_type = 1\n",
    "    ic(file)\n",
    "    ic(ref_type)\n",
    "    ic(len(refs_local))\n",
    "    # ic(refs_local)\n",
    "    # ic(text)\n",
    "\n",
    "\n",
    "\n",
    "test_ref_type('./refs/2106.12139.pdf')\n",
    "test_ref_type('./refs/download/A compact embedding for facial expression similarity.pdf')\n",
    "test_ref_type('./refs/download/Central similarity quantization for efficient image and video retrieval.pdf')\n",
    "test_ref_type('./refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf')\n",
    "test_ref_type('./refs/download/Efficientnet: Rethinking model scaling for convolutional neural networks.pdf')\n",
    "test_ref_type('./refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf')\n",
    "test_ref_type('./refs/download/Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.pdf')\n",
    "test_ref_type('./refs/download/Fashionista: A fashion-aware graphical system for exploring visually similar items.pdf')\n",
    "test_ref_type('./refs/download/Hashnet: Deep learning to hash by continuation.pdf')\n",
    "test_ref_type('./refs/download/Imagenet: A large-scale hierarchical image database.pdf')\n",
    "test_ref_type('./refs/download/Partnet: A large-scale benchmark for finegrained and hierarchical part-level 3d object understanding.pdf')\n",
    "test_ref_type('./refs/download/Shapenet: An information-rich 3d model repository.pdf')\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| file: './refs/2106.12139.pdf'\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 41\n",
      "ic| file: './refs/download/A compact embedding for facial expression similarity.pdf'\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 36\n",
      "ic| file: ('./refs/download/Central similarity quantization for efficient image and '\n",
      "           'video retrieval.pdf')\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 31\n",
      "ic| file: './refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf'\n",
      "ic| ref_type: 5\n",
      "ic| len(refs_local): 0\n",
      "ic| file: ('./refs/download/Efficientnet: Rethinking model scaling for convolutional '\n",
      "           'neural networks.pdf')\n",
      "ic| ref_type: 2\n",
      "ic| len(refs_local): 50\n",
      "ic| file: './refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf'\n",
      "ic| ref_type: 2\n",
      "ic| len(refs_local): 29\n",
      "ic| file: ('./refs/download/Fashion-mnist: a novel image dataset for benchmarking '\n",
      "           'machine learning algorithms.pdf')\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 2\n",
      "ic| file: ('./refs/download/Fashionista: A fashion-aware graphical system for exploring '\n",
      "           'visually similar items.pdf')\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 8\n",
      "ic| file: './refs/download/Hashnet: Deep learning to hash by continuation.pdf'\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 45\n",
      "ic| file: './refs/download/Imagenet: A large-scale hierarchical image database.pdf'\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 13\n",
      "ic| file: ('./refs/download/Partnet: A large-scale benchmark for finegrained and '\n",
      "           'hierarchical part-level 3d object understanding.pdf')\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 46\n",
      "ic| file: './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
      "ic| ref_type: 4\n",
      "ic| len(refs_local): 37\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "source": [
    "d = './refs/download'\n",
    "\n",
    "# d = './refs'\n",
    "all_freqs = {**freqs}\n",
    "all_references = references.copy()\n",
    "for file in os.listdir(d):\n",
    "    # if file.endswith(\"Fashionista: A fashion-aware graphical system for exploring visually similar items.pdf\"):\n",
    "    # if file.endswith(\"A compact embedding for facial expression similarity.pdf\"):\n",
    "    # if file.endswith(\"Fashion-gen: The generative fashion dataset and challenge.pdf\"):\n",
    "    # if file.endswith(\"2106.12139.pdf\"):\n",
    "    if file.endswith(\".pdf\") and file.find('Deepfashion') == -1:\n",
    "        f = os.path.join(d, file)\n",
    "        ic(f)\n",
    "        try:\n",
    "            text_local = textract.process(f).decode('utf-8')\n",
    "            text_local = text_local.replace('\\n', ' ')\n",
    "            all_freqs = word_frequencies_dist(text_local, all_freqs)\n",
    "            refs_local, ref_type  = gen_refs_multiple_pass(text_local)\n",
    "\n",
    "            ic(len(refs_local))\n",
    "            ic(ref_type)\n",
    "            counter = 0\n",
    "            for r in refs_local:\n",
    "                counter = counter + 1\n",
    "                # if counter > 3:\n",
    "                #     break\n",
    "                # ic(r)\n",
    "\n",
    "                # authors, title, journal, publisher, volume, year, pages = parse_ref(r, ref_type)\n",
    "                # ic(authors)\n",
    "                # ic(title)\n",
    "                # ic(journal)\n",
    "                # ic(publisher)\n",
    "                # ic(volume)\n",
    "                # ic(year)\n",
    "                # ic(pages)\n",
    "\n",
    "                all_references = all_references.append(parse_ref_and_makedataframe(r, ref_type, f, ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            ic(e)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| f: ('./refs/download/Fashionista: A fashion-aware graphical system for exploring '\n",
      "        'visually similar items.pdf')\n",
      "ic| len(refs_local): 8\n",
      "ic| ref_type: 4\n",
      "ic| f: './refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf'\n",
      "ic| len(refs_local): 29\n",
      "ic| ref_type: 2\n",
      "ic| f: ('./refs/download/Central similarity quantization for efficient image and '\n",
      "        'video retrieval.pdf')\n",
      "ic| len(refs_local): 31\n",
      "ic| ref_type: 4\n",
      "ic| f: './refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf'\n",
      "ic| len(refs_local): 0\n",
      "ic| ref_type: 5\n",
      "ic| f: ('./refs/download/Partnet: A large-scale benchmark for finegrained and '\n",
      "        'hierarchical part-level 3d object understanding.pdf')\n",
      "ic| len(refs_local): 46\n",
      "ic| ref_type: 4\n",
      "ic| f: ('./refs/download/Efficientnet: Rethinking model scaling for convolutional '\n",
      "        'neural networks.pdf')\n",
      "ic| len(refs_local): 50\n",
      "ic| ref_type: 2\n",
      "ic| f: './refs/download/A compact embedding for facial expression similarity.pdf'\n",
      "ic| len(refs_local): 36\n",
      "ic| ref_type: 4\n",
      "ic| f: ('./refs/download/Fashion-mnist: a novel image dataset for benchmarking '\n",
      "        'machine learning algorithms.pdf')\n",
      "ic| len(refs_local): 2\n",
      "ic| ref_type: 4\n",
      "ic| f: './refs/download/Imagenet: A large-scale hierarchical image database.pdf'\n",
      "ic| len(refs_local): 13\n",
      "ic| ref_type: 4\n",
      "ic| f: './refs/download/Hashnet: Deep learning to hash by continuation.pdf'\n",
      "ic| len(refs_local): 45\n",
      "ic| ref_type: 4\n",
      "ic| f: './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
      "ic| len(refs_local): 37\n",
      "ic| ref_type: 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "source": [
    "all_references.to_csv('data/all_references.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# # f = './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
    "# f = './refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf'\n",
    "# # f = './refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf'\n",
    "# # f = './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
    "\n",
    "# t = textract.process(f).decode('utf-8')\n",
    "# print(t)\n",
    "# # t = t.replace('\\n', ' ')\n",
    "# # regex = r'\\[\\d*\\]\\ [^\\n]*\\n[^\\n]*\\d\\d\\d\\d\\.'\n",
    "# # regex = r'[A-Z][A-Za-z\\-]+\\,(.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# # regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "# # regex = r'\\ [A-Z][A-Za-z\\,\\.\\ \\-\\n]+[A-Z]\\.\\ [A-Z](?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# # regex = r'[A-Z][A-Za-z\\,\\.\\ ]+[A-Z]\\.\\ [A-Z](.\\n.)*\\1{5}.*\\d\\d\\d\\d\\.\\ '\n",
    "# # regex = r'\\[(?:[A-Za-z\\ \\.]*\\,\\ \\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "# regex = r'\\[(?:\\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "# pattern = re.compile(regex)\n",
    "# iterator = re.findall(pattern, t)\n",
    "\n",
    "# print(len(iterator))\n",
    "# refs = []\n",
    "# for match in iterator:\n",
    "#     refs.append(match) \n",
    "# print(refs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "wc(all_freqs, 'all_freqs')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "s = \"\"\"Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\n",
    "face attributes in the wild. In Proceedings of the IEEE\n",
    "International Conference on Computer Vision, pp. 3730–\n",
    "3738, 2015.\"\"\"\n",
    "\n",
    "# \\1{5}.*\\d\\d\\d\\d\\.\n",
    "# regex = r'[A-Z][A-Za-z\\,\\.\\ ]+[A-Z]\\.\\ [A-Z](?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "pattern = re.compile(regex)\n",
    "iterator = re.findall(pattern, s)\n",
    "print(iterator)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\\nface attributes in the wild. In Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 3730–\\n3738, 2015.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "top_dist(freqs, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 95\n",
      "industrial 92\n",
      "good 81\n",
      "view 67\n",
      "patentnet 61\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "top_dist(all_freqs, 1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 495\n",
      "model 309\n",
      "part 283\n",
      "hash 280\n",
      "method 267\n",
      "learning 262\n",
      "category 261\n",
      "dataset 256\n",
      "shape 249\n",
      "data 247\n",
      "network 240\n",
      "view 207\n",
      "object 190\n",
      "deep 187\n",
      "segmentation 185\n",
      "figure 181\n",
      "annotation 177\n",
      "center 162\n",
      "training 154\n",
      "different 153\n",
      "code 149\n",
      "using 146\n",
      "retrieval 135\n",
      "set 131\n",
      "result 130\n",
      "similarity 129\n",
      "hashnet 129\n",
      "scale 127\n",
      "large 126\n",
      "function 126\n",
      "table 126\n",
      "level 123\n",
      "imagenet 122\n",
      "fashion 122\n",
      "multi 122\n",
      "accuracy 119\n",
      "instance 118\n",
      "loss 116\n",
      "label 115\n",
      "recognition 114\n",
      "information 110\n",
      "datasets 110\n",
      "number 109\n",
      "bit 109\n",
      "expression 107\n",
      "ieee 105\n",
      "triplet 105\n",
      "use 103\n",
      "fine 101\n",
      "two 100\n",
      "depth 100\n",
      "based 99\n",
      "three 99\n",
      "visual 98\n",
      "semantic 98\n",
      "class 97\n",
      "computer 97\n",
      "efficientnet 97\n",
      "performance 96\n",
      "vision 96\n",
      "grained 95\n",
      "scaling 95\n",
      "hashing 95\n",
      "hierarchical 94\n",
      "page 94\n",
      "feature 93\n",
      "also 93\n",
      "point 93\n",
      "embedding 93\n",
      "industrial 92\n",
      "good 92\n",
      "similar 92\n",
      "one 92\n",
      "matrix 91\n",
      "item 89\n",
      "work 88\n",
      "conference 88\n",
      "neural 87\n",
      "space 86\n",
      "example 85\n",
      "algorithm 85\n",
      "classification 84\n",
      "show 83\n",
      "proposed 82\n",
      "incomplete 80\n",
      "top 79\n",
      "used 78\n",
      "clustering 78\n",
      "first 76\n",
      "shapenet 76\n",
      "map 76\n",
      "binary 76\n",
      "layer 76\n",
      "task 74\n",
      "benchmark 72\n",
      "liu 71\n",
      "paper 70\n",
      "better 70\n",
      "human 67\n",
      "problem 67\n",
      "video 67\n",
      "multiple 65\n",
      "template 64\n",
      "metric 63\n",
      "wang 63\n",
      "pair 63\n",
      "hamming 63\n",
      "existing 62\n",
      "chair 62\n",
      "distance 62\n",
      "query 62\n",
      "patentnet 61\n",
      "average 61\n",
      "resolution 61\n",
      "cvpr 60\n",
      "experiment 59\n",
      "provide 59\n",
      "activation 59\n",
      "node 59\n",
      "text 58\n",
      "parameter 57\n",
      "evaluation 57\n",
      "architecture 57\n",
      "score 57\n",
      "size 56\n",
      "pattern 56\n",
      "database 55\n",
      "value 54\n",
      "research 54\n",
      "system 54\n",
      "analysis 53\n",
      "huang 53\n",
      "learn 53\n",
      "row 52\n",
      "best 52\n",
      "test 52\n",
      "representation 52\n",
      "input 52\n",
      "generate 52\n",
      "baseline 52\n",
      "max 52\n",
      "detail 51\n",
      "application 50\n",
      "dimension 50\n",
      "criterion 50\n",
      "proceeding 50\n",
      "facial 50\n",
      "dhn 50\n",
      "fecnet 50\n",
      "face 49\n",
      "section 49\n",
      "comparison 49\n",
      "zhang 49\n",
      "however 48\n",
      "search 48\n",
      "resnet 48\n",
      "precision 47\n",
      "visualization 47\n",
      "many 46\n",
      "challenge 46\n",
      "respectively 46\n",
      "http 46\n",
      "partnet 46\n",
      "mnist 46\n",
      "daimc 46\n",
      "given 46\n",
      "cl 46\n",
      "product 45\n",
      "train 45\n",
      "convolutional 45\n",
      "approach 45\n",
      "defined 45\n",
      "contains 44\n",
      "well 44\n",
      "user 44\n",
      "generated 44\n",
      "csq 43\n",
      "description 43\n",
      "preprint 43\n",
      "trained 43\n",
      "within 42\n",
      "common 42\n",
      "new 42\n",
      "acm 42\n",
      "learned 42\n",
      "prediction 42\n",
      "hi 42\n",
      "optimization 42\n",
      "width 42\n",
      "design 41\n",
      "shown 41\n",
      "state 41\n",
      "penalty 41\n",
      "time 40\n",
      "art 40\n",
      "following 40\n",
      "transaction 40\n",
      "inception 40\n",
      "stage 40\n",
      "flop 40\n",
      "supervised 40\n",
      "affnet 40\n",
      "sign 40\n",
      "sample 39\n",
      "corresponding 38\n",
      "fashionista 38\n",
      "machine 38\n",
      "pairwise 38\n",
      "emotion 38\n",
      "available 37\n",
      "type 37\n",
      "compared 37\n",
      "coco 36\n",
      "understanding 36\n",
      "real 36\n",
      "lamp 36\n",
      "alignment 36\n",
      "central 36\n",
      "consistent 36\n",
      "descriptor 36\n",
      "continuous 36\n",
      "report 36\n",
      "hj 36\n",
      "structure 35\n",
      "thus 35\n",
      "propose 35\n",
      "international 35\n",
      "need 35\n",
      "present 35\n",
      "action 35\n",
      "high 34\n",
      "fig 34\n",
      "validation 34\n",
      "attribute 34\n",
      "synset 34\n",
      "concept 33\n",
      "visually 33\n",
      "may 33\n",
      "help 33\n",
      "small 33\n",
      "l2 33\n",
      "semantics 33\n",
      "convnets 33\n",
      "much 32\n",
      "world 32\n",
      "single 32\n",
      "graphic 32\n",
      "chen 32\n",
      "dimensional 32\n",
      "curve 32\n",
      "unit 32\n",
      "study 31\n",
      "wide 31\n",
      "stackgan 31\n",
      "statistic 31\n",
      "quality 30\n",
      "knowledge 30\n",
      "clothing 30\n",
      "wordnet 30\n",
      "future 30\n",
      "neighbor 30\n",
      "processing 30\n",
      "process 30\n",
      "generative 30\n",
      "quantization 30\n",
      "weight 30\n",
      "rate 30\n",
      "coarse 30\n",
      "tanh 30\n",
      "key 29\n",
      "main 29\n",
      "related 29\n",
      "annotated 29\n",
      "original 29\n",
      "find 29\n",
      "continuation 29\n",
      "log 29\n",
      "bag 29\n",
      "nu 29\n",
      "entropy 29\n",
      "miou 29\n",
      "sgn 29\n",
      "com 28\n",
      "detection 28\n",
      "see 28\n",
      "due 28\n",
      "nmf 28\n",
      "across 27\n",
      "obtain 27\n",
      "pre 27\n",
      "since 27\n",
      "interface 27\n",
      "four 27\n",
      "several 27\n",
      "weighted 27\n",
      "maximum 27\n",
      "pointnet 27\n",
      "missing 26\n",
      "among 26\n",
      "trend 26\n",
      "v2 26\n",
      "previous 26\n",
      "bottom 25\n",
      "case 25\n",
      "order 25\n",
      "per 25\n",
      "distribution 25\n",
      "mean 25\n",
      "step 25\n",
      "wa 25\n",
      "efficient 25\n",
      "generation 25\n",
      "end 25\n",
      "line 25\n",
      "hinge 25\n",
      "compound 25\n",
      "fec 25\n",
      "estimator 25\n",
      "multiview 24\n",
      "property 24\n",
      "way 24\n",
      "back 24\n",
      "hierarchy 24\n",
      "make 24\n",
      "fei 24\n",
      "lin 24\n",
      "repository 24\n",
      "global 24\n",
      "embeddings 24\n",
      "via 24\n",
      "door 24\n",
      "leaf 24\n",
      "batch 24\n",
      "hhi 24\n",
      "splitter 24\n",
      "labeled 23\n",
      "higher 23\n",
      "material 23\n",
      "right 23\n",
      "standard 23\n",
      "relationship 23\n",
      "bed 23\n",
      "subset 23\n",
      "nmi 23\n",
      "achieve 23\n",
      "fashionability 23\n",
      "capture 23\n",
      "larger 22\n",
      "left 22\n",
      "clock 22\n",
      "collection 22\n",
      "provided 22\n",
      "total 22\n",
      "compact 22\n",
      "efficiency 22\n",
      "tree 22\n",
      "objective 22\n",
      "semi 22\n",
      "gradient 22\n",
      "mesh 22\n",
      "taxonomy 22\n",
      "million 21\n",
      "intelligence 21\n",
      "particular 21\n",
      "note 21\n",
      "estimation 21\n",
      "goal 21\n",
      "random 21\n",
      "labeling 21\n",
      "b0 21\n",
      "achieves 21\n",
      "provides 21\n",
      "v1 21\n",
      "increase 21\n",
      "chang 21\n",
      "hu 21\n",
      "aligned 21\n",
      "term 21\n",
      "recent 21\n",
      "compare 21\n",
      "gini 21\n",
      "symmetry 21\n",
      "scene 20\n",
      "long 20\n",
      "ground 20\n",
      "novel 20\n",
      "advance 20\n",
      "multimedia 20\n",
      "google 20\n",
      "non 20\n",
      "synthesis 20\n",
      "ci 20\n",
      "annotator 20\n",
      "avg 20\n",
      "resource 20\n",
      "exactly 20\n",
      "university 19\n",
      "community 19\n",
      "issue 19\n",
      "cloud 19\n",
      "component 19\n",
      "focus 19\n",
      "imbalance 19\n",
      "acc 19\n",
      "various 19\n",
      "th 19\n",
      "aaai 19\n",
      "online 19\n",
      "observe 19\n",
      "definition 19\n",
      "dissimilar 19\n",
      "define 19\n",
      "e2 19\n",
      "convergence 19\n",
      "b7 19\n",
      "gpipe 19\n",
      "raters 19\n",
      "mpeg 19\n",
      "demonstrate 18\n",
      "cad 18\n",
      "field 18\n",
      "general 18\n",
      "often 18\n",
      "effort 18\n",
      "cover 18\n",
      "candidate 18\n",
      "source 18\n",
      "name 18\n",
      "currently 18\n",
      "get 18\n",
      "perform 18\n",
      "sun 18\n",
      "van 18\n",
      "low 18\n",
      "correspondence 18\n",
      "inference 18\n",
      "xi 18\n",
      "smaller 18\n",
      "coefficient 18\n",
      "nv 18\n",
      "ni 18\n",
      "mask 18\n",
      "plane 18\n",
      "side 17\n",
      "address 17\n",
      "including 17\n",
      "describe 17\n",
      "important 17\n",
      "adopt 17\n",
      "consists 17\n",
      "middle 17\n",
      "experimental 17\n",
      "img 17\n",
      "directly 17\n",
      "truth 17\n",
      "refer 17\n",
      "deng 17\n",
      "rich 17\n",
      "style 17\n",
      "szegedy 17\n",
      "le 17\n",
      "zhao 17\n",
      "joint 17\n",
      "share 17\n",
      "ranking 17\n",
      "hyper 17\n",
      "iteration 17\n",
      "l1 17\n",
      "ac 17\n",
      "geometric 17\n",
      "median 17\n",
      "tog 17\n",
      "convnet 17\n",
      "transfer 17\n",
      "scaled 17\n",
      "hik 17\n",
      "introduce 16\n",
      "patent 16\n",
      "second 16\n",
      "around 16\n",
      "therefore 16\n",
      "collect 16\n",
      "include 16\n",
      "evaluate 16\n",
      "conduct 16\n",
      "widely 16\n",
      "difference 16\n",
      "explore 16\n",
      "critical 16\n",
      "cross 16\n",
      "su 16\n",
      "yi 16\n",
      "tan 16\n",
      "like 16\n",
      "learns 16\n",
      "color 16\n",
      "bar 16\n",
      "output 16\n",
      "hinton 16\n",
      "zhou 16\n",
      "compute 16\n",
      "volume 16\n",
      "variation 16\n",
      "wild 16\n",
      "journal 16\n",
      "hat 16\n",
      "performs 16\n",
      "kernel 16\n",
      "basis 16\n",
      "orientation 16\n",
      "challenging 15\n",
      "according 15\n",
      "comprehensive 15\n",
      "ambiguity 15\n",
      "denotes 15\n",
      "setting 15\n",
      "natural 15\n",
      "ab 15\n",
      "possible 15\n",
      "cannot 15\n",
      "become 15\n",
      "adversarial 15\n",
      "zhu 15\n",
      "web 15\n",
      "still 15\n",
      "www 15\n",
      "though 15\n",
      "either 15\n",
      "retrieved 15\n",
      "cnn 15\n",
      "latent 15\n",
      "difficult 15\n",
      "recently 15\n",
      "collected 15\n",
      "flower 15\n",
      "close 15\n",
      "whole 15\n",
      "likelihood 15\n",
      "squared 15\n",
      "recall 15\n",
      "ucf101 15\n",
      "regression 15\n",
      "pvc 15\n",
      "jk 15\n",
      "knife 15\n",
      "iou 15\n",
      "zoph 15\n",
      "album 15\n",
      "hjk 15\n",
      "multilabel 14\n",
      "characteristic 14\n",
      "able 14\n",
      "introduction 14\n",
      "improve 14\n",
      "usually 14\n",
      "edu 14\n",
      "designed 14\n",
      "aim 14\n",
      "densenet 14\n",
      "sc 14\n",
      "graph 14\n",
      "wu 14\n",
      "springer 14\n",
      "yu 14\n",
      "guibas 14\n",
      "workshop 14\n",
      "rethinking 14\n",
      "without 14\n",
      "another 14\n",
      "even 14\n",
      "temporal 14\n",
      "specifically 14\n",
      "form 14\n",
      "factorization 14\n",
      "sne 14\n",
      "eccv 14\n",
      "final 14\n",
      "content 14\n",
      "vector 14\n",
      "lstm 14\n",
      "yan 14\n",
      "hadamard 14\n",
      "hence 14\n",
      "individual 14\n",
      "c0i 14\n",
      "calculate 14\n",
      "corresponds 14\n",
      "ratio 14\n",
      "itq 14\n",
      "outperforms 14\n",
      "margin 14\n",
      "computing 14\n",
      "norm 14\n",
      "error 14\n",
      "base 14\n",
      "colour 14\n",
      "histogram 14\n",
      "binarization 14\n",
      "abstract 13\n",
      "significantly 13\n",
      "detailed 13\n",
      "artificial 13\n",
      "appearance 13\n",
      "front 13\n",
      "furniture 13\n",
      "organized 13\n",
      "follows 13\n",
      "finally 13\n",
      "researcher 13\n",
      "although 13\n",
      "pose 13\n",
      "gen 13\n",
      "support 13\n",
      "described 13\n",
      "nearest 13\n",
      "take 13\n",
      "ai 13\n",
      "aware 13\n",
      "convolution 13\n",
      "yang 13\n",
      "corpus 13\n",
      "org 13\n",
      "consider 13\n",
      "index 13\n",
      "presented 13\n",
      "id 13\n",
      "randomly 13\n",
      "summary 13\n",
      "photo 13\n",
      "yes 13\n",
      "fixed 13\n",
      "gan 13\n",
      "prior 13\n",
      "encoder 13\n",
      "hidden 13\n",
      "instead 13\n",
      "optimizing 13\n",
      "sec 13\n",
      "dnnh 13\n",
      "c1 13\n",
      "equation 13\n",
      "physical 13\n",
      "bowl 13\n",
      "mug 13\n",
      "trash 13\n",
      "vase 13\n",
      "additional 13\n",
      "nasnet 13\n",
      "rater 13\n",
      "cld 13\n",
      "ehd 13\n",
      "big 12\n",
      "china 12\n",
      "author 12\n",
      "variety 12\n",
      "context 12\n",
      "optimize 12\n",
      "fewer 12\n",
      "complete 12\n",
      "associated 12\n",
      "third 12\n",
      "target 12\n",
      "easily 12\n",
      "partial 12\n",
      "block 12\n",
      "scenario 12\n",
      "conclusion 12\n",
      "language 12\n",
      "especially 12\n",
      "reference 12\n",
      "tang 12\n",
      "xiao 12\n",
      "funkhouser 12\n",
      "connected 12\n",
      "amazon 12\n",
      "modeling 12\n",
      "surface 12\n",
      "lu 12\n",
      "krizhevsky 12\n",
      "textual 12\n",
      "expert 12\n",
      "framework 12\n",
      "cnnh 12\n",
      "dh 12\n",
      "combination 12\n",
      "lq 12\n",
      "min 12\n",
      "tab 12\n",
      "alexnet 12\n",
      "representative 12\n",
      "digit 12\n",
      "shen 12\n",
      "solution 12\n",
      "mic 12\n",
      "eq 12\n",
      "edge 12\n",
      "confusion 12\n",
      "ensemble 12\n",
      "bigger 12\n",
      "sgpn 12\n",
      "manual 12\n",
      "classifier 12\n",
      "affectnet 12\n",
      "useful 12\n",
      "crammer 12\n",
      "singer 12\n",
      "technology 11\n",
      "current 11\n",
      "perspective 11\n",
      "addition 11\n",
      "dress 11\n",
      "construct 11\n",
      "next 11\n",
      "five 11\n",
      "accessory 11\n",
      "measure 11\n",
      "build 11\n",
      "could 11\n",
      "applied 11\n",
      "every 11\n",
      "obtained 11\n",
      "observation 11\n",
      "technique 11\n",
      "etc 11\n",
      "attention 11\n",
      "computation 11\n",
      "song 11\n",
      "karras 11\n",
      "simple 11\n",
      "question 11\n",
      "significant 11\n",
      "specific 11\n",
      "go 11\n",
      "basic 11\n",
      "han 11\n",
      "complexity 11\n",
      "net 11\n",
      "lee 11\n",
      "boot 11\n",
      "handle 11\n",
      "generic 11\n",
      "commonly 11\n",
      "faster 11\n",
      "proposes 11\n",
      "smooth 11\n",
      "effective 11\n",
      "discrete 11\n",
      "negative 11\n",
      "zero 11\n",
      "normalization 11\n",
      "annotate 11\n",
      "bottle 11\n",
      "root 11\n",
      "geometry 11\n",
      "shapenetcore 11\n",
      "mobile 11\n",
      "v4 11\n",
      "zalando 11\n",
      "file 11\n",
      "ovr 11\n",
      "rigid 11\n",
      "multimodal 10\n",
      "guangdong 10\n",
      "area 10\n",
      "traditional 10\n",
      "factor 10\n",
      "texture 10\n",
      "domain 10\n",
      "github 10\n",
      "least 10\n",
      "science 10\n",
      "car 10\n",
      "document 10\n",
      "mainly 10\n",
      "public 10\n",
      "range 10\n",
      "already 10\n",
      "gap 10\n",
      "short 10\n",
      "group 10\n",
      "indicates 10\n",
      "region 10\n",
      "relatively 10\n",
      "grouping 10\n",
      "xu 10\n",
      "fashionable 10\n",
      "evolution 10\n",
      "shirt 10\n",
      "considered 10\n",
      "dynamic 10\n",
      "similarly 10\n",
      "give 10\n",
      "list 10\n",
      "filter 10\n",
      "estimate 10\n",
      "people 10\n",
      "idea 10\n",
      "us 10\n",
      "functionality 10\n",
      "enable 10\n",
      "nip 10\n",
      "thomas 10\n",
      "conditional 10\n",
      "advantage 10\n",
      "display 10\n",
      "bi 10\n",
      "open 10\n",
      "unsupervised 10\n",
      "imbalanced 10\n",
      "discriminative 10\n",
      "cost 10\n",
      "guarantee 10\n",
      "di 10\n",
      "column 10\n",
      "positive 10\n",
      "fast 10\n",
      "nonnegative 10\n",
      "regularization 10\n",
      "subspace 10\n",
      "entire 10\n",
      "kb 10\n",
      "driven 10\n",
      "distinct 10\n",
      "pointcnn 10\n",
      "channel 10\n",
      "howard 10\n",
      "wij 10\n",
      "protocol 10\n",
      "namely 9\n",
      "manually 9\n",
      "subclass 9\n",
      "structural 9\n",
      "publicly 9\n",
      "modal 9\n",
      "contain 9\n",
      "last 9\n",
      "pixel 9\n",
      "far 9\n",
      "coverage 9\n",
      "rest 9\n",
      "represent 9\n",
      "cluster 9\n",
      "mutual 9\n",
      "nature 9\n",
      "full 9\n",
      "automatic 9\n",
      "luo 9\n",
      "robust 9\n",
      "generator 9\n",
      "zisserman 9\n",
      "reed 9\n",
      "deeper 9\n",
      "residual 9\n",
      "maaten 9\n",
      "adaptive 9\n",
      "constraint 9\n",
      "reason 9\n",
      "co 9\n",
      "contrast 9\n",
      "popularity 9\n",
      "improves 9\n",
      "ii 9\n",
      "link 9\n",
      "rank 9\n",
      "preference 9\n",
      "fact 9\n",
      "jointly 9\n",
      "providing 9\n",
      "iccv 9\n",
      "bengio 9\n",
      "corr 9\n",
      "xie 9\n",
      "separated 9\n",
      "ten 9\n",
      "require 9\n",
      "becomes 9\n",
      "exp 9\n",
      "returned 9\n",
      "default 9\n",
      "diagonal 9\n",
      "computed 9\n",
      "apply 9\n",
      "yield 9\n",
      "coding 9\n",
      "shao 9\n",
      "towards 9\n",
      "reducing 9\n",
      "consistency 9\n",
      "reduce 9\n",
      "k2 9\n",
      "multinmf 9\n",
      "granularity 9\n",
      "decomposition 9\n",
      "laptop 9\n",
      "refinement 9\n",
      "vertical 9\n",
      "spidercnn 9\n",
      "transformation 9\n",
      "seat 9\n",
      "change 9\n",
      "efficientnets 9\n",
      "b4 9\n",
      "b3 9\n",
      "pipeline 9\n",
      "i1 9\n",
      "linear 9\n",
      "icml 9\n",
      "i2 9\n",
      "facsnet 9\n",
      "ill 9\n",
      "propagation 9\n",
      "gik 9\n",
      "upright 9\n",
      "intellectual 8\n",
      "highly 8\n",
      "diverse 8\n",
      "whose 8\n",
      "benchmarking 8\n",
      "agreement 8\n",
      "potential 8\n",
      "doe 8\n",
      "identify 8\n",
      "belongs 8\n",
      "project 8\n",
      "version 8\n",
      "stanford 8\n",
      "deepfashion 8\n",
      "year 8\n",
      "follow 8\n",
      "found 8\n",
      "want 8\n",
      "rule 8\n",
      "represents 8\n",
      "unique 8\n",
      "assigned 8\n",
      "divided 8\n",
      "weak 8\n",
      "naturally 8\n",
      "tend 8\n",
      "fan 8\n",
      "lower 8\n",
      "select 8\n",
      "choose 8\n",
      "reported 8\n",
      "lack 8\n",
      "effectively 8\n",
      "national 8\n",
      "grant 8\n",
      "european 8\n",
      "finegrained 8\n",
      "exploring 8\n",
      "cao 8\n",
      "der 8\n",
      "recommendation 8\n",
      "generating 8\n",
      "together 8\n",
      "considering 8\n",
      "interaction 8\n",
      "evaluated 8\n",
      "explorer 8\n",
      "discus 8\n",
      "selected 8\n",
      "selection 8\n",
      "simply 8\n",
      "introduced 8\n",
      "produce 8\n",
      "gain 8\n",
      "relevant 8\n",
      "direction 8\n",
      "gans 8\n",
      "picture 8\n",
      "enables 8\n",
      "respect 8\n",
      "overall 8\n",
      "empirical 8\n",
      "lead 8\n",
      "predict 8\n",
      "remove 8\n",
      "boost 8\n",
      "effectiveness 8\n",
      "sampling 8\n",
      "fully 8\n",
      "c2 8\n",
      "hk 8\n",
      "power 8\n",
      "probability 8\n",
      "lc 8\n",
      "difficulty 8\n",
      "cca 8\n",
      "ksh 8\n",
      "backbone 8\n",
      "survey 8\n",
      "summarization 8\n",
      "besides 8\n",
      "making 8\n",
      "manifold 8\n",
      "optimal 8\n",
      "update 8\n",
      "vt 8\n",
      "control 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('patents-ai': conda)"
  },
  "interpreter": {
   "hash": "9492b3f3ffaca3e3deebd53b740a5e3b3a42fa9584de750d11d509ba01fba996"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "# conda install pytorch torchvision torchaudio -c pytorch\n",
    "# pip install transformers\n",
    "# pip install nltk\n",
    "# pip install numpy \n",
    "# pip install pandas\n",
    "#pip install wordcloud\n",
    "# conda install matplotlib\n",
    "# pip install scholarly"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import textract\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "from scholarly import scholarly"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Preparatory steps\n",
    "nltk.download('stopwords')\n",
    "full_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "full_stop_words.add('b')\n",
    "full_stop_words.add('c')\n",
    "full_stop_words.add('d')\n",
    "full_stop_words.add('e')\n",
    "full_stop_words.add('f')\n",
    "full_stop_words.add('g')\n",
    "full_stop_words.add('h')\n",
    "full_stop_words.add('j')\n",
    "full_stop_words.add('k')\n",
    "full_stop_words.add('l')\n",
    "full_stop_words.add('m')\n",
    "full_stop_words.add('n')\n",
    "full_stop_words.add('p')\n",
    "full_stop_words.add('q')\n",
    "full_stop_words.add('r')\n",
    "full_stop_words.add('u')\n",
    "full_stop_words.add('v')\n",
    "full_stop_words.add('x')\n",
    "full_stop_words.add('w')\n",
    "full_stop_words.add('y')\n",
    "full_stop_words.add('z')\n",
    "full_stop_words.add('pp')\n",
    "full_stop_words.add('et')\n",
    "full_stop_words.add('al')\n",
    "full_stop_words.add('ha')\n",
    "full_stop_words.add('li')\n",
    "full_stop_words.add('sij')\n",
    "full_stop_words.add('arxiv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/liam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# Common functions\n",
    "\n",
    "def re_matches(text, regex):\n",
    "    iterator = re.findall(regex, text)\n",
    "\n",
    "    refs = []\n",
    "    for match in iterator:\n",
    "        refs.append(match) \n",
    "\n",
    "    # print(len(refs))\n",
    "    return refs\n",
    "\n",
    "def gen_refs(text):\n",
    "    regex = r'\\[\\d*\\]\\ ([A-Z]\\.[^\\[]*)\\n'\n",
    "    return re_matches(text, regex)\n",
    "    \n",
    "def gen_refs_no_number(text):\n",
    "    regex = r'\\[\\d*\\]\\ ([A-Z]\\.[^\\[]*)\\n'\n",
    "    return re_matches(text, regex)\n",
    "\n",
    "def gen_refs_end_year(text):\n",
    "    regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_end_year_with_text_brackets(text):\n",
    "    regex = r'\\[(?:[A-Za-z\\ ]*\\.?\\,\\ \\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def gen_refs_end_year_with_number_brackets(text):\n",
    "    regex = r'\\[(?:\\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "    return re_matches(text, regex) \n",
    "    \n",
    "def gen_refs_end_pages(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    regex = r'\\[\\d*\\]\\ [^[]*pp\\.\\ \\d+\\.\\ '\n",
    "    return re_matches(text, regex) \n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def remove_set_from_dict(s, d):\n",
    "    for t in s:\n",
    "        if t in d:\n",
    "            del d[t]\n",
    "    return d\n",
    "    \n",
    "def word_frequencies(text):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    frequencies = {}\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "def word_frequencies_dist(text, frequencies):\n",
    "    words = re.findall('[A-Za-z][A-Za-z0-9]*', text)\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = get_lemma(w)\n",
    "        if w not in full_stop_words:\n",
    "            if w in frequencies:\n",
    "                frequencies[w] += 1\n",
    "            else:\n",
    "                frequencies[w] = 1\n",
    "    return frequencies\n",
    "\n",
    "\n",
    "\n",
    "def wc(freqs, file_name):\n",
    "    wordcloud = WordCloud(background_color=\"white\", width=600, height=600, max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "    #wordcloud.generate(long_string)\n",
    "    wordcloud.generate_from_frequencies(freqs)\n",
    "    wordcloud.to_file(f'data/word-cloud-{file_name}.png')\n",
    "    wordcloud.to_image()\n",
    "\n",
    "\n",
    "def parse_ref_default(ref, source_file, source_title):\n",
    "    segments = ref.split(',')\n",
    "    authors = []\n",
    "    title = ''\n",
    "    journal = ''\n",
    "    publisher = ''\n",
    "    volume = ''\n",
    "    year = ''\n",
    "    pages = ''\n",
    "    finished_authors = False\n",
    "    author_count = 0\n",
    "    for count, value in enumerate(segments):\n",
    "        if value.find('.') > -1 and finished_authors == False:\n",
    "            authors.append(value.strip())\n",
    "            author_count = author_count + 1\n",
    "        elif value.find('.') == -1 and count < author_count + 2:\n",
    "            if finished_authors == False:\n",
    "                finished_authors = True\n",
    "                title = value.strip()\n",
    "            else:\n",
    "                journal = value.strip()\n",
    "        elif count == len(segments) - 1 and author_count + 2 == len(segments) - 1:\n",
    "            match = re.search(r'(.*)\\((\\d\\d\\d\\d)\\)(.*)', value)\n",
    "            if match is not None:\n",
    "                volume = match.groups()[0]\n",
    "                year = match.groups()[1]\n",
    "                pages = match.groups()[2]\n",
    "        # For when year and pages are comma-separated\n",
    "        elif count == len(segments) - 2 and author_count + 3 == len(segments) - 1:\n",
    "            year = value\n",
    "        elif count == len(segments) - 1 and author_count + 3 == len(segments) - 1:\n",
    "            pages = value\n",
    "        # For when volume / publisher, year and pages are comma-separated\n",
    "        elif count == len(segments) - 3 and author_count + 4 == len(segments) - 1:\n",
    "            if value.isdigit():\n",
    "                volume = value\n",
    "            else:\n",
    "                publisher = value\n",
    "        elif count == len(segments) - 2 and author_count + 4 == len(segments) - 1:\n",
    "            year = value\n",
    "        elif count == len(segments) - 1 and author_count + 4 == len(segments) - 1:\n",
    "            pages = value\n",
    "    return pd.DataFrame([[(', ').join(authors), title, journal, publisher, volume, year, pages, ref, source_file, source_title]], \n",
    "                        columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "def parse_ref_1(ref, source_file, source_title):\n",
    "    pattern = re.compile(r'[A-Z][a-z\\-]+\\, [A-Z\\.]+')\n",
    "    authors = []\n",
    "    title = ''\n",
    "    journal = ''\n",
    "    publisher = ''\n",
    "    volume = ''\n",
    "    year = ''\n",
    "    pages = ''\n",
    "    finished_authors = False\n",
    "    author_count = 0\n",
    "    return pd.DataFrame([[(', ').join(authors), title, journal, publisher, volume, year, pages, ref, source_file, source_title]], \n",
    "                        columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "def parse_ref(ref, ref_type, source_file, source_title):\n",
    "    print(ref_type)\n",
    "    if ref_type == 0:\n",
    "        return parse_ref_default(ref, source_file, source_title)\n",
    "    elif ref_type == 1:\n",
    "        return parse_ref_1(ref, source_file, source_title)\n",
    "    else:\n",
    "        return parse_ref_default(ref, source_file, source_title)\n",
    "\n",
    "def arxiv_results(title):\n",
    "    t =  urllib.parse.quote_plus(title)\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:'+t+'&start=0&max_results=1'\n",
    "    data = urllib.request.urlopen(url)\n",
    "    results = data.read().decode('utf-8')\n",
    "    return results\n",
    "\n",
    "def make_up_file_name(file_name):\n",
    "    return 'refs/download/' +  file_name + '.pdf'\n",
    "\n",
    "def extract_and_save_pdf_from_atom(atom_xml, title, title_no_colon, title_stem, pass_through=False):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    for e in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        t = e.find('{http://www.w3.org/2005/Atom}title')\n",
    "        link = e.find('{http://www.w3.org/2005/Atom}link')\n",
    "        if t.text.lower().find(title_stem.lower()) == 0:\n",
    "            for link in e.findall('{http://www.w3.org/2005/Atom}link'):\n",
    "                # Title must match and link must be a pdf\n",
    "                if 'type' in link.attrib and link.attrib['type'] == 'application/pdf':\n",
    "                    u  = link.attrib['href']\n",
    "                    print(u, title)\n",
    "                    if not pass_through:\n",
    "                        urllib.request.urlretrieve(u, make_up_file_name(title))\n",
    "\n",
    "def test_element(atom_xml):\n",
    "    root = ET.fromstring(atom_xml)\n",
    "    entry = root.find('{http://www.w3.org/2005/Atom}entry')\n",
    "    if entry is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def top_dist(freqs, n):\n",
    "    counter = 0\n",
    "    for w in sorted(freqs, key = freqs.get, reverse = True):\n",
    "        counter = counter + 1\n",
    "        print(w, freqs[w])\n",
    "        if counter == n:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "file_name = '2106.12139.pdf'\n",
    "title = 'PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database'\n",
    "\n",
    "\n",
    "text = textract.process(\"refs/\" + file_name).decode('utf-8')\n",
    "freqs = word_frequencies(text)\n",
    "\n",
    "references = pd.DataFrame(columns = ['authors', 'title', 'journal', 'publisher', 'volume', 'year', 'pages', 'full_ref', 'source_file', 'source_title'])\n",
    "\n",
    "refs = gen_refs(text)\n",
    "for r in refs:\n",
    "    r = r.replace('\\n', ' ')\n",
    "    references = references.append(parse_ref(r, file_name, title))\n",
    "\n",
    "references.to_csv('data/references.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# Generate the word cloud from the seed file frequencies.\n",
    "wc(freqs, file_name)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# Test code\n",
    "\n",
    "# t = references['title'].iloc[7]\n",
    "# print(t)\n",
    "# r = arxiv_results(t.strip())\n",
    "# print(r)\n",
    "# # ET.tostring(r)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Attempt to locate references on Arxiv, and save\n",
    "counter = 0\n",
    "for title in references['title']:\n",
    "    f = make_up_file_name(title)\n",
    "    counter = counter + 1\n",
    "    if not os.path.isfile(f):\n",
    "        r = arxiv_results(title)\n",
    "        title_stem = title\n",
    "        title_no_colon = title\n",
    "        if title.find(':') > -1:\n",
    "            title_stem = title[:title.index(':')]\n",
    "            title_no_colon = title[:title.index(':')] + title[title.index(':')+1:]\n",
    "        \n",
    "        has_entry = test_element(r)\n",
    "\n",
    "        # Remove the semi-colon - seems to confuse Arxiv API\n",
    "        if not has_entry:\n",
    "            r = arxiv_results(title_no_colon)\n",
    "            has_entry = test_element(r)\n",
    "\n",
    "        # Remove everything after the semi-colon\n",
    "        if not has_entry:\n",
    "            r = arxiv_results(title_stem)\n",
    "            has_entry = test_element(r)\n",
    "\n",
    "        print(counter, title)\n",
    "        if has_entry:\n",
    "            extract_and_save_pdf_from_atom(r, title, title_no_colon, title_stem, False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Deep learning for visual understanding: A review\n",
      "2 Deep convolutional neural networks for image classification: A comprehensive review\n",
      "4 Microsoft coco: Common objects in context\n",
      "5 Deepfashion: Powering robust clothes recognition and retrieval with rich annotations\n",
      "http://arxiv.org/pdf/1901.07973v1 Deepfashion: Powering robust clothes recognition and retrieval with rich annotations\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cb7ada3e6ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_entry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mextract_and_save_pdf_from_atom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_no_colon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_stem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-4e56010d4d80>\u001b[0m in \u001b[0;36mextract_and_save_pdf_from_atom\u001b[0;34m(atom_xml, title, title_no_colon, title_stem, pass_through)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpass_through\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_up_file_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/patents-ai/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "def gen_refs_multiple_pass(text):\n",
    "    refs = gen_refs(text)\n",
    "    ref_type = 0\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_year(text)\n",
    "        ref_type = 1\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_year_with_text_brackets(text)\n",
    "        ref_type = 2\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_year_with_number_brackets(text)\n",
    "        ref_type = 3\n",
    "    if len(refs) == 0:\n",
    "        refs = gen_refs_end_pages(text)\n",
    "        ref_type = 4\n",
    "    return refs, ref_type\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "d = './refs/download'\n",
    "all_freqs = {**freqs}\n",
    "all_references = references.copy()\n",
    "for file in os.listdir(d):\n",
    "    if file.endswith(\"Fashion-gen: The generative fashion dataset and challenge.pdf\"):\n",
    "    # if file.endswith(\".pdf\"):\n",
    "        print(os.path.join(d, file))\n",
    "        f = os.path.join(d, file)\n",
    "        try:\n",
    "            text_local = textract.process(f).decode('utf-8')\n",
    "            all_freqs = word_frequencies_dist(text_local, all_freqs)\n",
    "            refs_local, ref_type = gen_refs_multiple_pass(text_local)\n",
    "\n",
    "            whole_pattern = re.compile(r'((?:[A-Z][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+(?:\\,\\ )?)*)(?:\\,?\\ and\\ )?([A-Z][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+)([^\\.]*)\\.\\ (.*)\\,\\ (\\d{4})')\n",
    "            author_subpatten = re.compile(r'[A-Z][A-Za-zá\\-\\ ]+\\,\\ (?:\\-?[A-Z]\\.\\ ?)+')\n",
    "            for r in refs_local:\n",
    "                r = r.replace('\\n', ' ')\n",
    "                print(r)\n",
    "                # pattern = re.compile(r'([A-Z][a-z\\-]+\\,\\ (?:[A-Z]\\.?)*[\\,\\ ]){0,8}([^\\.]*)(.*)\\,\\ (\\d){4}')\n",
    "                it = re.findall(whole_pattern, r)\n",
    "                for i in it:\n",
    "                    first_authors = i[0]\n",
    "                    last_author = i[1].strip()\n",
    "                    title = i[2]\n",
    "                    journal = i[3]\n",
    "                    year = i[4]\n",
    "                    all_authors = []\n",
    "                    if first_authors != '':\n",
    "                        authors = re.findall(author_subpatten, first_authors)\n",
    "                        # print(first_authors)\n",
    "                        # print(authors)\n",
    "                        for a in authors:\n",
    "                            all_authors.append(a)\n",
    "                    all_authors.append(last_author)\n",
    "                    print(all_authors)\n",
    "                    print(title)\n",
    "                    print(journal)\n",
    "                    print(year)\n",
    "# Han, X., Wu, Z., Wu, Z., Yu, R., and Davis, L. S. Viton: An image-based virtual try-on network. arXiv preprint arXiv:1711.08447, 2017.\n",
    "\n",
    "                # all_references = all_references.append(parse_ref(r, ref_type, f, ''))\n",
    "\n",
    "        except:\n",
    "            print('Failed to read', f)\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf\n",
      "Han, X., Wu, Z., Wu, Z., Yu, R., and Davis, L. S. Viton: An image-based virtual try-on network. arXiv preprint arXiv:1711.08447, 2017.\n",
      "['Han, X.', 'Wu, Z.', 'Wu, Z.', 'Yu, R.', 'Davis, L. S.']\n",
      "Viton: An image-based virtual try-on network\n",
      "arXiv preprint arXiv:1711.08447\n",
      "2017\n",
      "Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., and Belongie, S. Stacked generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 4, 2017.\n",
      "['Huang, X.', 'Li, Y.', 'Poursaeed, O.', 'Hopcroft, J.', 'Belongie, S.']\n",
      "Stacked generative adversarial networks\n",
      "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 4\n",
      "2017\n",
      "Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks. arXiv preprint, 2017.\n",
      "['Isola, P.', 'Zhu, J.-Y.', 'Zhou, T.', 'Efros, A. A.']\n",
      "Image-toimage translation with conditional adversarial networks\n",
      "arXiv preprint\n",
      "2017\n",
      "Kalantidis, Y., Kennedy, L., and Li, L.-J. Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos. In Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, pp. 105–112. ACM, 2013.\n",
      "['Kalantidis, Y.', 'Kennedy, L.', 'Li, L.-J.']\n",
      "Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos\n",
      "In Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, pp. 105–112. ACM\n",
      "2013\n",
      "Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.  Barratt, S. and Sharma, R. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.\n",
      "['Karras, T.', 'Aila, T.', 'Laine, S.', 'Lehtinen, J.']\n",
      "Progressive growing of gans for improved quality, stability, and variation\n",
      "arXiv preprint arXiv:1710.10196, 2017.  Barratt, S. and Sharma, R. A note on the inception score. arXiv preprint arXiv:1801.01973\n",
      "2018\n",
      "Kiapour, M. H., Han, X., Lazebnik, S., Berg, A. C., and Berg, T. L. Where to buy it: Matching street clothing photos in online shops. In ICCV, pp. 3343–3351, 2015.\n",
      "['Kiapour, M. H.', 'Han, X.', 'Lazebnik, S.', 'Berg, A. C.', 'Berg, T. L.']\n",
      "Where to buy it: Matching street clothing photos in online shops\n",
      "In ICCV, pp. 3343–3351\n",
      "2015\n",
      "Belghazi, M. I., Rajeswar, S., Mastropietro, O., Rostamzadeh, N., Mitrovic, J., and Courville, A. Hierarchical adversarially learned inference. arXiv preprint arXiv:1802.01071, 2018.\n",
      "['Belghazi, M. I.', 'Rajeswar, S.', 'Mastropietro, O.', 'Rostamzadeh, N.', 'Mitrovic, J.', 'Courville, A.']\n",
      "Hierarchical adversarially learned inference\n",
      "arXiv preprint arXiv:1802.01071\n",
      "2018\n",
      "Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint, 2016.\n",
      "['Ledig, C.', 'Theis, L.', 'Huszár, F.', 'Caballero, J.', 'Cunningham, A.', 'Acosta, A.', 'Aitken, A.', 'Tejani, A.', 'Totz, J.', 'Wang, Z.']\n",
      ", et al\n",
      "Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint\n",
      "2016\n",
      "Bossard, L., Dantone, M., Leistner, C., Wengert, C., Quack, T., and Van Gool, L. Apparel classification with style. In Asian conference on computer vision, pp. 321–335. Springer, 2012.\n",
      "['Bossard, L.', 'Dantone, M.', 'Leistner, C.', 'Wengert, C.', 'Quack, T.', 'Van Gool, L.']\n",
      "Apparel classification with style\n",
      "In Asian conference on computer vision, pp. 321–335. Springer\n",
      "2012\n",
      "Liang, X., Lin, L., Yang, W., Luo, P., Huang, J., and Yan, S. Clothes co-parsing via joint image segmentation and labeling with application to clothing retrieval. IEEE Transactions on Multimedia, 18(6):1175–1186, 2016.  Chen, H., Gallagher, A., and Girod, B. Describing clothing by semantic attributes. In European conference on computer vision, pp. 609–623. Springer, 2012.\n",
      "['Liang, X.', 'Lin, L.', 'Yang, W.', 'Luo, P.', 'Huang, J.', 'Yan, S.']\n",
      "Clothes co-parsing via joint image segmentation and labeling with application to clothing retrieval\n",
      "IEEE Transactions on Multimedia, 18(6):1175–1186, 2016.  Chen, H., Gallagher, A., and Girod, B. Describing clothing by semantic attributes. In European conference on computer vision, pp. 609–623. Springer\n",
      "2012\n",
      "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.\n",
      "['Lin, T.-Y.', 'Maire, M.', 'Belongie, S.', 'Hays, J.', 'Perona, P.', 'Ramanan, D.', 'Dollár, P.', 'Zitnick, C. L.']\n",
      "Microsoft coco: Common objects in context\n",
      "In European conference on computer vision, pp. 740–755. Springer\n",
      "2014\n",
      "Chen, Q., Huang, J., Feris, R., Brown, L. M., Dong, J., and Yan, S. Deep domain adaptation for describing people based on fine-grained clothing attributes. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 5315–5324. IEEE, 2015.\n",
      "['Chen, Q.', 'Huang, J.', 'Feris, R.', 'Brown, L. M.', 'Dong, J.', 'Yan, S.']\n",
      "Deep domain adaptation for describing people based on fine-grained clothing attributes\n",
      "In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 5315–5324. IEEE\n",
      "2015\n",
      "Denton, E. and Fergus, R. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687, 2018.\n",
      "['Denton, E.', 'Fergus, R.']\n",
      "Stochastic video generation with a learned prior\n",
      "arXiv preprint arXiv:1802.07687\n",
      "2018\n",
      "Liu, S., Song, Z., Liu, G., Xu, C., Lu, H., and Yan, S. Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3330–3337. IEEE, 2012.\n",
      "['Liu, S.', 'Song, Z.', 'Liu, G.', 'Xu, C.', 'Lu, H.', 'Yan, S.']\n",
      "Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set\n",
      "In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3330–3337. IEEE\n",
      "2012\n",
      "Denton, E. L. et al. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems, pp. 4417–4426, 2017.\n",
      "['Denton, E. L.']\n",
      "et al\n",
      "Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems, pp. 4417–4426\n",
      "2017\n",
      "Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730– 3738, 2015.\n",
      "['Liu, Z.', 'Luo, P.', 'Wang, X.', 'Tang, X.']\n",
      "Deep learning face attributes in the wild\n",
      "In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730– 3738\n",
      "2015\n",
      "Nilsback, M.-E. and Zisserman, A. Automated flower classification over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIP’08. Sixth Indian Conference on, pp. 722–729. IEEE, 2008.\n",
      "['Nilsback, M.-E.', 'Zisserman, A.']\n",
      "Automated flower classification over a large number of classes\n",
      "In Computer Vision, Graphics & Image Processing, 2008. ICVGIP’08. Sixth Indian Conference on, pp. 722–729. IEEE\n",
      "2008\n",
      "Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016a.\n",
      "['Reed, S.', 'Akata, Z.', 'Yan, X.', 'Logeswaran, L.', 'Schiele, B.', 'Lee, H.']\n",
      "Generative adversarial text to image synthesis\n",
      "arXiv preprint arXiv:1605.05396\n",
      "2016\n",
      "Reed, S., Akata, Z., Lee, H., and Schiele, B. Learning deep representations of fine-grained visual descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 49–58, 2016b.\n",
      "['Reed, S.', 'Akata, Z.', 'Lee, H.', 'Schiele, B.']\n",
      "Learning deep representations of fine-grained visual descriptions\n",
      "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 49–58\n",
      "2016\n",
      "Reed, S. E., Akata, Z., Schiele, B., and Lee, H. Learning deep representations of fine-grained visual descriptions. CoRR, abs/1605.05395, 2016.\n",
      "['Reed, S. E.', 'Akata, Z.', 'Schiele, B.', 'Lee, H.']\n",
      "Learning deep representations of fine-grained visual descriptions\n",
      "CoRR, abs/1605.05395\n",
      "2016\n",
      "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234–2242, 2016.\n",
      "['Salimans, T.', 'Goodfellow, I.', 'Zaremba, W.', 'Cheung, V.', 'Radford, A.', 'Chen, X.']\n",
      "Improved techniques for training gans\n",
      "In Advances in Neural Information Processing Systems, pp. 2234–2242\n",
      "2016\n",
      "Schuster, M., Paliwal, K. K., and General, A. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 1997.\n",
      "['Schuster, M.', 'Paliwal, K. K.', 'General, A.']\n",
      "Bidirectional recurrent neural networks\n",
      "IEEE Transactions on Signal Processing\n",
      "1997\n",
      "Simo-Serra, E., Fidler, S., Moreno-Noguer, F., and Urtasun, R. Neuroaesthetics in fashion: Modeling the perception of fashionability. In CVPR, volume 2, pp. 6, 2015. Sønderby, C. K., Caballero, J., Theis, L., Shi, W., and Huszár, F. Amortised map inference for image superresolution. arXiv preprint arXiv:1610.04490, 2016.\n",
      "['Simo-Serra, E.', 'Fidler, S.', 'Moreno-Noguer, F.', 'Urtasun, R.']\n",
      "Neuroaesthetics in fashion: Modeling the perception of fashionability\n",
      "In CVPR, volume 2, pp. 6, 2015. Sønderby, C. K., Caballero, J., Theis, L., Shi, W., and Huszár, F. Amortised map inference for image superresolution. arXiv preprint arXiv:1610.04490\n",
      "2016\n",
      "Taigman, Y., Polyak, A., and Wolf, L. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016. van der Maaten, L. and Hinton, G. Visualizing data using t-SNE. Journal of Machine Learning Research, 9: 2579–2605, 2008.\n",
      "['Taigman, Y.', 'Polyak, A.', 'Wolf, L.']\n",
      "Unsupervised cross-domain image generation\n",
      "arXiv preprint arXiv:1611.02200, 2016. van der Maaten, L. and Hinton, G. Visualizing data using t-SNE. Journal of Machine Learning Research, 9: 2579–2605\n",
      "2008\n",
      "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. CoRR, abs/1706.03762, 2017.\n",
      "['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.', 'Uszkoreit, J.', 'Jones, L.', 'Gomez, A. N.', 'Kaiser, L.', 'Polosukhin, I.']\n",
      "Attention is all you need\n",
      "CoRR, abs/1706.03762\n",
      "2017\n",
      "Veit, A., Kovacs, B., Bell, S., McAuley, J., Bala, K., and Belongie, S. Learning visual clothing style with heterogeneous dyadic co-occurrences. In Computer Vision (ICCV), 2015 IEEE International Conference on, pp. 4642–4650. IEEE, 2015.\n",
      "['Veit, A.', 'Kovacs, B.', 'Bell, S.', 'McAuley, J.', 'Bala, K.', 'Belongie, S.']\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences\n",
      "In Computer Vision (ICCV), 2015 IEEE International Conference on, pp. 4642–4650. IEEE\n",
      "2015\n",
      "Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–2699, 2015.\n",
      "['Xiao, T.', 'Xia, T.', 'Yang, Y.', 'Huang, C.', 'Wang, X.']\n",
      "Learning from massive noisy labeled data for image classification\n",
      "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–2699\n",
      "2015\n",
      "Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., and Metaxas, D. N. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. CoRR, abs/1710.10916, 2017b.\n",
      "['Zhang, H.', 'Xu, T.', 'Li, H.', 'Zhang, S.', 'Wang, X.', 'Huang, X.', 'Metaxas, D. N.']\n",
      "Stackgan++: Realistic image synthesis with stacked generative adversarial networks\n",
      "CoRR, abs/1710.10916\n",
      "2017\n",
      "Zhang, Z., Xie, Y., and Yang, L. Photographic text-toimage synthesis with a hierarchically-nested adversarial network. arXiv preprint arXiv:1802.09178, 2018. Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017a.\n",
      "['Zhang, Z.', 'Xie, Y.', 'Yang, L.']\n",
      "Photographic text-toimage synthesis with a hierarchically-nested adversarial network\n",
      "arXiv preprint arXiv:1802.09178, 2018. Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593\n",
      "2017\n",
      "Zhu, S., Fidler, S., Urtasun, R., Lin, D., and Loy, C. C. Be your own prada: Fashion synthesis with structural coherence. arXiv preprint arXiv:1710.07346, 2017b.\n",
      "['Zhu, S.', 'Fidler, S.', 'Urtasun, R.', 'Lin, D.', 'Loy, C. C.']\n",
      "Be your own prada: Fashion synthesis with structural coherence\n",
      "arXiv preprint arXiv:1710.07346\n",
      "2017\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# f = './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
    "f = './refs/download/Fashion-gen: The generative fashion dataset and challenge.pdf'\n",
    "# f = './refs/download/Doubly Aligned Incomplete Multi-view Clustering.pdf'\n",
    "# f = './refs/download/Shapenet: An information-rich 3d model repository.pdf'\n",
    "\n",
    "t = textract.process(f).decode('utf-8')\n",
    "print(t)\n",
    "# t = t.replace('\\n', ' ')\n",
    "# regex = r'\\[\\d*\\]\\ [^\\n]*\\n[^\\n]*\\d\\d\\d\\d\\.'\n",
    "# regex = r'[A-Z][A-Za-z\\-]+\\,(.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){1,4}\\,\\ \\d\\d\\d\\d[a-b]?\\.'\n",
    "# regex = r'\\ [A-Z][A-Za-z\\,\\.\\ \\-\\n]+[A-Z]\\.\\ [A-Z](?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "# regex = r'[A-Z][A-Za-z\\,\\.\\ ]+[A-Z]\\.\\ [A-Z](.\\n.)*\\1{5}.*\\d\\d\\d\\d\\.\\ '\n",
    "# regex = r'\\[(?:[A-Za-z\\ \\.]*\\,\\ \\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "regex = r'\\[(?:\\d+)\\]\\ (?:.*\\n.*){1,4}\\d\\d\\d\\d[a-b]?\\.'\n",
    "pattern = re.compile(regex)\n",
    "iterator = re.findall(pattern, t)\n",
    "\n",
    "print(len(iterator))\n",
    "refs = []\n",
    "for match in iterator:\n",
    "    refs.append(match) \n",
    "print(refs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ShapeNet: An Information-Rich 3D Model Repository\n",
      "http://www.shapenet.org\n",
      "1\n",
      "\n",
      "Angel X. Chang , Thomas Funkhouser2 , Leonidas Guibas1 , Pat Hanrahan1 , Qixing Huang3 , Zimo Li3 ,\n",
      "Silvio Savarese1 , Manolis Savva∗1 , Shuran Song2 , Hao Su∗1 , Jianxiong Xiao2 , Li Yi1 , and Fisher Yu2\n",
      "\n",
      "arXiv:1512.03012v1 [cs.GR] 9 Dec 2015\n",
      "\n",
      "1\n",
      "\n",
      "Stanford University — 2 Princeton University — 3 Toyota Technological Institute at Chicago\n",
      "Authors listed alphabetically\n",
      "\n",
      "Abstract\n",
      "\n",
      "tial scans is a research goal shared by computer graphics\n",
      "and vision. Scene understanding from 2D images is a grand\n",
      "challenge in vision that has recently benefited tremendously\n",
      "from 3D CAD models [28, 34]. Navigation of autonomous\n",
      "robots and planning of grasping manipulations are two large\n",
      "areas in robotics that benefit from an understanding of 3D\n",
      "shapes. At the root of all these research problems lies\n",
      "the need for attaching semantics to representations of 3D\n",
      "shapes, and doing so at large scale.\n",
      "Recently, data-driven methods from the machine learning community have been exploited by researchers in vision\n",
      "and NLP (natural language processing). “Big data” in the\n",
      "visual and textual domains has led to tremendous progress\n",
      "towards associating semantics with content in both fields.\n",
      "Mirroring this pattern, recent work in computer graphics\n",
      "has also applied similar approaches to specific problems in\n",
      "the synthesis of new shape variations [10] and new arrangements of shapes [6]. However, a critical bottleneck facing\n",
      "the adoption of data-driven methods for 3D content is the\n",
      "lack of large-scale, curated datasets of 3D models that are\n",
      "available to the community.\n",
      "Motivated by the far-reaching impact of dataset efforts\n",
      "such as the Penn Treebank [20], WordNet [21] and ImageNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D\n",
      "model dataset. Making a comprehensive, semantically enriched shape dataset available to the community can have\n",
      "immense impact, enabling many avenues of future research.\n",
      "In constructing ShapeNet we aim to fulfill several goals:\n",
      "• Collect and centralize 3D model datasets, helping to\n",
      "organize effort in the research community.\n",
      "\n",
      "We present ShapeNet: a richly-annotated, large-scale\n",
      "repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of\n",
      "semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many\n",
      "semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes,\n",
      "physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and\n",
      "provide a large-scale quantitative benchmark for research\n",
      "in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135\n",
      "categories (WordNet synsets). In this report we describe the\n",
      "ShapeNet effort as a whole, provide details for all currently\n",
      "available datasets, and summarize future plans.\n",
      "\n",
      "1. Introduction\n",
      "Recent technological developments have led to an explosion in the amount of 3D data that we can generate and\n",
      "store. Repositories of 3D CAD models are expanding continuously, predominantly through aggregation of 3D content\n",
      "on the web. RGB-D sensors and other technology for scanning and reconstruction are providing increasingly higher\n",
      "fidelity geometric representations of objects and real environments that can eventually become CAD-quality models.\n",
      "At the same time, there are many open research problems due to fundamental challenges in using 3D content.\n",
      "Computing segmentations of 3D shapes, and establishing\n",
      "correspondences between them are two basic problems in\n",
      "geometric shape analysis. Recognition of shapes from par∗ Contact\n",
      "\n",
      "• Support data-driven methods requiring 3D model data.\n",
      "• Enable evaluation and comparison of algorithms for\n",
      "fundamental tasks involving geometry (e.g., segmentation, alignment, correspondence).\n",
      "• Serve as a knowledge base for representing real-world\n",
      "objects and their semantics.\n",
      "\n",
      "authors: {msavva,haosu}@cs.stanford.edu\n",
      "\n",
      "1\n",
      "\n",
      "\fThese goals imply several desiderata for ShapeNet:\n",
      "• Broad and deep coverage of objects observed in the\n",
      "real world, with thousands of object categories and\n",
      "millions of total instances.\n",
      "\n",
      "The Princeton Shape Benchmark is probably the most\n",
      "well-known and frequently used 3D shape collection to date\n",
      "(with over 1000 citations) [27]. It contains around 1,800 3D\n",
      "models grouped into 90 categories, but has no annotations\n",
      "beyond category labels. Other commonly-used datasets\n",
      "contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models\n",
      "with images [35], semantic ontologies [5], and other functional annotations — but again only for small size datasets.\n",
      "For example, the Benchmark for 3D Mesh Segmentation\n",
      "contains just 380 models in 19 object classes [2].\n",
      "In contrast, there has been a flurry of activity on collecting, organizing, and labeling large datasets in computer vision and related fields. For example, ImageNet [4] provides\n",
      "a set of 14M images organized into 20K categories associated with “synsets” of WordNet [21]. LabelMe provides\n",
      "segmentations and label annotations of hundreds of thousands of objects in tens of thousands of images [24]. The\n",
      "SUN dataset provides 3M annotations of objects in 4K categories appearing in 131K images of 900 types of scenes.\n",
      "Recent work demonstrated the benefit of a large dataset\n",
      "of 120K 3D CAD models in training a convolutional neural network for object recognition and next-best view prediction in RGB-D data [34]. Large datasets such as this\n",
      "and others (e.g., [14, 18]) have revitalized data-driven algorithms for recognition, detection, and editing of images,\n",
      "which have revolutionized computer vision.\n",
      "Similarly, large collections of annotated 3D data have\n",
      "had great influence on progress in other disciplines. For example, the Protein Data Bank [1] provides a database with\n",
      "100K protein 3D structures, each labeled with its source\n",
      "and links to structural and functional annotations [15]. This\n",
      "database is a common repository of all 3D protein structures\n",
      "solved to date and provides a shared infrastructure for the\n",
      "collection and transfer of knowledge about each entry. It has\n",
      "accelerated the development of data-driven algorithms, facilitated the creation of benchmarks, and linked researchers\n",
      "and industry from around the world. We aim to provide a\n",
      "similar resource for 3D models of everyday objects.\n",
      "\n",
      "• Categorization scheme connected to other modalities\n",
      "of knowledge such as 2D images and language.\n",
      "• Annotation of salient physical attributes on models,\n",
      "such as canonical orientations, planes of symmetry,\n",
      "and part decompositions.\n",
      "• Web-based interfaces for searching, viewing and retrieving models in the dataset through several modalities: textual keywords, taxonomy traversal, image and\n",
      "shape similarity search.\n",
      "Achieving these goals and providing the resulting dataset\n",
      "to the community will enable many advances and applications in computer graphics and vision.\n",
      "In this report, we first situate ShapeNet, explaining the\n",
      "overall goals of the effort and the types of data it is intended to contain, as well as motivating the long-term vision and infrastructural design decisions (Section 3). We\n",
      "then describe the acquisition and validation of annotations\n",
      "collected so far (Section 4), summarize the current state of\n",
      "all available ShapeNet datasets, and provide basic statistics\n",
      "on the collected annotations (Section 5). We end with a discussion of ShapeNet’s future trajectory and connect it with\n",
      "several research directions (Section 7).\n",
      "\n",
      "2. Background and Related Work\n",
      "There has been substantial growth in the number of of\n",
      "3D models available online over the last decade, with repositories like the Trimble 3D Warehouse providing millions\n",
      "of 3D polygonal models covering thousands of object and\n",
      "scene categories. Yet, there are few collections of 3D models that provide useful organization and annotations. Meaningful textual descriptions are rarely provided for individual models, and online repositories are usually either unorganized or grouped into gross categories (e.g., furniture,\n",
      "architecture, etc. [7]). As a result, they have been poorly\n",
      "utilized in research and applications.\n",
      "There have been previous efforts to build organized collections of 3D models (e.g., [5, 7]). However, they have\n",
      "provided quite small datasets, covered only a small number of semantic categories, and included few structural and\n",
      "semantic annotations. Most of these previous collections\n",
      "have been developed for evaluating shape retrieval and classification algorithms. For example, datasets are created annually for the Shape Retrieval Contest (SHREC) that commonly contains sets of models organized in object categories. However, those datasets are very small — the most\n",
      "recent SHREC iteration in 2014 [17] contains a “large”\n",
      "dataset with around 9,000 models consisting of models from\n",
      "a variety of sources organized into 171 categories (Table 1).\n",
      "\n",
      "3. ShapeNet: An Information-Rich 3D Model\n",
      "Repository\n",
      "ShapeNet is a large, information-rich repository of 3D\n",
      "models. It contains models spanning a multitude of semantic categories. Unlike previous 3D model repositories, it\n",
      "provides extensive sets of annotations for every model and\n",
      "links between models in the repository and other multimedia data outside the repository.\n",
      "Like ImageNet, ShapeNet provides a view of the contained data in a hierarchical categorization according to\n",
      "WordNet synsets (Figure 1). Unlike other model repositories, ShapeNet also provides a rich set of annotations for\n",
      "2\n",
      "\n",
      "\fBenchmarks\n",
      "SHREC14LSGTB\n",
      "\n",
      "Types\n",
      "\n",
      "# models\n",
      "\n",
      "# classes\n",
      "\n",
      "Avg # models per class\n",
      "\n",
      "Generic\n",
      "\n",
      "8,987\n",
      "\n",
      "171\n",
      "\n",
      "53\n",
      "\n",
      "PSB\n",
      "Generic\n",
      "907+907 (train+test)\n",
      "90+92 (train+test)\n",
      "10+10 (train+test)\n",
      "SHREC12GTB\n",
      "Generic\n",
      "1200\n",
      "60\n",
      "20\n",
      "TSB\n",
      "Generic\n",
      "10,000\n",
      "352\n",
      "28\n",
      "CCCC\n",
      "Generic\n",
      "473\n",
      "55\n",
      "9\n",
      "WMB\n",
      "Watertight (articulated)\n",
      "400\n",
      "20\n",
      "20\n",
      "MSB\n",
      "Articulated\n",
      "457\n",
      "19\n",
      "24\n",
      "BAB\n",
      "Architecture\n",
      "2257\n",
      "183+180 (function+form) 12+13 (function+form)\n",
      "ESB\n",
      "CAD\n",
      "867\n",
      "45\n",
      "19\n",
      "Table 1. Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark\n",
      "(SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape\n",
      "Benchmark (ESB) [9].\n",
      "\n",
      "each shape and correspondences between shapes. The annotations include geometric attributes such as upright and\n",
      "front orientation vectors, parts and keypoints, shape symmetries (reflection plane, other rotational symmetries), and\n",
      "scale of object in real world units. These attributes provide\n",
      "valuable resources for processing, understanding and visualizing 3D shapes in a way that is aware of the semantics of\n",
      "the shape.\n",
      "We have currently collected approximately 3 million\n",
      "shapes from online 3D model repositories, and categorized\n",
      "300 thousand of them against the WordNet taxonomy. We\n",
      "have also annotated a subset of these models with shape\n",
      "properties such as upright and front orientations, symmetries, and hierarchical part decompositions. We are continuing the process of expanding the annotated set of models\n",
      "and also collecting new models from new data sources.\n",
      "In the following sections, we discuss how 3D models\n",
      "are collected for ShapeNet, what annotations will be added,\n",
      "how those annotations will be generated, how annotations\n",
      "will be updated as the dataset evolves over time, and what\n",
      "tools will be provided for the community to search, browse,\n",
      "and utilize existing data, as well as contribute new data.\n",
      "\n",
      "Figure 1. Screenshot of the online ShapeNet taxonomy view, organizing contained 3D models under WordNet synsets.\n",
      "\n",
      "shapes from a broad set of object and scene categories\n",
      "— e.g., many organic shape categories (e.g., humans and\n",
      "mammals), which are rare in Warehouse3D, are plentiful in\n",
      "Yobi3D. For more detailed statistics on the currently available ShapeNet models refer to Section 5.\n",
      "\n",
      "3.1. Data Collection\n",
      "The raw 3D model data for ShapeNet comes from public\n",
      "online repositories or existing research datasets. ShapeNet\n",
      "is intended to be an evolving repository with regular updates\n",
      "as more and more 3D models become available, as more\n",
      "people contribute annotations, and as the data captured with\n",
      "new 3D sensors become prevalent.\n",
      "We have collected 3D polygonal models from two\n",
      "popular public repositories: Trimble 3D Warehouse1 and\n",
      "Yobi3D2 . The Trimble 3D Warehouse contains 2.4M userdesigned 3D models and scenes. Yobi3D contains 350K\n",
      "additional models collected from a wide range of other online repositories. Together, they provide a diverse set of\n",
      "\n",
      "Though the tools developed for this project will be\n",
      "general-purpose, we intend to include only 3D models of\n",
      "objects encountered by people in the everyday world. That\n",
      "is, it will not include CAD mechanical parts, molecular\n",
      "structures, or other domain-specific objects. However, we\n",
      "will include scenes (e.g., office), objects (e.g., laptop computer), and parts of objects (e.g., keyboard). Models are\n",
      "organized under WordNet [21] noun “synsets” (synonym\n",
      "sets). WordNet provides a broad and deep taxonomy with\n",
      "over 80K distinct synsets representing distinct noun concepts arranged as a DAG network of hyponym relationships\n",
      "(e.g., “canary” is a hyponym of “bird”). This taxonomy has\n",
      "been used by ImageNet to describe categories of objects at\n",
      "\n",
      "1 https://3dwarehouse.sketchup.com/\n",
      "2 https://yobi3d.com\n",
      "\n",
      "3\n",
      "\n",
      "\fmultiple scales [4]. Though we first use WordNet due to its\n",
      "popularity, the ShapeNet UI is designed to allow multiple\n",
      "views into the collection of shapes that it contains, including different taxonomy views and faceted navigation.\n",
      "\n",
      "their semantics. For example, often different materials\n",
      "are associated with different parts. We intend to capture as much of that as possible into ShapeNet.\n",
      "• Symmetry: Bilateral symmetry planes and rotational\n",
      "symmetries are prevalent in artificial and natural objects, and deeply connected with the alignment and\n",
      "functionality of shapes. We refer to Section 4.4 for\n",
      "more details on how we compute symmetries for the\n",
      "shapes in ShapeNet.\n",
      "\n",
      "3.2. Annotation Types\n",
      "We envision ShapeNet as far more than a collection of\n",
      "3D models. ShapeNet will include a rich set of annotations that provide semantic information about those models, establish links between them, and links to other modalities of data (e.g., images). These annotations are exactly\n",
      "what make ShapeNet uniquely valuable. Figure 2 illustrates\n",
      "the value of this dense network of interlinked attributes on\n",
      "shapes, which we describe below.\n",
      "\n",
      "• Object Size: Object size is useful for many applications, such as reducing the hypothesis space in object\n",
      "recognition. Size annotations are discussed in Section 5.2.\n",
      "\n",
      "Language-related Annotations: Naming objects by\n",
      "their basic category is useful for indexing, grouping, and\n",
      "linking to related sources of data. As described in the previous section, we organize ShapeNet based on the WordNet [21] taxonomy. Synsets are interlinked with various\n",
      "relations, such as hyper and hyponym, and part-whole relations. Due to the popularity of WordNet, we can leverage\n",
      "other resources linked to WordNet such as ImageNet, ConceptNet, Freebase, and Wikipedia. In particular, linking to\n",
      "ImageNet [4] will help transport information between images and shapes. We assign each 3D model in ShapeNet to\n",
      "one or more synsets in the WordNet taxonomy (i.e., we populate each synset with a collection of shapes). Please refer\n",
      "to Section 4.1 for details on the acquisition and validation\n",
      "of basic category annotations. Future planned annotations\n",
      "include natural language descriptions of objects and object\n",
      "part-part relation descriptions.\n",
      "\n",
      "Functional Annotations: Many objects, especially manmade artifacts such as furniture and appliances, can be used\n",
      "by humans. Functional annotations describe these usage\n",
      "patterns. Such annotations are often highly correlated with\n",
      "specific regions of an object. In addition, it is often related\n",
      "with the specific type of human action. ShapeNet aims to\n",
      "store functional annotations at the global shape level and at\n",
      "the object part level.\n",
      "• Functional Parts: Parts are critical for understanding object structure, human activities involving a 3D\n",
      "shape, and ergonomic product design. We plan to annotate parts according to their function — in fact the\n",
      "very definition of parts has to be based on both geometric and functional criteria.\n",
      "• Affordances: We are interested in affordance annotations that are function and activity specific. Examples\n",
      "of such annotations include supporting plane annotations, and graspable region annotations for various object manipulations.\n",
      "\n",
      "Geometric Annotations: A critical property that distinguishes ShapeNet from image and video datasets is the fidelity with which 3D geometry represents real-world structures. We combine algorithmic predictions and manual\n",
      "annotations to organize shapes by category-level geometric properties and further derive rich geometric annotations\n",
      "from the raw 3D model geometry.\n",
      "\n",
      "Physical Annotations: Real objects exist in the physical\n",
      "world and typically have fixed physical properties such as\n",
      "dimensions and densities. Thus, it is important to store\n",
      "physical attribute annotations for 3D shapes.\n",
      "• Surface Material: We are especially interested in the\n",
      "optical properties and semantic names of surface materials. They are important for applications such as rendering and structural strength estimation.\n",
      "\n",
      "• Rigid Alignments: Establishing a consistent canonical orientation (e.g., upright and front) for every\n",
      "model is important for various tasks such as visualizing shapes [13], shape classification [8] and shape\n",
      "recognition [34]. Fortunately, most raw 3D model data\n",
      "is by default placed in an upright orientation, and the\n",
      "front orientations are typically aligned with an axis.\n",
      "This allows us to use a hierarchical clustering and\n",
      "alignment approach to ensure consistent rigid alignments within each category (see Section 4.2).\n",
      "\n",
      "• Weight: A basic property of objects which is very useful for physical simulations, and reasoning about stability and static support.\n",
      "In general, the issue of compact and informative representations for all the above attributes over shapes raises\n",
      "many interesting questions that we will need to address\n",
      "as part of the ShapeNet effort. Many annotations are currently ongoing projects and involve interesting open research problems.\n",
      "\n",
      "• Parts and Keypoints: Many shapes contain or have\n",
      "natural decompositions into important parts, as well as\n",
      "significant keypoints related to both their geometry and\n",
      "4\n",
      "\n",
      "\fLink to WordNet Taxonomy Alignment+Symmetry\n",
      "ImageNet\n",
      "\n",
      "Part Hierarchy\n",
      "\n",
      "Part Correspondences\n",
      "\n",
      "Backrest\n",
      "\n",
      "Swivel chair\n",
      "\n",
      "Dim: 50 x 45 x 5 cm\n",
      "Material: foam, fabric\n",
      "Mass: 5 Kg\n",
      "Function: support\n",
      "\n",
      "Seat\n",
      "\n",
      "WordNet synset\n",
      "Base\n",
      "\n",
      "Swivel chair: a chair that swivels\n",
      "on its base\n",
      "\n",
      "Leg\n",
      "Wheel\n",
      "\n",
      "Hypernyms: chair > seat > furniture > ...\n",
      "Part meronyms: backrest, seat, base\n",
      "Sister terms: armchair, barber chair, ...\n",
      "\n",
      "Figure 2. ShapeNet annotations illustrated for an example chair model. Left: links to the WordNet taxonomy provide definitions of objects,\n",
      "is-a and has-a relations, and a connection to images from ImageNet. Middle-left: shape is aligned to a consistent upright and front\n",
      "orientation, and symmetries are computed Middle-right: hierarchical decomposition of shape into parts on which various attributes are\n",
      "defined: names, symmetries, dimensions, materials, and masses. Right: part-to-part and point-to-point connections are established at all\n",
      "levels within ShapeNet producing a dense and semantically rich network of correspondences. The gray background indicates annotations\n",
      "that are currently ongoing and not yet available for release.\n",
      "\n",
      "3.3. Annotation Methodology\n",
      "\n",
      "3.4. Annotation Schema and Web API\n",
      "\n",
      "Though at first glance it might seem reasonable to collect\n",
      "the annotations we describe purely through manual human\n",
      "effort, we will in general take a hybrid approach. For annotation types where it is possible, we will first algorithmically\n",
      "predict the annotation for each model instance (e.g., global\n",
      "symmetry planes, consistent rigid alignments). We will then\n",
      "verify these predictions through crowd-sourcing pipelines\n",
      "and inspection by human experts. This hybrid strategy is\n",
      "sensible in the context of 3D shape data as there are already\n",
      "various algorithms we can leverage, and collecting corresponding annotations entirely through manual effort can be\n",
      "extremely labor intensive. In particular, since objects in a\n",
      "3D representation are both more pure and more complete\n",
      "than objects in images, we can expect better and easier to\n",
      "establish correspondences between 3D shapes, enabling algorithmic transport of semantic annotations. In many cases,\n",
      "the design of the human annotation interfaces themselves is\n",
      "an open question — which stands in contrast to largely manual image labeling efforts such as ImageNet. As a concrete\n",
      "example, shape part annotation can be presented and performed in various ways with different trade-offs in the type\n",
      "of obtained part annotation, the accuracy and the efficiency\n",
      "of the annotation process.\n",
      "Coupled with this hybrid annotation strategy, we also\n",
      "take particular care to preserve the provenance and confidence of each algorithmic and human annotation. The annotation source (whether an algorithm, or human effort), and\n",
      "a measure of the trust we can place in each annotation are\n",
      "critical pieces of information especially when we have to\n",
      "combine, aggregate, and reconcile several annotations.\n",
      "\n",
      "To provide convenient access to all of the model and annotation data contained within ShapeNet, we construct an\n",
      "index over all the 3D models and their associated annotations using the Apache Solr framework.3 Each stored annotation for a given 3D model is contained within the index\n",
      "as a separate attribute that can be easily queried and filtered\n",
      "through a simple web-based UI. In addition, to make the\n",
      "dataset conveniently accessible to researchers, we provide a\n",
      "batched download capability.\n",
      "\n",
      "4. Annotation Acquisition and Validation\n",
      "A key challenge in constructing ShapeNet is the methodology for acquiring and validating annotations. Our goal\n",
      "is to provide all annotations with high accuracy. In cases\n",
      "where full verification is not yet available, we aim to estimate a confidence metric for each annotation, as well as\n",
      "record its provenance. This will enable others to properly\n",
      "estimate the trustworthiness of the information we provide\n",
      "and use it for different applications.\n",
      "\n",
      "4.1. Category Annotation\n",
      "As described in Section 3.2, we assign each 3D model to\n",
      "one or more synsets in the WordNet taxonomy.\n",
      "Annotation Models are retrieved by textual query into the\n",
      "online repositories that we collected, and the initial category\n",
      "annotation is set to the used textual query for each retrieved\n",
      "3 http://lucene.apache.org/solr/\n",
      "\n",
      "5\n",
      "\n",
      "\fmodel. After we retrieve these models we use the popularity score of each model on the repository to sort models and\n",
      "ask human workers to verify the assigned category annotation. This is sensible since the more popular models tend to\n",
      "be high quality and correctly retrieved through the category\n",
      "keyword textual query. We stop verifying category annotations with people once the positive ratio is lower than a 2%\n",
      "threshold.\n",
      "Clean-up In order for the dataset to be easily usable by researchers it should contain clean and high quality 3D models. Through inspection, we identify and group 3D models\n",
      "into the following categories: single 3D models, 3D scenes,\n",
      "billboards, and big ground plane.\n",
      "• Single 3D models: semantically distinct objects; focus\n",
      "of our ShapeNetCore annotation effort.\n",
      "• 3D scenes: detected by counting the number of connected components in a voxelized representation. We\n",
      "manually verify these detections and mark scenes for\n",
      "future analysis.\n",
      "\n",
      "Figure 3. Examples of aligned models in the chair, laptop, bench,\n",
      "and airplane synsets.\n",
      "\n",
      "the concept of an upright orientation still applies throughout\n",
      "most levels of the taxonomy.\n",
      "Following the above discussion, it is natural for us to propose a hierarchical alignment method, with a small amount\n",
      "of human supervision. The basic idea is to hierarchically\n",
      "align models following the taxonomy of ShapeNet in a\n",
      "bottom-up manner, i.e., we start from aligning shapes in\n",
      "low-level categories and then gradually elevate to higher\n",
      "level categories. When we proceed to the higher level, the\n",
      "self-consistent orientation within a subcategory should be\n",
      "maintained. For the alignment at each level, we first use\n",
      "a geometric algorithm described in the Appendix A.1, and\n",
      "then ask human experts to check and correct possible misalignments. With this strategy, we efficiently obtain consistent orientations. In practice, most shapes in the same lowlevel categories can be well aligned algorithmically, requiring limited manual correction. Though the proportion of\n",
      "manual corrections increases for aligning higher-level categories, the number of categories at each level becomes logarithmically smaller.\n",
      "\n",
      "• Billboards: planes with a painted texture. Often used\n",
      "to represent people and trees. These models are generally not useful for geometric analysis. They can be\n",
      "detected by checking whether a single plane can fit all\n",
      "vertices.\n",
      "• Big ground plane: object of interest placed on a large\n",
      "horizontal plane or in front of large vertical plane. Although we do not currently use these models, the plane\n",
      "can easily be identified and removed through simple\n",
      "geometric analysis.\n",
      "We currently include the single 3D models in the\n",
      "ShapeNetCore subset of ShapeNet.\n",
      "\n",
      "4.2. Hierarchical Rigid Alignment\n",
      "The goal of this step is to establish a consistent canonical orientation for models within each category. Such\n",
      "alignment is important for various tasks such as visualizing\n",
      "shapes, shape classification and shape recognition. Figure 3\n",
      "shows several categories in ShapeNet that have been consistently aligned.\n",
      "Though the concept of consistent orientation seems natural, one issue has to be addressed. We explain by an example. “armchair”, “chair” and “seat” are three categories\n",
      "in our taxonomy, each being a subcategory of its successor. Consistent orientation can be well defined for shapes in\n",
      "the “armchair” category, by checking arms, legs and backs.\n",
      "Yet, it becomes difficult to define for the “chair” category.\n",
      "For example, “side chair” and “swivel chair” are both subcategories of “chair”, however, swivel chairs have a very\n",
      "different leg structure than most side chairs. It becomes\n",
      "even more ambiguous to define for “seat”, which has subcategories such as “stool”, “couch”, and “chair”. However,\n",
      "\n",
      "4.3. Parts and Keypoints\n",
      "To obtain part and keypoint annotations we start from\n",
      "some curated part annotations within each category. For\n",
      "parts, this acquisition can be speeded up by having algorithmically generated segmentations and then having users\n",
      "accept or modify parts from these. We intend to experiment\n",
      "with both 2D and 3D interfaces for this task. We then exploit a number of different algorithmic techniques to propagate this information to other nearby shapes. Such methods\n",
      "can rely on rigid alignments in 3D, feature descriptor alignments in an appropriately defined feature space, or general\n",
      "shape correspondences. We iterate this pipeline, using active learning to estimate the 3D models and regions of these\n",
      "6\n",
      "\n",
      "\fmodels where further human annotation would be most informative, generate a new set of crowd-sourced annotation\n",
      "tasks, algorithmically propagate their results, and so on. In\n",
      "the end we have users verify all proposed parts and keypoints, as verification is much faster than direct annotation.\n",
      "\n",
      "ber of shapes per category, as well as the number of categories.\n",
      "We observe that ShapeNet as a whole is strongly biased\n",
      "towards categories of rigid man-made artifacts, due to the\n",
      "bias of the source 3D model repositories. This is in contrast to common image database statistics that contain more\n",
      "natural objects such as plants and animals [30]. This distribution bias is probably due to a combination of factors: 1)\n",
      "meshes of natural objects are more difficult to design using\n",
      "common CAD software; 2) 3D model consumers are typically more interested in artificial objects such as those observed in modern urban lifestyles. The former factor can be\n",
      "mitigated in the near future by using the rapidly improving\n",
      "depth sensing and 3D scanning technology.\n",
      "\n",
      "4.4. Symmetry Estimation\n",
      "We provide bilateral symmetry plane detections for all\n",
      "3D models in ShapeNetCore. Our method is a modified\n",
      "version of [22]. The basic idea is to use hough transform\n",
      "to vote on the parameters of the symmetry plane. More\n",
      "specifically, we generate all combinations of pairs of vertices from the mesh. Each pair casts a vote of a possible\n",
      "symmetry plane in the discretized space of plane parameters partitioned evenly. We then pick the parameter with\n",
      "the most votes as the symmetry plane candidate. As a final\n",
      "step, this candidate is verified to ensure that every vertex\n",
      "has a symmetric counterpart.\n",
      "\n",
      "5.1. ShapeNetCore\n",
      "ShapeNetCore is a subset of the full ShapeNet dataset\n",
      "with single clean 3D models and manually verified category\n",
      "and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object\n",
      "categories of PASCAL 3D+[35], a popular computer vision\n",
      "3D benchmark dataset, are all covered by ShapeNetCore.\n",
      "The category distribution of ShapeNetCore is shown in Table 2.\n",
      "\n",
      "4.5. Physical Property Estimation\n",
      "Before computing physical attribute annotations, the dimensions of the models need to be correspond to the real\n",
      "world. We estimate the absolute dimensions of models using prior work in size estimation [25], followed by manual verification. With the given absolute dimensions, we\n",
      "now compute the total solid volume of each model through\n",
      "filled-in voxelization. We use the space carving approach\n",
      "implemented by Binvox [23]. Categories of objects that are\n",
      "known to be container-like (i.e., bottles, microwaves) are\n",
      "annotated as such and only the surface voxelization volume\n",
      "is used instead. We then estimate the proportional material composition of each object category and use a table of\n",
      "material densities along with each model instance volume\n",
      "to compute a rough total weight estimate for that instance.\n",
      "More details about the acquisition of these physical attribute\n",
      "annotations are available separately [26].\n",
      "\n",
      "5.2. ShapeNetSem\n",
      "ShapeNetSem is a smaller, more densely annotated subset consisting of 12,000 models spread over a broader set of\n",
      "270 categories. In addition to manually verified category\n",
      "labels and consistent alignments, these models are annotated with real-world dimensions, estimates of their material composition at the category level, and estimates of their\n",
      "total volume and weight. The total numbers of models for\n",
      "the top 100 categories in this subset are given in Table 3.\n",
      "\n",
      "6. Discussion and Future Work\n",
      "The construction of ShapeNet is a continuous, ongoing\n",
      "effort. Here we have just described the initial steps we have\n",
      "taken in defining ShapeNet and populating a core subset\n",
      "of model annotations that we hope will prove useful to the\n",
      "community. We plan to grow ShapeNet in four distinct directions:\n",
      "\n",
      "5. Current Statistics\n",
      "At the time of this technical report, ShapeNet has indexed roughly 3,000,000 models. 220,000 models of\n",
      "these models are classified into 3,135 categories (WordNet synsets). Below we provide detailed statistics for the\n",
      "currently annotated models in ShapeNet as a whole, as\n",
      "well as details of the available publicly released subsets of\n",
      "ShapeNet.\n",
      "\n",
      "Additional annotation types We will introduce several\n",
      "additional types of annotations that have strong connections\n",
      "to the semantics and functionality of objects. Firstly, hierarchical part decompositions of objects will provide a useful\n",
      "finer granularity description of object structure that can be\n",
      "leveraged for part segmentation and shape synthesis. Secondly, physical object property annotations such as materials and their attributes will allow higher fidelity physics and\n",
      "appearance simulation, adding another layer of understanding to methods in vision and graphics.\n",
      "\n",
      "Category Distribution Figure 4 shows the distributions\n",
      "of the number of shapes per synset at various taxonomy\n",
      "levels for the current ShapeNetCore corpus. To the best of\n",
      "our knowledge, ShapeNet is the largest clean shape dataset\n",
      "available in terms of total number of shapes, average num7\n",
      "\n",
      "\fRoot\n",
      "\n",
      "ar\n",
      "t\n",
      "i\n",
      "f\n",
      "act\n",
      "pl\n",
      "ant\n",
      "per\n",
      "son\n",
      "ani\n",
      "mal\n",
      "at\n",
      "hl\n",
      "et\n",
      "i\n",
      "cs\n",
      "nat\n",
      "ur\n",
      "alobj\n",
      "ect\n",
      "geol\n",
      "ogi\n",
      "cal\n",
      "0K 5K 10K 15K 20K 25K 30K 35K 40K 45K 50K 55K 60K 65K 70K 75K 80K 85K 90K 95K100K105K110K115K120K\n",
      "\n",
      "Ar\n",
      "t\n",
      "i\n",
      "f\n",
      "act\n",
      "\n",
      "devi\n",
      "ce\n",
      "f\n",
      "ur\n",
      "ni\n",
      "t\n",
      "ur\n",
      "e\n",
      "cont\n",
      "ai\n",
      "ner\n",
      "t\n",
      "r\n",
      "anspor\n",
      "t\n",
      "equi\n",
      "pment\n",
      "i\n",
      "mpl\n",
      "ement\n",
      "weaponr\n",
      "y\n",
      "0K\n",
      "\n",
      "2K\n",
      "\n",
      "4K\n",
      "\n",
      "6K\n",
      "\n",
      "8K\n",
      "\n",
      "10K\n",
      "\n",
      "12K\n",
      "\n",
      "14K\n",
      "\n",
      "16K\n",
      "\n",
      "18K\n",
      "\n",
      "Fur\n",
      "ni\n",
      "t\n",
      "ur\n",
      "e\n",
      "\n",
      "seat\n",
      "t\n",
      "abl\n",
      "e\n",
      "cabi\n",
      "net\n",
      "l\n",
      "amp\n",
      "bedr\n",
      "oom f\n",
      "ur\n",
      "ni\n",
      "t\n",
      "ur\n",
      "e\n",
      "bookcase\n",
      "war\n",
      "dr\n",
      "obe\n",
      "of\n",
      "f\n",
      "i\n",
      "cef\n",
      "ur\n",
      "ni\n",
      "t\n",
      "ur\n",
      "e\n",
      "0K\n",
      "\n",
      "Seat\n",
      "\n",
      "1K\n",
      "\n",
      "2K\n",
      "\n",
      "3K\n",
      "\n",
      "desk\n",
      "r\n",
      "ect\n",
      "angul\n",
      "art\n",
      "abl\n",
      "e\n",
      "wor\n",
      "kshopt\n",
      "abl\n",
      "e\n",
      "si\n",
      "det\n",
      "abl\n",
      "e\n",
      "count\n",
      "er\n",
      "shor\n",
      "tt\n",
      "abl\n",
      "e\n",
      "wor\n",
      "kt\n",
      "abl\n",
      "e\n",
      "0\n",
      "\n",
      "5K\n",
      "\n",
      "6K\n",
      "\n",
      "7K\n",
      "\n",
      "Chai\n",
      "r\n",
      "\n",
      "chai\n",
      "r\n",
      "sof\n",
      "a\n",
      "st\n",
      "ool\n",
      "bench\n",
      "ot\n",
      "t\n",
      "oman\n",
      "t\n",
      "oi\n",
      "l\n",
      "etseat\n",
      "0K 1K\n",
      "\n",
      "Tabl\n",
      "e\n",
      "\n",
      "4K\n",
      "\n",
      "si\n",
      "dechai\n",
      "r\n",
      "ar\n",
      "mchai\n",
      "r\n",
      "cl\n",
      "ubchai\n",
      "r\n",
      "swi\n",
      "velchai\n",
      "r\n",
      "f\n",
      "ol\n",
      "di\n",
      "ngchai\n",
      "r\n",
      "c\n",
      "a\n",
      "n\n",
      "t\n",
      "i\n",
      "l\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "c\n",
      "h\n",
      "a\n",
      "i\n",
      "r\n",
      "2K 3K 4K 5K 6K\n",
      "chai\n",
      "se\n",
      "r\n",
      "ocki\n",
      "ngchai\n",
      "r\n",
      "l\n",
      "awnchai\n",
      "r\n",
      "r\n",
      "exchai\n",
      "r\n",
      "z\n",
      "i\n",
      "gz\n",
      "agchai\n",
      "r\n",
      "wheel\n",
      "chai\n",
      "r\n",
      "bar\n",
      "cel\n",
      "onachai\n",
      "r\n",
      "t\n",
      "ul\n",
      "i\n",
      "pchai\n",
      "r\n",
      "bal\n",
      "lchai\n",
      "r\n",
      "500 1000 1500 2000 2500\n",
      "0\n",
      "\n",
      "8K\n",
      "\n",
      "20K\n",
      "\n",
      "22K\n",
      "\n",
      "24K\n",
      "\n",
      "Wheel\n",
      "edVehi\n",
      "cl\n",
      "e\n",
      "mot\n",
      "orvehi\n",
      "cl\n",
      "e\n",
      "ar\n",
      "mor\n",
      "edvehi\n",
      "cl\n",
      "e\n",
      "r\n",
      "ecr\n",
      "eat\n",
      "i\n",
      "onalvehi\n",
      "cl\n",
      "e\n",
      "t\n",
      "r\n",
      "ackedvehi\n",
      "cl\n",
      "e\n",
      "t\n",
      "r\n",
      "ai\n",
      "l\n",
      "er\n",
      "l\n",
      "ocomot\n",
      "i\n",
      "ve\n",
      "r\n",
      "ai\n",
      "l\n",
      "car\n",
      "t\n",
      "r\n",
      "act\n",
      "or\n",
      "skat\n",
      "eboar\n",
      "d\n",
      "0K\n",
      "\n",
      "26K\n",
      "\n",
      "1K\n",
      "\n",
      "28K\n",
      "\n",
      "30K\n",
      "\n",
      "2K\n",
      "\n",
      "32K\n",
      "\n",
      "3K\n",
      "\n",
      "34K\n",
      "\n",
      "4K\n",
      "\n",
      "5K\n",
      "\n",
      "Car\n",
      "\n",
      "sedan\n",
      "spor\n",
      "tut\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "coupe\n",
      "r\n",
      "acer\n",
      "spor\n",
      "t\n",
      "scar\n",
      "conver\n",
      "t\n",
      "i\n",
      "bl\n",
      "e\n",
      "l\n",
      "i\n",
      "mousi\n",
      "ne\n",
      "mi\n",
      "ni\n",
      "van\n",
      "ambul\n",
      "ance\n",
      "r\n",
      "oadst\n",
      "er\n",
      "st\n",
      "at\n",
      "i\n",
      "onwagon\n",
      "t\n",
      "our\n",
      "i\n",
      "ngcar\n",
      "cr\n",
      "ui\n",
      "ser\n",
      "j\n",
      "eep\n",
      "hotr\n",
      "od\n",
      "hat\n",
      "chback\n",
      "cab\n",
      "500 1000 1500 20002500\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300 400\n",
      "\n",
      "Figure 4. Plots of the distribution of ShapeNet models over WordNet synsets at multiple levels of the taxonomy (only the top few children\n",
      "synsets are shown at each level). The highest level (root) is at the top and the taxonomy levels become lower downwards and to the right.\n",
      "Note the bias towards rigid man-made artifacts at the top and the broad coverage of many low level categories towards the bottom.\n",
      "\n",
      "ID\n",
      "04379243\n",
      "02958343\n",
      "03001627\n",
      "02691156\n",
      "04256520\n",
      "04090263\n",
      "03636649\n",
      "04530566\n",
      "02828884\n",
      "03691459\n",
      "02933112\n",
      "03211117\n",
      "04401088\n",
      "02924116\n",
      "02808440\n",
      "03467517\n",
      "03325088\n",
      "03046257\n",
      "03991062\n",
      "\n",
      "Name\n",
      "table\n",
      "car\n",
      "chair\n",
      "airplane\n",
      "sofa\n",
      "rifle\n",
      "lamp\n",
      "watercraft\n",
      "bench\n",
      "loudspeaker\n",
      "cabinet\n",
      "display\n",
      "telephone\n",
      "bus\n",
      "bathtub\n",
      "guitar\n",
      "faucet\n",
      "clock\n",
      "flowerpot\n",
      "\n",
      "Num\n",
      "8443\n",
      "7497\n",
      "6778\n",
      "4045\n",
      "3173\n",
      "2373\n",
      "2318\n",
      "1939\n",
      "1816\n",
      "1618\n",
      "1572\n",
      "1095\n",
      "1052\n",
      "939\n",
      "857\n",
      "797\n",
      "744\n",
      "655\n",
      "602\n",
      "\n",
      "ID\n",
      "03593526\n",
      "02876657\n",
      "02871439\n",
      "03642806\n",
      "03624134\n",
      "04468005\n",
      "02747177\n",
      "03790512\n",
      "03948459\n",
      "03337140\n",
      "02818832\n",
      "03928116\n",
      "04330267\n",
      "03797390\n",
      "02880940\n",
      "04554684\n",
      "04004475\n",
      "03513137\n",
      "03761084\n",
      "\n",
      "Name\n",
      "jar\n",
      "bottle\n",
      "bookshelf\n",
      "laptop\n",
      "knife\n",
      "train\n",
      "trash bin\n",
      "motorbike\n",
      "pistol\n",
      "file cabinet\n",
      "bed\n",
      "piano\n",
      "stove\n",
      "mug\n",
      "bowl\n",
      "washer\n",
      "printer\n",
      "helmet\n",
      "microwaves\n",
      "\n",
      "Num\n",
      "597\n",
      "498\n",
      "466\n",
      "460\n",
      "424\n",
      "389\n",
      "343\n",
      "337\n",
      "307\n",
      "298\n",
      "254\n",
      "239\n",
      "218\n",
      "214\n",
      "186\n",
      "169\n",
      "166\n",
      "162\n",
      "152\n",
      "\n",
      "ID\n",
      "04225987\n",
      "04460130\n",
      "02942699\n",
      "02801938\n",
      "02946921\n",
      "03938244\n",
      "03710193\n",
      "03207941\n",
      "04099429\n",
      "02773838\n",
      "02843684\n",
      "03261776\n",
      "03759954\n",
      "04074963\n",
      "03085013\n",
      "02834778\n",
      "02954340\n",
      "\n",
      "Name\n",
      "skateboard\n",
      "tower\n",
      "camera\n",
      "basket\n",
      "can\n",
      "pillow\n",
      "mailbox\n",
      "dishwasher\n",
      "rocket\n",
      "bag\n",
      "birdhouse\n",
      "earphone\n",
      "microphone\n",
      "remote\n",
      "keyboard\n",
      "bicycle\n",
      "cap\n",
      "\n",
      "Num\n",
      "152\n",
      "133\n",
      "113\n",
      "113\n",
      "108\n",
      "96\n",
      "94\n",
      "93\n",
      "85\n",
      "83\n",
      "73\n",
      "73\n",
      "67\n",
      "67\n",
      "65\n",
      "59\n",
      "56\n",
      "\n",
      "Total\n",
      "\n",
      "57386\n",
      "\n",
      "Table 2. Statistics of ShapeNetCore synsets. ID corresponds to WordNet synset offset, which is aligned with ImageNet.\n",
      "\n",
      "8\n",
      "\n",
      "\fCategory\n",
      "Chair\n",
      "Lamp\n",
      "ChestOfDrawers\n",
      "Table\n",
      "Couch\n",
      "Computer\n",
      "Dresser\n",
      "TV\n",
      "WallArt\n",
      "Bed\n",
      "Cabinet\n",
      "FloorLamp\n",
      "Desk\n",
      "PottedPlant\n",
      "FoodItem\n",
      "Laptop\n",
      "Vase\n",
      "TableLamp\n",
      "OfficeChair\n",
      "CellPhone\n",
      "\n",
      "Num\n",
      "696\n",
      "663\n",
      "511\n",
      "427\n",
      "413\n",
      "244\n",
      "234\n",
      "233\n",
      "222\n",
      "221\n",
      "221\n",
      "201\n",
      "189\n",
      "188\n",
      "180\n",
      "173\n",
      "163\n",
      "142\n",
      "137\n",
      "130\n",
      "\n",
      "Category\n",
      "Monitor\n",
      "RoundTable\n",
      "TrashBin\n",
      "DrinkingUtensil\n",
      "DeskLamp\n",
      "Clock\n",
      "ToyFigure\n",
      "Plant\n",
      "Armoire\n",
      "QueenBed\n",
      "Stool\n",
      "EndTable\n",
      "Bottle\n",
      "DiningTable\n",
      "Bookcase\n",
      "CeilingLamp\n",
      "Bench\n",
      "Book\n",
      "CoffeeTable\n",
      "Pencil\n",
      "\n",
      "Num\n",
      "127\n",
      "120\n",
      "117\n",
      "112\n",
      "110\n",
      "101\n",
      "101\n",
      "98\n",
      "95\n",
      "94\n",
      "92\n",
      "91\n",
      "88\n",
      "88\n",
      "87\n",
      "86\n",
      "84\n",
      "84\n",
      "81\n",
      "80\n",
      "\n",
      "Category\n",
      "WallLamp\n",
      "SideChair\n",
      "VideoGameConsole\n",
      "MediaStorage\n",
      "Painting\n",
      "Desktop\n",
      "AccentTable\n",
      "Camera\n",
      "Picture\n",
      "Refrigerator\n",
      "Speaker\n",
      "Sideboard\n",
      "Barstool\n",
      "Guitar\n",
      "MediaPlayer\n",
      "Ipod\n",
      "PersonStanding\n",
      "Piano\n",
      "Curtain\n",
      "Candle\n",
      "\n",
      "Num\n",
      "78\n",
      "77\n",
      "75\n",
      "73\n",
      "73\n",
      "71\n",
      "70\n",
      "70\n",
      "69\n",
      "68\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "62\n",
      "59\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "\n",
      "Category\n",
      "Gun\n",
      "Nightstand\n",
      "Mug\n",
      "AccentChair\n",
      "ChessBoard\n",
      "Rug\n",
      "WallUnit\n",
      "Mirror\n",
      "Bowl\n",
      "SodaCan\n",
      "VideoGameController\n",
      "WallClock\n",
      "Printer\n",
      "Sword\n",
      "USBStick\n",
      "Chaise\n",
      "OfficeSideChair\n",
      "Poster\n",
      "Sink\n",
      "Telephone\n",
      "\n",
      "Num\n",
      "54\n",
      "53\n",
      "51\n",
      "50\n",
      "49\n",
      "49\n",
      "46\n",
      "45\n",
      "44\n",
      "44\n",
      "44\n",
      "43\n",
      "42\n",
      "40\n",
      "40\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "\n",
      "Category\n",
      "FlagPole\n",
      "TvStand\n",
      "Fireplace\n",
      "Rack\n",
      "LightSwitch\n",
      "Oven\n",
      "Airplane\n",
      "DresserWithMirror\n",
      "Calculator\n",
      "TableClock\n",
      "Toilet\n",
      "Cup\n",
      "Stapler\n",
      "PaperBox\n",
      "SpaceShip\n",
      "Toy\n",
      "ToiletPaper\n",
      "Knife\n",
      "PictureFrame\n",
      "Recliner\n",
      "\n",
      "Num\n",
      "38\n",
      "38\n",
      "37\n",
      "37\n",
      "36\n",
      "36\n",
      "35\n",
      "35\n",
      "34\n",
      "34\n",
      "34\n",
      "33\n",
      "33\n",
      "32\n",
      "32\n",
      "32\n",
      "31\n",
      "30\n",
      "30\n",
      "30\n",
      "\n",
      "Table 3. Total number of models for the top 100 ShapeNetSem categories (out of 270 categories). Each category is also linked to the\n",
      "corresponding WordNet synset, establishing the same linkage to WordNet and ImageNet as with ShapeNetCore.\n",
      "\n",
      "Data-driven research By establishing ShapeNet as the\n",
      "first large-scale 3D shape dataset of its kind we can help\n",
      "to move computer graphics research toward a data-driven\n",
      "direction following recent developments in vision and NLP.\n",
      "Additionally, we can help to enable larger-scale quantitative\n",
      "analysis of proposed systems that can clarify the benefits of\n",
      "particular methodologies against a broader and more representative variety of 3D model data.\n",
      "\n",
      "Correspondences One of the most important goals of\n",
      "ShapeNet is to provide a dense network of correspondences\n",
      "between 3D models and their parts. This will be invaluable for enabling much shape analysis research and helping\n",
      "to improve and evaluate methods for many traditional tasks\n",
      "such as alignment and segmentation. Additionally, we plan\n",
      "to provide correspondences between 3D model parts and\n",
      "image patches in ImageNet — a link that will be critical\n",
      "for propagating information between image space and 3D\n",
      "models.\n",
      "\n",
      "Training resource By providing a large-scale, richly annotated dataset we can also promote a broad class of recently resurgent machine learning and neural network methods for applications dealing with geometric data. Much\n",
      "like research in computer vision and natural language understanding, computational geometry and graphics stand\n",
      "to benefit immensely from these data-driven learning approaches.\n",
      "\n",
      "RGB-D data The rapid proliferation of commodity\n",
      "RGB-D sensors is already making the process of capturing\n",
      "real-world environments better and more efficient. Expanding ShapeNet to include shapes reconstructed from scanned\n",
      "RGB-D data is a critical goal. We foresee that over time,\n",
      "the amount of available reconstructed shape data will overshadow the existing designed 3D model data and as such\n",
      "this is a natural growth direction for ShapeNet. A related effort that we are currently undertaking is to align 3D models\n",
      "to objects observed in RGB-D frames. This will establish\n",
      "a powerful connection between real world observations and\n",
      "3D models.\n",
      "\n",
      "Benchmark dataset We hope that ShapeNet will grow to\n",
      "become a canonical benchmark dataset for several evaluation tasks and challenges. In this way, we would like to engage the broader research community in helping us define\n",
      "and grow ShapeNet to be a pivotal dataset with long-lasting\n",
      "impact.\n",
      "\n",
      "Annotation coverage We will continue to expand the set\n",
      "of annotated models to cover a bigger subset of the entirety\n",
      "of ShapeNet. We will explore combinations of algorithmic\n",
      "propagation methods and crowd-sourcing for verification of\n",
      "the algorithmic results.\n",
      "\n",
      "References\n",
      "[1] Helen M Berman, John Westbrook, Zukang Feng, Gary\n",
      "Gilliland, TN Bhat, Helge Weissig, Ilya N Shindyalov, and\n",
      "Philip E Bourne. The protein data bank. Nucleic Acids Res,\n",
      "28:235–242, 2000. 2\n",
      "\n",
      "7. Conclusion\n",
      "\n",
      "[2] Xiaobai Chen, Aleksey Golovinskiy, and Thomas\n",
      "Funkhouser. A benchmark for 3D mesh segmentation.\n",
      "ACM TOG, 28(3):73:1–73:12, July 2009. 2\n",
      "\n",
      "We firmly believe that ShapeNet will prove to be an immensely useful resource to several research communities in\n",
      "several ways:\n",
      "9\n",
      "\n",
      "\fLarge scale comprehensive 3D shape retrieval. In Eurographics Workshop on 3D Object Retrieval, 2014. 2\n",
      "\n",
      "[3] Xiaobai Chen, Abulhair Saparov, Bill Pang, and Thomas\n",
      "Funkhouser. Schelling points on 3D surface meshes. ACM\n",
      "TOG, August 2012. 2\n",
      "\n",
      "[18] Joerg Liebelt and Cordelia Schmid. Multi-view object class\n",
      "detection with a 3D geometric model. In CVPR, pages 1688–\n",
      "1695. IEEE, 2010. 2\n",
      "\n",
      "[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\n",
      "and Li Fei-Fei. ImageNet: A large-scale hierarchical image\n",
      "database. In CVPR, 2009. 1, 2, 4\n",
      "\n",
      "[19] Tianqiang Liu, Siddhartha Chaudhuri, Vladimir G. Kim, QiXing Huang, Niloy J. Mitra, and Thomas Funkhouser. Creating consistent scene graphs using a probabilistic grammar.\n",
      "ACM TOG, December 2014. 2\n",
      "\n",
      "[5] Bianca Falcidieno.\n",
      "Aim@shape.\n",
      "http://www.\n",
      "aimatshape.net/ontologies/shapes/, 2005. 2\n",
      "[6] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas\n",
      "Funkhouser, and Pat Hanrahan. Example-based synthesis of\n",
      "3D object arrangements. ACM TOG, 31(6):135, 2012. 1\n",
      "\n",
      "[20] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice\n",
      "Santorini. Building a large annotated corpus of english: The\n",
      "Penn Treebank. Computational linguistics, 19(2):313–330,\n",
      "1993. 1\n",
      "\n",
      "[7] Paul-Louis George. Gamma. http://www.rocq.\n",
      "inria.fr/gamma/download/download.php,\n",
      "2007. 2\n",
      "\n",
      "[21] George A. Miller. WordNet: a lexical database for English.\n",
      "CACM, 1995. 1, 2, 3, 4\n",
      "\n",
      "[8] Qixing Huang, Hao Su, and Leonidas Guibas. Fine-grained\n",
      "semi-supervised labeling of large shape collections. ACM\n",
      "TOG, 32:190:1–190:10, 2013. 4, 11\n",
      "\n",
      "[22] Niloy J Mitra, Mark Pauly, Michael Wand, and Duygu Ceylan. Symmetry in 3D geometry: Extraction and applications.\n",
      "In Computer Graphics Forum, volume 32, pages 1–23, 2013.\n",
      "7\n",
      "\n",
      "[9] Subramaniam Jayanti, Yagnanarayanan Kalyanaraman, Natraj Iyer, and Karthik Ramani. Developing an engineering\n",
      "shape benchmark for CAD models. Computer-Aided Design,\n",
      "2006. 3\n",
      "\n",
      "[23] Fakir S. Nooruddin and Greg Turk. Simplification and repair\n",
      "of polygonal models using volumetric techniques. Visualization and Computer Graphics, IEEE Transactions on, 2003.\n",
      "7\n",
      "\n",
      "[10] Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne\n",
      "Koller, and Vladlen Koltun. A probabilistic model for\n",
      "component-based shape synthesis. ACM TOG, 31:55, 2012.\n",
      "1\n",
      "\n",
      "[24] Bryan C Russell and Antonio Torralba. Building a database\n",
      "of 3D scenes from user annotations. In CVPR, 2009. 2\n",
      "\n",
      "[11] Vladimir Kim, Yaron Lipman, Xiaobai Chen, and Thomas\n",
      "Funkhouser. Mobius transformations for global intrinsic\n",
      "symmetry analysis. Symposium on Geometry Processing,\n",
      "July 2010. 2\n",
      "\n",
      "[25] Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. On being the right\n",
      "scale: Sizing large collections of 3D models. In SIGGRAPH\n",
      "Asia 2014 Workshop on Indoor Scene Understanding: Where\n",
      "Graphics meets Vision, 2014. 7\n",
      "\n",
      "[12] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha\n",
      "Chaudhuri, Stephen DiVerdi, and Thomas Funkhouser.\n",
      "Learning part-based templates from large collections of 3D\n",
      "shapes. ACM TOG, 32(4):70:1–70:12, July 2013. 2\n",
      "\n",
      "[26] Manolis Savva, Angel X. Chang, and Pat Hanrahan.\n",
      "Semantically-Enriched 3D Models for Common-sense\n",
      "Knowledge.\n",
      "CVPR 2015 Workshop on Functionality,\n",
      "Physics, Intentionality and Causality, 2015. 7\n",
      "\n",
      "[13] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen\n",
      "DiVerdi, and Thomas Funkhouser. Exploring collections\n",
      "of 3D models using fuzzy correspondences. ACM TOG,\n",
      "31(4):54:1–54:11, July 2012. 2, 4\n",
      "\n",
      "[27] Philip Shilane, Patrick Min, Michael Kazhdan, and Thomas\n",
      "Funkhouser. The Princeton shape benchmark. In Shape\n",
      "Modeling Applications. IEEE, 2004. 2, 3\n",
      "\n",
      "[14] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n",
      "3D object representations for fine-grained categorization. In\n",
      "4th International IEEE Workshop on 3D Representation and\n",
      "Recognition (3dRR-13), Sydney, Australia, 2013. 2\n",
      "\n",
      "[28] Shuran Song and Jianxiong Xiao. Sliding shapes for 3D object detection in depth images. In ECCV, 2014. 1\n",
      "\n",
      "[15] Roman A Laskowski, E Gail Hutchinson, Alex D Michie,\n",
      "Andrew C Wallace, Martin L Jones, and Janet M Thornton.\n",
      "PDBsum: A web-based database of summaries and analyses\n",
      "of all PDB structures. Trends Biochem. Sci., 22:488–490,\n",
      "1997. 2\n",
      "\n",
      "[29] Atsushi Tatsuma, Hitoshi Koyanagi, and Masaki Aono. A\n",
      "large-scale shape benchmark for 3D object retrieval: Toyohashi shape benchmark. In Asia Pacific Signal and Information Processing Association, 2012. 3\n",
      "[30] Antonio Torralba, Bryan C Russell, and Jenny Yuen. LabelMe: Online image annotation and applications. Proceedings of the IEEE, 98(8):1467–1484, 2010. 7\n",
      "\n",
      "[16] Bo Li, Afzal Godil, Masaki Aono, X Bai, Takahiko Furuya,\n",
      "L Li, R López-Sastre, Henry Johan, Ryutarou Ohbuchi, Carolina Redondo-Cabrera, et al. SHREC’12 track: generic 3D\n",
      "shape retrieval. In 5th Eurographics Conference on 3D Object Retrieval, 2012. 3\n",
      "\n",
      "[31] Remco C. Veltkamp and FB ter Harr. SHREC 2007 3D shape\n",
      "retrieval contest. Technical report, Utrecht University Technical Report UU-CS-2007-015, 2007. 3\n",
      "\n",
      "[17] Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias\n",
      "Schreck, Masaki Aono, Qiang Chen, Nihad Karim Chowdhury, Bin Fang, Takahiko Furuya, et al. SHREC’14 track:\n",
      "\n",
      "[32] Dejan V Vranić. 3D model retrieval. University of Leipzig,\n",
      "Germany, PhD thesis, 2004. 3\n",
      "\n",
      "10\n",
      "\n",
      "\fA. Appendix\n",
      "\n",
      "[33] Raoul Wessel, Ina Blümel, and Reinhard Klein. A 3D shape\n",
      "benchmark for retrieval and automatic classification of architectural data. In Eurographics 2009 Workshop on 3D Object Retrieval, pages 53–56. The Eurographics Association,\n",
      "2009. 3\n",
      "\n",
      "A.1. Hierarchical Rigid Alignment\n",
      "In the following, we describe our hierarchical rigid alignment algorithm in more detail.\n",
      "As a pre-processing step, we first semi-automatically\n",
      "align the upright orientation of each shape. Fortunately,\n",
      "most shapes downloaded from the web are by default placed\n",
      "in the upright orientations. For those that are not, we filter\n",
      "them out by manual inspection. We then convert models to\n",
      "point clouds through furthest point sampling and perform\n",
      "PCA on the point sets. Finally, we ask a person to pick\n",
      "the vector of correct upright orientation from six candidates\n",
      "containing the PCA axes and their reverse directions.\n",
      "Starting from a leaf category in ShapeNet, we jointly\n",
      "align all shapes following prior work [8]. If a leaf category has more than 100 shapes, we further partition it into\n",
      "smaller, more coherent clusters by k-means clustering using pose-invariant global features, such as phase-invariant\n",
      "HoG features [see appendix]. Here we briefly review [8].\n",
      "Each shape is associated with a random variable, denoting the transformation of the shape from its original pose\n",
      "to the consistent canonical pose. Over the set of shapes, a\n",
      "Markov Random Field (MRF) is constructed, whose energy\n",
      "function measures the consistency of all pairs of shapes after applying their transformations. In practice, the space of\n",
      "rigid transformations is discretized into N bins. We perform\n",
      "MAP inference over the MRF to find the optimal transformation for each shape. We then manual inspect the results\n",
      "and correct occasional errors.\n",
      "After this step, we represent each leaf node category by\n",
      "the shape in the centroid of the feature space. Then, we\n",
      "gather the representative shapes for all leaf categories of an\n",
      "intermediate category and apply [8] again for joint alignment. This higher-level algorithmic alignment is verified\n",
      "by a person again. The procedure is applied along the taxonomy hierarchy until the root node is reached.\n",
      "\n",
      "[34] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D\n",
      "ShapeNets: A Deep Representation for Volumetric Shapes.\n",
      "CVPR, 2015. 1, 2, 4\n",
      "[35] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\n",
      "PASCAL: A benchmark for 3D object detection in the wild.\n",
      "In WACV, 2014. 2, 7\n",
      "[36] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.\n",
      "SUN3D: A database of big spaces reconstructed using SfM\n",
      "and object labels. In ICCV, pages 1625–1632, 2013. 2\n",
      "[37] Juan Zhang, Kaleem Siddiqi, Diego Macrini, Ali Shokoufandeh, and Sven Dickinson. Retrieving articulated 3-D models using medial surfaces and their graph spectra. In Energy\n",
      "minimization methods in computer vision and pattern recognition, 2005. 3\n",
      "\n",
      "11\n",
      "\n",
      "\f\n",
      "33\n",
      "['[1] Helen M Berman, John Westbrook, Zukang Feng, Gary\\nGilliland, TN Bhat, Helge Weissig, Ilya N Shindyalov, and\\nPhilip E Bourne. The protein data bank. Nucleic Acids Res,\\n28:235–242, 2000.', '[2] Xiaobai Chen, Aleksey Golovinskiy, and Thomas\\nFunkhouser. A benchmark for 3D mesh segmentation.\\nACM TOG, 28(3):73:1–73:12, July 2009.', '[3] Xiaobai Chen, Abulhair Saparov, Bill Pang, and Thomas\\nFunkhouser. Schelling points on 3D surface meshes. ACM\\nTOG, August 2012.', '[18] Joerg Liebelt and Cordelia Schmid. Multi-view object class\\ndetection with a 3D geometric model. In CVPR, pages 1688–\\n1695. IEEE, 2010.', '[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\\ndatabase. In CVPR, 2009.', '[19] Tianqiang Liu, Siddhartha Chaudhuri, Vladimir G. Kim, QiXing Huang, Niloy J. Mitra, and Thomas Funkhouser. Creating consistent scene graphs using a probabilistic grammar.\\nACM TOG, December 2014.', '[5] Bianca Falcidieno.\\nAim@shape.\\nhttp://www.\\naimatshape.net/ontologies/shapes/, 2005.', '[6] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas\\nFunkhouser, and Pat Hanrahan. Example-based synthesis of\\n3D object arrangements. ACM TOG, 31(6):135, 2012.', '[20] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice\\nSantorini. Building a large annotated corpus of english: The\\nPenn Treebank. Computational linguistics, 19(2):313–330,\\n1993.', '[7] Paul-Louis George. Gamma. http://www.rocq.\\ninria.fr/gamma/download/download.php,\\n2007.', '[21] George A. Miller. WordNet: a lexical database for English.\\nCACM, 1995.', '[8] Qixing Huang, Hao Su, and Leonidas Guibas. Fine-grained\\nsemi-supervised labeling of large shape collections. ACM\\nTOG, 32:190:1–190:10, 2013.', '[22] Niloy J Mitra, Mark Pauly, Michael Wand, and Duygu Ceylan. Symmetry in 3D geometry: Extraction and applications.\\nIn Computer Graphics Forum, volume 32, pages 1–23, 2013.', '[9] Subramaniam Jayanti, Yagnanarayanan Kalyanaraman, Natraj Iyer, and Karthik Ramani. Developing an engineering\\nshape benchmark for CAD models. Computer-Aided Design,\\n2006.', '[23] Fakir S. Nooruddin and Greg Turk. Simplification and repair\\nof polygonal models using volumetric techniques. Visualization and Computer Graphics, IEEE Transactions on, 2003.', '[10] Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne\\nKoller, and Vladlen Koltun. A probabilistic model for\\ncomponent-based shape synthesis. ACM TOG, 31:55, 2012.', '[24] Bryan C Russell and Antonio Torralba. Building a database\\nof 3D scenes from user annotations. In CVPR, 2009.', '[11] Vladimir Kim, Yaron Lipman, Xiaobai Chen, and Thomas\\nFunkhouser. Mobius transformations for global intrinsic\\nsymmetry analysis. Symposium on Geometry Processing,\\nJuly 2010.', '[25] Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. On being the right\\nscale: Sizing large collections of 3D models. In SIGGRAPH\\nAsia 2014 Workshop on Indoor Scene Understanding: Where\\nGraphics meets Vision, 2014.', '[12] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha\\nChaudhuri, Stephen DiVerdi, and Thomas Funkhouser.\\nLearning part-based templates from large collections of 3D\\nshapes. ACM TOG, 32(4):70:1–70:12, July 2013.', '[26] Manolis Savva, Angel X. Chang, and Pat Hanrahan.\\nSemantically-Enriched 3D Models for Common-sense\\nKnowledge.\\nCVPR 2015 Workshop on Functionality,\\nPhysics, Intentionality and Causality, 2015.', '[13] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen\\nDiVerdi, and Thomas Funkhouser. Exploring collections\\nof 3D models using fuzzy correspondences. ACM TOG,\\n31(4):54:1–54:11, July 2012.', '[27] Philip Shilane, Patrick Min, Michael Kazhdan, and Thomas\\nFunkhouser. The Princeton shape benchmark. In Shape\\nModeling Applications. IEEE, 2004.', '[14] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\\n3D object representations for fine-grained categorization. In\\n4th International IEEE Workshop on 3D Representation and\\nRecognition (3dRR-13), Sydney, Australia, 2013.', '[15] Roman A Laskowski, E Gail Hutchinson, Alex D Michie,\\nAndrew C Wallace, Martin L Jones, and Janet M Thornton.\\nPDBsum: A web-based database of summaries and analyses\\nof all PDB structures. Trends Biochem. Sci., 22:488–490,\\n1997.', '[29] Atsushi Tatsuma, Hitoshi Koyanagi, and Masaki Aono. A\\nlarge-scale shape benchmark for 3D object retrieval: Toyohashi shape benchmark. In Asia Pacific Signal and Information Processing Association, 2012. 3\\n[30] Antonio Torralba, Bryan C Russell, and Jenny Yuen. LabelMe: Online image annotation and applications. Proceedings of the IEEE, 98(8):1467–1484, 2010.', '[16] Bo Li, Afzal Godil, Masaki Aono, X Bai, Takahiko Furuya,\\nL Li, R López-Sastre, Henry Johan, Ryutarou Ohbuchi, Carolina Redondo-Cabrera, et al. SHREC’12 track: generic 3D\\nshape retrieval. In 5th Eurographics Conference on 3D Object Retrieval, 2012.', '[31] Remco C. Veltkamp and FB ter Harr. SHREC 2007 3D shape\\nretrieval contest. Technical report, Utrecht University Technical Report UU-CS-2007-015, 2007.', '[17] Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias\\nSchreck, Masaki Aono, Qiang Chen, Nihad Karim Chowdhury, Bin Fang, Takahiko Furuya, et al. SHREC’14 track:\\n\\n[32] Dejan V Vranić. 3D model retrieval. University of Leipzig,\\nGermany, PhD thesis, 2004.', '[33] Raoul Wessel, Ina Blümel, and Reinhard Klein. A 3D shape\\nbenchmark for retrieval and automatic classification of architectural data. In Eurographics 2009 Workshop on 3D Object Retrieval, pages 53–56. The Eurographics Association,\\n2009.', '[34] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D\\nShapeNets: A Deep Representation for Volumetric Shapes.\\nCVPR, 2015.', '[35] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\\nPASCAL: A benchmark for 3D object detection in the wild.\\nIn WACV, 2014.', '[36] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.\\nSUN3D: A database of big spaces reconstructed using SfM\\nand object labels. In ICCV, pages 1625–1632, 2013. 2\\n[37] Juan Zhang, Kaleem Siddiqi, Diego Macrini, Ali Shokoufandeh, and Sven Dickinson. Retrieving articulated 3-D models using medial surfaces and their graph spectra. In Energy\\nminimization methods in computer vision and pattern recognition, 2005.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "wc(all_freqs, 'all_freqs')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "s = \"\"\"Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\n",
    "face attributes in the wild. In Proceedings of the IEEE\n",
    "International Conference on Computer Vision, pp. 3730–\n",
    "3738, 2015.\"\"\"\n",
    "\n",
    "# \\1{5}.*\\d\\d\\d\\d\\.\n",
    "# regex = r'[A-Z][A-Za-z\\,\\.\\ ]+[A-Z]\\.\\ [A-Z](?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "regex = r'[A-Z][A-Za-z\\-]+\\,\\ [A-Z]\\.(?:.*\\n.*){0,5}\\,\\ \\d\\d\\d\\d\\.'\n",
    "pattern = re.compile(regex)\n",
    "iterator = re.findall(pattern, s)\n",
    "print(iterator)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning\\nface attributes in the wild. In Proceedings of the IEEE\\nInternational Conference on Computer Vision, pp. 3730–\\n3738, 2015.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "top_dist(freqs, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 95\n",
      "industrial 92\n",
      "good 81\n",
      "view 67\n",
      "patentnet 61\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "top_dist(all_freqs, 1000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 495\n",
      "model 309\n",
      "part 283\n",
      "hash 280\n",
      "method 267\n",
      "learning 262\n",
      "category 261\n",
      "dataset 256\n",
      "shape 249\n",
      "data 247\n",
      "network 240\n",
      "view 207\n",
      "object 190\n",
      "deep 187\n",
      "segmentation 185\n",
      "figure 181\n",
      "annotation 177\n",
      "center 162\n",
      "training 154\n",
      "different 153\n",
      "code 149\n",
      "using 146\n",
      "retrieval 135\n",
      "set 131\n",
      "result 130\n",
      "similarity 129\n",
      "hashnet 129\n",
      "scale 127\n",
      "large 126\n",
      "function 126\n",
      "table 126\n",
      "level 123\n",
      "imagenet 122\n",
      "fashion 122\n",
      "multi 122\n",
      "accuracy 119\n",
      "instance 118\n",
      "loss 116\n",
      "label 115\n",
      "recognition 114\n",
      "information 110\n",
      "datasets 110\n",
      "number 109\n",
      "bit 109\n",
      "expression 107\n",
      "ieee 105\n",
      "triplet 105\n",
      "use 103\n",
      "fine 101\n",
      "two 100\n",
      "depth 100\n",
      "based 99\n",
      "three 99\n",
      "visual 98\n",
      "semantic 98\n",
      "class 97\n",
      "computer 97\n",
      "efficientnet 97\n",
      "performance 96\n",
      "vision 96\n",
      "grained 95\n",
      "scaling 95\n",
      "hashing 95\n",
      "hierarchical 94\n",
      "page 94\n",
      "feature 93\n",
      "also 93\n",
      "point 93\n",
      "embedding 93\n",
      "industrial 92\n",
      "good 92\n",
      "similar 92\n",
      "one 92\n",
      "matrix 91\n",
      "item 89\n",
      "work 88\n",
      "conference 88\n",
      "neural 87\n",
      "space 86\n",
      "example 85\n",
      "algorithm 85\n",
      "classification 84\n",
      "show 83\n",
      "proposed 82\n",
      "incomplete 80\n",
      "top 79\n",
      "used 78\n",
      "clustering 78\n",
      "first 76\n",
      "shapenet 76\n",
      "map 76\n",
      "binary 76\n",
      "layer 76\n",
      "task 74\n",
      "benchmark 72\n",
      "liu 71\n",
      "paper 70\n",
      "better 70\n",
      "human 67\n",
      "problem 67\n",
      "video 67\n",
      "multiple 65\n",
      "template 64\n",
      "metric 63\n",
      "wang 63\n",
      "pair 63\n",
      "hamming 63\n",
      "existing 62\n",
      "chair 62\n",
      "distance 62\n",
      "query 62\n",
      "patentnet 61\n",
      "average 61\n",
      "resolution 61\n",
      "cvpr 60\n",
      "experiment 59\n",
      "provide 59\n",
      "activation 59\n",
      "node 59\n",
      "text 58\n",
      "parameter 57\n",
      "evaluation 57\n",
      "architecture 57\n",
      "score 57\n",
      "size 56\n",
      "pattern 56\n",
      "database 55\n",
      "value 54\n",
      "research 54\n",
      "system 54\n",
      "analysis 53\n",
      "huang 53\n",
      "learn 53\n",
      "row 52\n",
      "best 52\n",
      "test 52\n",
      "representation 52\n",
      "input 52\n",
      "generate 52\n",
      "baseline 52\n",
      "max 52\n",
      "detail 51\n",
      "application 50\n",
      "dimension 50\n",
      "criterion 50\n",
      "proceeding 50\n",
      "facial 50\n",
      "dhn 50\n",
      "fecnet 50\n",
      "face 49\n",
      "section 49\n",
      "comparison 49\n",
      "zhang 49\n",
      "however 48\n",
      "search 48\n",
      "resnet 48\n",
      "precision 47\n",
      "visualization 47\n",
      "many 46\n",
      "challenge 46\n",
      "respectively 46\n",
      "http 46\n",
      "partnet 46\n",
      "mnist 46\n",
      "daimc 46\n",
      "given 46\n",
      "cl 46\n",
      "product 45\n",
      "train 45\n",
      "convolutional 45\n",
      "approach 45\n",
      "defined 45\n",
      "contains 44\n",
      "well 44\n",
      "user 44\n",
      "generated 44\n",
      "csq 43\n",
      "description 43\n",
      "preprint 43\n",
      "trained 43\n",
      "within 42\n",
      "common 42\n",
      "new 42\n",
      "acm 42\n",
      "learned 42\n",
      "prediction 42\n",
      "hi 42\n",
      "optimization 42\n",
      "width 42\n",
      "design 41\n",
      "shown 41\n",
      "state 41\n",
      "penalty 41\n",
      "time 40\n",
      "art 40\n",
      "following 40\n",
      "transaction 40\n",
      "inception 40\n",
      "stage 40\n",
      "flop 40\n",
      "supervised 40\n",
      "affnet 40\n",
      "sign 40\n",
      "sample 39\n",
      "corresponding 38\n",
      "fashionista 38\n",
      "machine 38\n",
      "pairwise 38\n",
      "emotion 38\n",
      "available 37\n",
      "type 37\n",
      "compared 37\n",
      "coco 36\n",
      "understanding 36\n",
      "real 36\n",
      "lamp 36\n",
      "alignment 36\n",
      "central 36\n",
      "consistent 36\n",
      "descriptor 36\n",
      "continuous 36\n",
      "report 36\n",
      "hj 36\n",
      "structure 35\n",
      "thus 35\n",
      "propose 35\n",
      "international 35\n",
      "need 35\n",
      "present 35\n",
      "action 35\n",
      "high 34\n",
      "fig 34\n",
      "validation 34\n",
      "attribute 34\n",
      "synset 34\n",
      "concept 33\n",
      "visually 33\n",
      "may 33\n",
      "help 33\n",
      "small 33\n",
      "l2 33\n",
      "semantics 33\n",
      "convnets 33\n",
      "much 32\n",
      "world 32\n",
      "single 32\n",
      "graphic 32\n",
      "chen 32\n",
      "dimensional 32\n",
      "curve 32\n",
      "unit 32\n",
      "study 31\n",
      "wide 31\n",
      "stackgan 31\n",
      "statistic 31\n",
      "quality 30\n",
      "knowledge 30\n",
      "clothing 30\n",
      "wordnet 30\n",
      "future 30\n",
      "neighbor 30\n",
      "processing 30\n",
      "process 30\n",
      "generative 30\n",
      "quantization 30\n",
      "weight 30\n",
      "rate 30\n",
      "coarse 30\n",
      "tanh 30\n",
      "key 29\n",
      "main 29\n",
      "related 29\n",
      "annotated 29\n",
      "original 29\n",
      "find 29\n",
      "continuation 29\n",
      "log 29\n",
      "bag 29\n",
      "nu 29\n",
      "entropy 29\n",
      "miou 29\n",
      "sgn 29\n",
      "com 28\n",
      "detection 28\n",
      "see 28\n",
      "due 28\n",
      "nmf 28\n",
      "across 27\n",
      "obtain 27\n",
      "pre 27\n",
      "since 27\n",
      "interface 27\n",
      "four 27\n",
      "several 27\n",
      "weighted 27\n",
      "maximum 27\n",
      "pointnet 27\n",
      "missing 26\n",
      "among 26\n",
      "trend 26\n",
      "v2 26\n",
      "previous 26\n",
      "bottom 25\n",
      "case 25\n",
      "order 25\n",
      "per 25\n",
      "distribution 25\n",
      "mean 25\n",
      "step 25\n",
      "wa 25\n",
      "efficient 25\n",
      "generation 25\n",
      "end 25\n",
      "line 25\n",
      "hinge 25\n",
      "compound 25\n",
      "fec 25\n",
      "estimator 25\n",
      "multiview 24\n",
      "property 24\n",
      "way 24\n",
      "back 24\n",
      "hierarchy 24\n",
      "make 24\n",
      "fei 24\n",
      "lin 24\n",
      "repository 24\n",
      "global 24\n",
      "embeddings 24\n",
      "via 24\n",
      "door 24\n",
      "leaf 24\n",
      "batch 24\n",
      "hhi 24\n",
      "splitter 24\n",
      "labeled 23\n",
      "higher 23\n",
      "material 23\n",
      "right 23\n",
      "standard 23\n",
      "relationship 23\n",
      "bed 23\n",
      "subset 23\n",
      "nmi 23\n",
      "achieve 23\n",
      "fashionability 23\n",
      "capture 23\n",
      "larger 22\n",
      "left 22\n",
      "clock 22\n",
      "collection 22\n",
      "provided 22\n",
      "total 22\n",
      "compact 22\n",
      "efficiency 22\n",
      "tree 22\n",
      "objective 22\n",
      "semi 22\n",
      "gradient 22\n",
      "mesh 22\n",
      "taxonomy 22\n",
      "million 21\n",
      "intelligence 21\n",
      "particular 21\n",
      "note 21\n",
      "estimation 21\n",
      "goal 21\n",
      "random 21\n",
      "labeling 21\n",
      "b0 21\n",
      "achieves 21\n",
      "provides 21\n",
      "v1 21\n",
      "increase 21\n",
      "chang 21\n",
      "hu 21\n",
      "aligned 21\n",
      "term 21\n",
      "recent 21\n",
      "compare 21\n",
      "gini 21\n",
      "symmetry 21\n",
      "scene 20\n",
      "long 20\n",
      "ground 20\n",
      "novel 20\n",
      "advance 20\n",
      "multimedia 20\n",
      "google 20\n",
      "non 20\n",
      "synthesis 20\n",
      "ci 20\n",
      "annotator 20\n",
      "avg 20\n",
      "resource 20\n",
      "exactly 20\n",
      "university 19\n",
      "community 19\n",
      "issue 19\n",
      "cloud 19\n",
      "component 19\n",
      "focus 19\n",
      "imbalance 19\n",
      "acc 19\n",
      "various 19\n",
      "th 19\n",
      "aaai 19\n",
      "online 19\n",
      "observe 19\n",
      "definition 19\n",
      "dissimilar 19\n",
      "define 19\n",
      "e2 19\n",
      "convergence 19\n",
      "b7 19\n",
      "gpipe 19\n",
      "raters 19\n",
      "mpeg 19\n",
      "demonstrate 18\n",
      "cad 18\n",
      "field 18\n",
      "general 18\n",
      "often 18\n",
      "effort 18\n",
      "cover 18\n",
      "candidate 18\n",
      "source 18\n",
      "name 18\n",
      "currently 18\n",
      "get 18\n",
      "perform 18\n",
      "sun 18\n",
      "van 18\n",
      "low 18\n",
      "correspondence 18\n",
      "inference 18\n",
      "xi 18\n",
      "smaller 18\n",
      "coefficient 18\n",
      "nv 18\n",
      "ni 18\n",
      "mask 18\n",
      "plane 18\n",
      "side 17\n",
      "address 17\n",
      "including 17\n",
      "describe 17\n",
      "important 17\n",
      "adopt 17\n",
      "consists 17\n",
      "middle 17\n",
      "experimental 17\n",
      "img 17\n",
      "directly 17\n",
      "truth 17\n",
      "refer 17\n",
      "deng 17\n",
      "rich 17\n",
      "style 17\n",
      "szegedy 17\n",
      "le 17\n",
      "zhao 17\n",
      "joint 17\n",
      "share 17\n",
      "ranking 17\n",
      "hyper 17\n",
      "iteration 17\n",
      "l1 17\n",
      "ac 17\n",
      "geometric 17\n",
      "median 17\n",
      "tog 17\n",
      "convnet 17\n",
      "transfer 17\n",
      "scaled 17\n",
      "hik 17\n",
      "introduce 16\n",
      "patent 16\n",
      "second 16\n",
      "around 16\n",
      "therefore 16\n",
      "collect 16\n",
      "include 16\n",
      "evaluate 16\n",
      "conduct 16\n",
      "widely 16\n",
      "difference 16\n",
      "explore 16\n",
      "critical 16\n",
      "cross 16\n",
      "su 16\n",
      "yi 16\n",
      "tan 16\n",
      "like 16\n",
      "learns 16\n",
      "color 16\n",
      "bar 16\n",
      "output 16\n",
      "hinton 16\n",
      "zhou 16\n",
      "compute 16\n",
      "volume 16\n",
      "variation 16\n",
      "wild 16\n",
      "journal 16\n",
      "hat 16\n",
      "performs 16\n",
      "kernel 16\n",
      "basis 16\n",
      "orientation 16\n",
      "challenging 15\n",
      "according 15\n",
      "comprehensive 15\n",
      "ambiguity 15\n",
      "denotes 15\n",
      "setting 15\n",
      "natural 15\n",
      "ab 15\n",
      "possible 15\n",
      "cannot 15\n",
      "become 15\n",
      "adversarial 15\n",
      "zhu 15\n",
      "web 15\n",
      "still 15\n",
      "www 15\n",
      "though 15\n",
      "either 15\n",
      "retrieved 15\n",
      "cnn 15\n",
      "latent 15\n",
      "difficult 15\n",
      "recently 15\n",
      "collected 15\n",
      "flower 15\n",
      "close 15\n",
      "whole 15\n",
      "likelihood 15\n",
      "squared 15\n",
      "recall 15\n",
      "ucf101 15\n",
      "regression 15\n",
      "pvc 15\n",
      "jk 15\n",
      "knife 15\n",
      "iou 15\n",
      "zoph 15\n",
      "album 15\n",
      "hjk 15\n",
      "multilabel 14\n",
      "characteristic 14\n",
      "able 14\n",
      "introduction 14\n",
      "improve 14\n",
      "usually 14\n",
      "edu 14\n",
      "designed 14\n",
      "aim 14\n",
      "densenet 14\n",
      "sc 14\n",
      "graph 14\n",
      "wu 14\n",
      "springer 14\n",
      "yu 14\n",
      "guibas 14\n",
      "workshop 14\n",
      "rethinking 14\n",
      "without 14\n",
      "another 14\n",
      "even 14\n",
      "temporal 14\n",
      "specifically 14\n",
      "form 14\n",
      "factorization 14\n",
      "sne 14\n",
      "eccv 14\n",
      "final 14\n",
      "content 14\n",
      "vector 14\n",
      "lstm 14\n",
      "yan 14\n",
      "hadamard 14\n",
      "hence 14\n",
      "individual 14\n",
      "c0i 14\n",
      "calculate 14\n",
      "corresponds 14\n",
      "ratio 14\n",
      "itq 14\n",
      "outperforms 14\n",
      "margin 14\n",
      "computing 14\n",
      "norm 14\n",
      "error 14\n",
      "base 14\n",
      "colour 14\n",
      "histogram 14\n",
      "binarization 14\n",
      "abstract 13\n",
      "significantly 13\n",
      "detailed 13\n",
      "artificial 13\n",
      "appearance 13\n",
      "front 13\n",
      "furniture 13\n",
      "organized 13\n",
      "follows 13\n",
      "finally 13\n",
      "researcher 13\n",
      "although 13\n",
      "pose 13\n",
      "gen 13\n",
      "support 13\n",
      "described 13\n",
      "nearest 13\n",
      "take 13\n",
      "ai 13\n",
      "aware 13\n",
      "convolution 13\n",
      "yang 13\n",
      "corpus 13\n",
      "org 13\n",
      "consider 13\n",
      "index 13\n",
      "presented 13\n",
      "id 13\n",
      "randomly 13\n",
      "summary 13\n",
      "photo 13\n",
      "yes 13\n",
      "fixed 13\n",
      "gan 13\n",
      "prior 13\n",
      "encoder 13\n",
      "hidden 13\n",
      "instead 13\n",
      "optimizing 13\n",
      "sec 13\n",
      "dnnh 13\n",
      "c1 13\n",
      "equation 13\n",
      "physical 13\n",
      "bowl 13\n",
      "mug 13\n",
      "trash 13\n",
      "vase 13\n",
      "additional 13\n",
      "nasnet 13\n",
      "rater 13\n",
      "cld 13\n",
      "ehd 13\n",
      "big 12\n",
      "china 12\n",
      "author 12\n",
      "variety 12\n",
      "context 12\n",
      "optimize 12\n",
      "fewer 12\n",
      "complete 12\n",
      "associated 12\n",
      "third 12\n",
      "target 12\n",
      "easily 12\n",
      "partial 12\n",
      "block 12\n",
      "scenario 12\n",
      "conclusion 12\n",
      "language 12\n",
      "especially 12\n",
      "reference 12\n",
      "tang 12\n",
      "xiao 12\n",
      "funkhouser 12\n",
      "connected 12\n",
      "amazon 12\n",
      "modeling 12\n",
      "surface 12\n",
      "lu 12\n",
      "krizhevsky 12\n",
      "textual 12\n",
      "expert 12\n",
      "framework 12\n",
      "cnnh 12\n",
      "dh 12\n",
      "combination 12\n",
      "lq 12\n",
      "min 12\n",
      "tab 12\n",
      "alexnet 12\n",
      "representative 12\n",
      "digit 12\n",
      "shen 12\n",
      "solution 12\n",
      "mic 12\n",
      "eq 12\n",
      "edge 12\n",
      "confusion 12\n",
      "ensemble 12\n",
      "bigger 12\n",
      "sgpn 12\n",
      "manual 12\n",
      "classifier 12\n",
      "affectnet 12\n",
      "useful 12\n",
      "crammer 12\n",
      "singer 12\n",
      "technology 11\n",
      "current 11\n",
      "perspective 11\n",
      "addition 11\n",
      "dress 11\n",
      "construct 11\n",
      "next 11\n",
      "five 11\n",
      "accessory 11\n",
      "measure 11\n",
      "build 11\n",
      "could 11\n",
      "applied 11\n",
      "every 11\n",
      "obtained 11\n",
      "observation 11\n",
      "technique 11\n",
      "etc 11\n",
      "attention 11\n",
      "computation 11\n",
      "song 11\n",
      "karras 11\n",
      "simple 11\n",
      "question 11\n",
      "significant 11\n",
      "specific 11\n",
      "go 11\n",
      "basic 11\n",
      "han 11\n",
      "complexity 11\n",
      "net 11\n",
      "lee 11\n",
      "boot 11\n",
      "handle 11\n",
      "generic 11\n",
      "commonly 11\n",
      "faster 11\n",
      "proposes 11\n",
      "smooth 11\n",
      "effective 11\n",
      "discrete 11\n",
      "negative 11\n",
      "zero 11\n",
      "normalization 11\n",
      "annotate 11\n",
      "bottle 11\n",
      "root 11\n",
      "geometry 11\n",
      "shapenetcore 11\n",
      "mobile 11\n",
      "v4 11\n",
      "zalando 11\n",
      "file 11\n",
      "ovr 11\n",
      "rigid 11\n",
      "multimodal 10\n",
      "guangdong 10\n",
      "area 10\n",
      "traditional 10\n",
      "factor 10\n",
      "texture 10\n",
      "domain 10\n",
      "github 10\n",
      "least 10\n",
      "science 10\n",
      "car 10\n",
      "document 10\n",
      "mainly 10\n",
      "public 10\n",
      "range 10\n",
      "already 10\n",
      "gap 10\n",
      "short 10\n",
      "group 10\n",
      "indicates 10\n",
      "region 10\n",
      "relatively 10\n",
      "grouping 10\n",
      "xu 10\n",
      "fashionable 10\n",
      "evolution 10\n",
      "shirt 10\n",
      "considered 10\n",
      "dynamic 10\n",
      "similarly 10\n",
      "give 10\n",
      "list 10\n",
      "filter 10\n",
      "estimate 10\n",
      "people 10\n",
      "idea 10\n",
      "us 10\n",
      "functionality 10\n",
      "enable 10\n",
      "nip 10\n",
      "thomas 10\n",
      "conditional 10\n",
      "advantage 10\n",
      "display 10\n",
      "bi 10\n",
      "open 10\n",
      "unsupervised 10\n",
      "imbalanced 10\n",
      "discriminative 10\n",
      "cost 10\n",
      "guarantee 10\n",
      "di 10\n",
      "column 10\n",
      "positive 10\n",
      "fast 10\n",
      "nonnegative 10\n",
      "regularization 10\n",
      "subspace 10\n",
      "entire 10\n",
      "kb 10\n",
      "driven 10\n",
      "distinct 10\n",
      "pointcnn 10\n",
      "channel 10\n",
      "howard 10\n",
      "wij 10\n",
      "protocol 10\n",
      "namely 9\n",
      "manually 9\n",
      "subclass 9\n",
      "structural 9\n",
      "publicly 9\n",
      "modal 9\n",
      "contain 9\n",
      "last 9\n",
      "pixel 9\n",
      "far 9\n",
      "coverage 9\n",
      "rest 9\n",
      "represent 9\n",
      "cluster 9\n",
      "mutual 9\n",
      "nature 9\n",
      "full 9\n",
      "automatic 9\n",
      "luo 9\n",
      "robust 9\n",
      "generator 9\n",
      "zisserman 9\n",
      "reed 9\n",
      "deeper 9\n",
      "residual 9\n",
      "maaten 9\n",
      "adaptive 9\n",
      "constraint 9\n",
      "reason 9\n",
      "co 9\n",
      "contrast 9\n",
      "popularity 9\n",
      "improves 9\n",
      "ii 9\n",
      "link 9\n",
      "rank 9\n",
      "preference 9\n",
      "fact 9\n",
      "jointly 9\n",
      "providing 9\n",
      "iccv 9\n",
      "bengio 9\n",
      "corr 9\n",
      "xie 9\n",
      "separated 9\n",
      "ten 9\n",
      "require 9\n",
      "becomes 9\n",
      "exp 9\n",
      "returned 9\n",
      "default 9\n",
      "diagonal 9\n",
      "computed 9\n",
      "apply 9\n",
      "yield 9\n",
      "coding 9\n",
      "shao 9\n",
      "towards 9\n",
      "reducing 9\n",
      "consistency 9\n",
      "reduce 9\n",
      "k2 9\n",
      "multinmf 9\n",
      "granularity 9\n",
      "decomposition 9\n",
      "laptop 9\n",
      "refinement 9\n",
      "vertical 9\n",
      "spidercnn 9\n",
      "transformation 9\n",
      "seat 9\n",
      "change 9\n",
      "efficientnets 9\n",
      "b4 9\n",
      "b3 9\n",
      "pipeline 9\n",
      "i1 9\n",
      "linear 9\n",
      "icml 9\n",
      "i2 9\n",
      "facsnet 9\n",
      "ill 9\n",
      "propagation 9\n",
      "gik 9\n",
      "upright 9\n",
      "intellectual 8\n",
      "highly 8\n",
      "diverse 8\n",
      "whose 8\n",
      "benchmarking 8\n",
      "agreement 8\n",
      "potential 8\n",
      "doe 8\n",
      "identify 8\n",
      "belongs 8\n",
      "project 8\n",
      "version 8\n",
      "stanford 8\n",
      "deepfashion 8\n",
      "year 8\n",
      "follow 8\n",
      "found 8\n",
      "want 8\n",
      "rule 8\n",
      "represents 8\n",
      "unique 8\n",
      "assigned 8\n",
      "divided 8\n",
      "weak 8\n",
      "naturally 8\n",
      "tend 8\n",
      "fan 8\n",
      "lower 8\n",
      "select 8\n",
      "choose 8\n",
      "reported 8\n",
      "lack 8\n",
      "effectively 8\n",
      "national 8\n",
      "grant 8\n",
      "european 8\n",
      "finegrained 8\n",
      "exploring 8\n",
      "cao 8\n",
      "der 8\n",
      "recommendation 8\n",
      "generating 8\n",
      "together 8\n",
      "considering 8\n",
      "interaction 8\n",
      "evaluated 8\n",
      "explorer 8\n",
      "discus 8\n",
      "selected 8\n",
      "selection 8\n",
      "simply 8\n",
      "introduced 8\n",
      "produce 8\n",
      "gain 8\n",
      "relevant 8\n",
      "direction 8\n",
      "gans 8\n",
      "picture 8\n",
      "enables 8\n",
      "respect 8\n",
      "overall 8\n",
      "empirical 8\n",
      "lead 8\n",
      "predict 8\n",
      "remove 8\n",
      "boost 8\n",
      "effectiveness 8\n",
      "sampling 8\n",
      "fully 8\n",
      "c2 8\n",
      "hk 8\n",
      "power 8\n",
      "probability 8\n",
      "lc 8\n",
      "difficulty 8\n",
      "cca 8\n",
      "ksh 8\n",
      "backbone 8\n",
      "survey 8\n",
      "summarization 8\n",
      "besides 8\n",
      "making 8\n",
      "manifold 8\n",
      "optimal 8\n",
      "update 8\n",
      "vt 8\n",
      "control 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "all_references.to_csv('data/all_references.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('patents-ai': conda)"
  },
  "interpreter": {
   "hash": "9492b3f3ffaca3e3deebd53b740a5e3b3a42fa9584de750d11d509ba01fba996"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}